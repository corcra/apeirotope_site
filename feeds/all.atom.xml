<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>apeirotope</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2021-01-04T00:00:00+00:00</updated><entry><title>redesign</title><link href="/meta/2021-01-04-redesign.html" rel="alternate"></link><published>2021-01-04T00:00:00+00:00</published><updated>2021-01-04T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2021-01-04:/meta/2021-01-04-redesign.html</id><summary type="html">&lt;p&gt;I have updated the design of the website! This started off as 'drop in a new Pelican theme and be done' and grew into a micro-project to do while confined to my parents house over Christmas (pandemic etc). I also watched lots of medium-to-terrible TV (GYNOTOPIA, SERIOUSLY?), which is a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have updated the design of the website! This started off as 'drop in a new Pelican theme and be done' and grew into a micro-project to do while confined to my parents house over Christmas (pandemic etc). I also watched lots of medium-to-terrible TV (GYNOTOPIA, SERIOUSLY?), which is a good accompaniment to making tiny tweaks in CSS and seeing what happens.&lt;/p&gt;
&lt;p&gt;For posterity, the previous version of apeirotope looked like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2021/apeirotope_medius_index.png"&gt;&lt;/p&gt;
&lt;p&gt;... which was an edited version of the &lt;a href="https://github.com/onur/medius"&gt;medius theme&lt;/a&gt;, which was designed to imitate Medium. Post example:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2021/apeirotope_medius_post.png"&gt;&lt;/p&gt;
&lt;p&gt;The new version, also for posterity (the internet is a flimsy place for memories) looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2021/apeirotope_voce_index.png"&gt;&lt;/p&gt;
&lt;p&gt;Post example:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2021/apeirotope_voce_post.png"&gt;&lt;/p&gt;
&lt;p&gt;The new design is a modified version of the &lt;a href="https://github.com/limbenjamin/voce"&gt;voce theme&lt;/a&gt; - my fork is &lt;a href="https://github.com/corcra/voce"&gt;here&lt;/a&gt;. You can do a diff between my fork and the original one for the most comprehensive account of the edits I made, but I'll describe them in English here. Also, for comparison, here's what apeirotope would look like in mostly-plain voce:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2021/apeirotope_voce_index_original.png"&gt;&lt;/p&gt;
&lt;p&gt;It's not really a fair comparison because the voce theme uses tags carefully and sparingly, whereas I use them freely, and instead use categories (which it doesn't really use) to categorise my posts. You can see a much better version of the voce theme on its &lt;a href="https://limbenjamin.com/"&gt;creator's personal page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Particular edits I made:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Obviously, get rid of the milieu of tags. I experimented with replacing them with my categories, but I didn't like the result. I settled with fully removing that section, and adding pages for Tags and Categories to the navigation bar.&lt;/li&gt;
&lt;li&gt;Removing more elements, I got rid of the "logo" area and (not shown) the copyright stuff at the bottom. As far as I can tell from 10s of searching, those types of copyright declarations are mostly for scaring people off plagiarising your stuff, but aren't actually necessary to legally assert copyright. Following up with a further 20s of searching, I'd probably put a CC-BY-SA license on this site if I had to. I haven't had any legal issues in the 5 years or so I've had the website, so maybe copyright will be abolished soon and I won't need to think further about this.&lt;/li&gt;
&lt;li&gt;It looks like I added an italic serif font, but he (Benjamin Lin, voce creator) seems to have done a nontrivial style update &lt;em&gt;four days ago&lt;/em&gt;, which included removing it. This is funny to me because the previous update was in April, so I guess we were both spending part of our new years time editing the voce design. Hi Benjamin!&lt;/li&gt;
&lt;li&gt;It's ideally not visible at all, but I ripped out a bunch of surveillance-flavoured internals, including: anything to do with google analytics, and (hopefully) all calls to external sources, including fonts, bootstrap CSS, etc. &lt;a href="https://twitter.com/corcra/status/1342171321472573442"&gt;Webfonts are hosted locally for apeirotope&lt;/a&gt;. Possibly didn't catch everything and maybe my website is slower now because a weak little OVH VPS is serving everything. My guess is that this site doesn't do enough traffic for that to be a real concern, but given the lack of analytics/me bothering to look at nginx logs ever, I have no idea.&lt;/li&gt;
&lt;li&gt;Introducing my "colour-coding categories" nonsense (which I already had in medius, but required rejigging given the new templates in voce). This was probably the most complicated part because I &lt;em&gt;sort of&lt;/em&gt; understand CSS, but not well enough to readily diagnose why certain styles are overriding each other. Concessions had to be made, but I am fairly happy with the result.&lt;/li&gt;
&lt;li&gt;Directly edited minified bootstrap CSS to remove a weird navy colour that kept appearing in my unflavoured links. Numerous other minor style edits, including removing that (admittedly lovely) turquoise colour, and making things generally smaller.&lt;/li&gt;
&lt;li&gt;The tags page sorts by number of posts, not alphabetically. Nobody should look at the tags page, though. I have a link-rot issue where I don't want to modify &lt;em&gt;categories&lt;/em&gt; (since they define absolute paths to posts), but it's open season on tags and some day I may do a pass on de-duplicating them.&lt;/li&gt;
&lt;li&gt;The archives page (new!) only segregates posts by month if there are more than 4 in a year. I am most proud of this edit because it required writing some Jinja, which first required learning the word "Jinja" so as to upgrade my search queries from "if statement pelican".&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, this was NOT procrastination from generating other types of content for the website.&lt;/p&gt;</content><category term="webdev"></category><category term="css"></category></entry><entry><title>podcast review 2020</title><link href="/tips/2020-11-29-podcasts-2020.html" rel="alternate"></link><published>2020-11-29T00:00:00+00:00</published><updated>2020-11-29T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2020-11-29:/tips/2020-11-29-podcasts-2020.html</id><summary type="html">&lt;p&gt;One of the nice things about having your own website is that you can do whatever you want. Today I am going to recommend/describe some podcasts I listen to. I usually do this directly to whoever speaks to me after I find a new one, but now I can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the nice things about having your own website is that you can do whatever you want. Today I am going to recommend/describe some podcasts I listen to. I usually do this directly to whoever speaks to me after I find a new one, but now I can do it to people on the internet too.&lt;/p&gt;
&lt;p&gt;This will be more of a review than strictly a list of &lt;em&gt;recommendations&lt;/em&gt;, so I'm not saying all of these are great. But I did listen to them.&lt;/p&gt;
&lt;p&gt;Warning: some of these podcasts will try to sell you a mattress.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://gimletmedia.com/shows/reply-all"&gt;Reply All&lt;/a&gt; is probably the most generic podcast recommendation I could give. It's slickly produced, it's reliably entertaining, it has interesting trivia and moments of heart and touches on contemporary issues in a way which is not especially challenging. You sort of know what you're getting, even if you don't know what it'll be about. I have a strange memory of listening to the first few episodes of this when it started back in 2014 and being like "what is this podcast supposed to be about". Then I stopped listening for 5 years and I still don't entirely know. It's just about... stuff, you know? Stuff on the internet. Episode rec: &lt;a href="https://gimletmedia.com/shows/reply-all/kwh23r/148-bedbugs-and-aliens"&gt;Bedbugs and Aliens&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://99percentinvisible.org/"&gt;99% Invisible&lt;/a&gt; is another reliable recommendation. The host (Roman Mars) has a strange and specific way of speaking which subtly aggravates me, but the content is really great. It's about "design", which is pretty broad - they talk about cities, infrastructure, architecture, objects, history, etc. The stories are interesting and hopefully mostly factual and the urbanism bias is very much up my alley. Episode rec: &lt;a href="https://99percentinvisible.org/episode/palaces-for-the-people/"&gt;Palaces for the People&lt;/a&gt; (spoiler: they are LIBRARIES)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://maximumfun.org/podcasts/adventure-zone/"&gt;The Adventure Zone&lt;/a&gt; is a RPG podcast, which is a genre of podcast where you listen to people play role-playing games (with some editing). TAZ features known podcasters "the McElroy family", who seem to be lovely people and also very funny. I listened to TAZ Amnesty, which was a &lt;a href="http://apocalypse-world.com/pbta/games/title/Monster_of_the_Week"&gt;Monster of the Week&lt;/a&gt; campaign. They managed to blend an interesting world, heartfelt story, and absurd humour. I understand there to be many RPG podcasts out there, but I have mostly only listened to this one. Episode rec: &lt;a href="https://maximumfun.org/episodes/adventure-zone/adventure-zone-amnesty-episode-1/"&gt;Episode 1 of Amnesty I guess&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.parcast.com/mythology"&gt;Mythology (on the Parcast Network!)&lt;/a&gt; is dramatisations of stories from mythology, in that they get voice actors to play out stories from mythology. I'm not mythology buff and suspect this is some entry-level mythology content. Please do not @ me with mythology-related screeds. It's light-weight story telling, although the reuse of voice actors gets a bit confusing if you listen for too long at once. The oddest thing about this podcast is how I get the feeling it's made by aliens who want to blend in, but who are too enthusiastic about mythology to pull it off. I will not elaborate on what I mean by this. Episode rec: The Abduction of Persephone.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://theconversation.com/uk/topics/the-anthill-podcast-27460"&gt;The Anthill&lt;/a&gt; is a podcast from &lt;a href="https://theconversation.com/"&gt;The Conversation&lt;/a&gt;. Their sthick is getting academics directly to write articles (or be interviewed on a podcast) on topics of their expertise. The format seems to be multi-episode mini-series with long gaps, so it's not a "reliable weekly" sort of thing. Direct academic involvement means the quality of information should be pretty high, and they better not have lied to me. I've only listened to three of the mini-series (&lt;a href="https://theconversation.com/india-tomorrow-part-1-fake-news-and-the-battle-for-information-113579"&gt;India Tomorrow&lt;/a&gt;, &lt;a href="https://theconversation.com/how-to-spot-a-conspiracy-theory-expert-guide-to-conspiracy-theories-part-one-133802"&gt;Conspiracy Theories&lt;/a&gt;, &lt;a href="https://theconversation.com/how-europe-recovered-from-the-black-death-recovery-podcast-series-part-one-139896"&gt;Recovery&lt;/a&gt;) and they were all great.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This concludes the list of &lt;em&gt;reasonably generic&lt;/em&gt; podcasts I listen to. Now comes the more esoteric stuff, I guess.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://lingthusiasm.com/"&gt;Lingthusiasm&lt;/a&gt; is about linguistics. It's a podcast hosted by two linguists, talking about linguistics. The enthusiasm they have for linguistics is really charming. I don't listen to this one all the time because whenever I learn things about linguistics I forget things about computers, but it's another podcast where I will reliably come away having learned something. About linguistics. Episode rec: &lt;a href="https://soundcloud.com/lingthusiasm/39-how-to-rebalance-a-lopsided-conversation"&gt;How to Rebalance a Lopsided Conversation&lt;/a&gt; or &lt;a href="https://soundcloud.com/lingthusiasm/12-sounds-you-cant-hear-babies-accents-and-phonemes"&gt;Sounds You Can't Hear&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.criticalfrequency.org/drilled"&gt;Drilled&lt;/a&gt; is a "True Crime Podcast about Climate Change". It focuses on fossil fuel propaganda and it's absolutely enraging. I actually stopped listening to it because I don't like making myself so simultaneously miserable and angry, but I did find it genuinely very informative. The second season (about crab fisherman) was a very different sort of climate story to what I'm used to. Overall, it's got that true crime investigative energy and it will make you want to burn Exxon to the ground.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/maintenancepod"&gt;Maintenance Phase&lt;/a&gt; is about debunking wellness culture, health and weight-loss fads. It &lt;em&gt;just&lt;/em&gt; started so I don't have a lot to go by, but the first few episodes have been strong. I find the discussion of dieting and diet culture very enlightening, because I'm a lucky person who eats whatever I want without thinking about it really. The hosts have something of a similar conversational style as Lingthusiasm, which is &lt;em&gt;very enthusiastic&lt;/em&gt; in a way I feel is quite American, but neither of the hosts of Lingthusiasm are American, so that's on me and I need to think about that. Episode rec: &lt;a href="https://open.spotify.com/episode/2PnFGxYEIgss1KISeICFHp?si=XIdHUSpPQ7Gk5_siyjXxLQ"&gt;Moon Juice&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://thehistoryofrome.typepad.com/revolutions_podcast/"&gt;Revolutions&lt;/a&gt; is my newest podcast addition and therefore the direct inspiration for this post. This is a history podcast about "great political revolutions" and has been a hard sell to my friends. I am shamefully ignorant of history and find myself going on occasional history splurges trying to catch up (I was really obsessed with World War 1 for a while after reading The Guns of August). It turns out that history is really interesting when it's told to you like a story, and not as a list of events to memorise. I've started right at the start of this podcast (in 2013) so am still learning about the English Civil War of 1642. I have good reason to believe there will be more revolutions to come.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.imaginaryworldspodcast.org/"&gt;Imaginary Worlds&lt;/a&gt; is about sci-fi and fantasy, with a focus (ostensibly) on world-building. I think it's broader than that though, and the episodes often touch on wider issues relating to the media, or more fundamental ideas which are explored in sci-fi/fantasty. I only listen when I've read/seen the thing, so I don't listen to this one all that often. I do love imaginary worlds, though. Episode recs: &lt;a href="https://www.imaginaryworldspodcast.org/episodes/the-power-of-the-makeover-mage"&gt;The Power of the Makeover Mage&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This concludes the list of &lt;em&gt;reasonably generic esoteric&lt;/em&gt; podcasts I listen to. Now for the flagrantly political ones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://podcasts.apple.com/us/podcast/mass-for-shut-ins-the-gin-and-tacos-podcast/id1341525125?mt=2"&gt;Mass for Shut-ins&lt;/a&gt; is the podcast version of &lt;a href="http://www.ginandtacos.com/"&gt;Gin and Tacos&lt;/a&gt;, a somewhat snarky political scientist who writes about American politics. I mostly view the podcast as the extension to his blog, which I've been reading on and off for many years now. I find what he has to say about the machinations of American politics interesting, e.g. &lt;a href="https://podcasts.apple.com/us/podcast/minicast-c6-fdrs-court-packing-scheme-1937/id1341525125?i=1000492197345"&gt;FDR's Court Packing Scheme&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://citationsneeded.libsyn.com/"&gt;Citations Needed&lt;/a&gt; is about "the intersection of media, PR, and power". This is a very well-produced podcast that will consistently point out how something you thought was benign is actually bad. It's interesting in a depressing way as a result, although it can be validating to have your suspicions (that something you thought was benign is actually bad) confirmed by someone else. Examples include &lt;a href="https://citationsneeded.libsyn.com/episode-116-the-pro-gentrification-aspirationalism-of-hgtvs-house-flipping-shows"&gt;The Pro-Gentrification Aspirationalism of HGTV's House-Flipping Shows&lt;/a&gt; and &lt;a href="https://citationsneeded.libsyn.com/episode-85-incitement-against-the-homeless-part-i-the-infestation-rhetoric-of-local-news"&gt;Incitement Against the Homeless (Part I) - The Infestation Rhetoric of Local News&lt;/a&gt;. My enthusiasm for this podcast started to wane after I felt like it was making me hopelessly cynical, so I only listen intermittently now.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.currentaffairs.org/podcast"&gt;Current Affairs&lt;/a&gt; is the podcast to accompany the magazine, of which I am also a subscriber. Current Affairs (both magazine and podcast) sit nicely in the intersection of serious leftist critique and frivolity. The typical podcast features some subset of the editors of CA talking about current events and more. My favourite segment is "Lefty Shark Tank", where someone proposes an absurd-sounding policy which will be judged by the others. Policies have included "Lower the voting age to zero", "Elected officials should have to wear burlap sacks", and other less memorable things that sound ridiculous but have surprising implications. It's difficult to select an episode to recommend as the panel episodes are basically miscellany and news, so take this Episode rec: &lt;a href="https://currentaffairs.simplecast.com/episodes/48-an-analysis-of-birds-large-and-small"&gt;An Analysis of Birds, Large and Small&lt;/a&gt;, or this horrifying/slightly inspiring one where two of the lawyers on the CA team talk about their work: &lt;a href="https://currentaffairs.simplecast.com/episodes/immigration-update-detention-and-covid"&gt;Immigration Update: Detention and COVID&lt;/a&gt;. Not sure why CA has so many lawyers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://workingclasshistory.com/podcast/"&gt;Working Class History&lt;/a&gt; is a history podcast, focusing on the role of normal (=working class) people in history. I got into it via a crossover they did with &lt;a href="https://workingclasshistory.com/2020/05/26/e38-mutiny-with-srsly-wrong/"&gt;Srsly Wrong on mutinies&lt;/a&gt;. I find this podcast can be a bit dry or slightly more taxing to listen to than some of the other more podcasts, so need to be in an attentive mood, but the subject matter is generally fascinating. Slightly esoteric subject matter means even people who know more history than me will probably learn something, but you must understand I'm really starting with nothing here, August 1914 notwithstanding. Episode rec: &lt;a href="https://workingclasshistory.com/2019/04/09/wch-crime-columbia-eagle-mutiny/"&gt;miniseries on the Columbia Eagle Mutiny&lt;/a&gt;, or &lt;a href="https://workingclasshistory.com/2020/02/17/e35-37-the-43-group/"&gt;miniseries on The 43 Group&lt;/a&gt; if you're less interested in mutinies than I am.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://generalintellectunit.net/"&gt;General Intellect Unit&lt;/a&gt; looks at the intersection of technology and left politics. I only got onto this one recently but suspect I will have many thoughts on it in future (given I am a Tech). They're quite interested in cybernetics and a bunch of the episodes are essentially summaries of the main ideas of Stafford Beer, e.g. &lt;a href="http://generalintellectunit.net/e/031-designing-freedom/"&gt;Designing Freedom&lt;/a&gt;, but they also cover the contents of related books, like &lt;a href="http://generalintellectunit.net/e/008-red-plenty-part-1/"&gt;Red Plenty&lt;/a&gt;, or sci-fi I already like, like &lt;a href="http://generalintellectunit.net/e/017-the-dispossessed-with-fraser-simons/"&gt;The Dispossesed&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://soundcloud.com/therednationpod"&gt;The Red Nation Podcast&lt;/a&gt; is about "Indigenous history, politics and culture from a left perspective". As an Irish person living in England, this podcast is like a dispatch from a totally different world, but it's a welcome change from the typical US-based perspectives. There's a bit of an academic energy to it at times, but mostly I find it interesting to hear what Indigenous people are saying about current and historical events. Episode rec: &lt;a href="https://soundcloud.com/therednationpod/the-fourth-of-you-lie-w-dallas"&gt;The Fourth of You-Lie&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://blubrry.com/wdtatw/"&gt;We Don't Talk About the Weather&lt;/a&gt; is a podcast I found while trying to get away from American podcasts (sorry, rest of list). WDTATW is two London-based guys talking about current affairs, mostly in the UK. The podcast aptly self-describes itself as "sounding like screaming and crying", which captures some of the spirit if not the actual energy levels of the podcast. I mostly use this as a palette cleanser from all the US-based takes beaming into my brain 23 hours a day, especially if something has happened in the UK that needs analysing. Unfortunately something has always just happened in the UK that needs analysing, and it's usually terrible. I feel obligated to be at least somewhat informed about the politics of the country I live in, so here we are. "Boris Johnson is a pagan" comes up more often than I think is normal. Episode rec is a bit hard but take &lt;a href="https://blubrry.com/wdtatw/59775210/episode-119-i-cant-believe-i-forgot-5g/"&gt;I Can't Believe I "Forgot" 5G&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://trashfuturepodcast.podbean.com/"&gt;Trashfuture&lt;/a&gt; is one I don't really listen to any more but want to include as part of the "Leftist Podcasts from the UK" genre, in case someone else likes it. It's a bunch of people discussing and making jokes about current events and politics. I stopped listening to it because it's too snarky and chaotic for me (people interrupting each other, etc), but if you like Spicy Takes and Internet Leftist Memes or whatever, you might like this. I will admit to enjoying the parts where they make fun of weird startups, though. Episode rec: &lt;a href="https://trashfuturepodcast.podbean.com/e/mit-media-lab-after-dark-part-3-we-just-make-boxes-here-feat-sarah-taber/"&gt;MIT Media Lab After Dark Part 3: We Just Make Boxes here feat. Sarah Taber&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://srslywrong.com/"&gt;Srsly Wrong&lt;/a&gt; is a "utopian leftist comedy podcast", and it's my favourite podcast. The first time I listened to it I couldn't figure out what was going on because the format is "mostly serious discussion of ideas interspersed with bizarre sketch comedy". What I really like about this podcast is more the underlying ideology espoused by the hosts rather than the format of the podcast itself (contrast this to Reply All, where the content is somewhat irrelevant but the experience of listening to the podcast is generally pleasant), although I also greatly enjoy the bizarre sketch comedy. I will write more about different ideas from this show, or some steps removed from this show, in due time. Discovering this podcast probably made my 2020. Episode recs: &lt;a href="https://srslywrong.com/podcast/189-library-socialism-usufruct/"&gt;Library Socialism and Usufruct&lt;/a&gt;, &lt;a href="https://srslywrong.com/podcast/217-trash/"&gt;Trash!&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This has been a non-exhaustive list of podcasts I have listened to in 2020. If you have suggestions for other podcasts you think I might enjoy, please let me know using normal communication channels.&lt;/p&gt;
&lt;p&gt;According to my podcast app (&lt;a href="https://www.pocketcasts.com/"&gt;Pocket Casts&lt;/a&gt;), I have spent 19 days and 21 hours listening to podcasts since November 2018, which is about 477 hours. Is that a large number? Compared to the amount of time I have spent playing Dota 2, the answer is no.&lt;/p&gt;</content><category term="podcasts"></category><category term="recommendations"></category><category term="politics"></category><category term="history"></category><category term="design"></category><category term="linguistics"></category><category term="science fiction"></category></entry><entry><title>new site for research</title><link href="/meta/2020-11-05-new-site.html" rel="alternate"></link><published>2020-11-05T00:00:00+00:00</published><updated>2020-11-05T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2020-11-05:/meta/2020-11-05-new-site.html</id><summary type="html">&lt;p&gt;I made this tweet several months ago:
&lt;a href="https://twitter.com/_hylandSL/status/1269620384908836865"&gt;&lt;img src="/images/2020/research_website.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(This website is now an elaborate mechanism for me to retweet myself.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In theory apeiroto.pe counts as a personal research page, but it's not of the "glorified CV" variety I was thinking about when I wrote the tweet.
When I started apeiroto …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I made this tweet several months ago:
&lt;a href="https://twitter.com/_hylandSL/status/1269620384908836865"&gt;&lt;img src="/images/2020/research_website.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(This website is now an elaborate mechanism for me to retweet myself.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In theory apeiroto.pe counts as a personal research page, but it's not of the "glorified CV" variety I was thinking about when I wrote the tweet.
When I started apeiroto.pe I liked the idea of being unashamedly &lt;em&gt;multifaceted&lt;/em&gt; (&lt;a href="https://en.wikipedia.org/wiki/Apeirotope"&gt;get it?&lt;/a&gt;), and I still do. But for the sake of those people who are only interseted in the facets of me which pertain to research, &lt;strong&gt;I've created a dedicated research-facing website: &lt;a href="https://sthy.land"&gt;sthy.land&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It's very sparse, and I don't 100% love the theme (it is of course also using Pelican), but it's good enough for now. Correspondingly, the "research" page on apeiroto.pe will link there from now on.&lt;/p&gt;
&lt;p&gt;This sort of post is usually accompanied by a promise to update more often. I refuse to make such a promise.&lt;/p&gt;</content><category term="pelican"></category><category term="webdev"></category><category term="research"></category></entry><entry><title>setting up mastodon with hometown</title><link href="/tips/2020-05-07-leafytown.html" rel="alternate"></link><published>2020-05-07T00:00:00+01:00</published><updated>2020-05-07T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2020-05-07:/tips/2020-05-07-leafytown.html</id><summary type="html">&lt;p&gt;This is a 'how to do something' sort of blog post, I might write about why later. The something is running a social network for my friends.&lt;/p&gt;
&lt;p&gt;The starting point is probably &lt;a href="https://runyourown.social/"&gt;run your own social&lt;/a&gt;, which covers a lot of the 'why' and some of the 'what'. Chances are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a 'how to do something' sort of blog post, I might write about why later. The something is running a social network for my friends.&lt;/p&gt;
&lt;p&gt;The starting point is probably &lt;a href="https://runyourown.social/"&gt;run your own social&lt;/a&gt;, which covers a lot of the 'why' and some of the 'what'. Chances are if you are reading this post you already know about it, but if not it's a good read and probably a prerequisite for caring about the rest of this post.&lt;/p&gt;
&lt;p&gt;In case it wasn't apparent from this website, I like running my own things. I decided that lockdown (&lt;em&gt;metadata: it has been approximately 55 days since I had a face-to-face conversation with someone I know&lt;/em&gt;) would be a good opportunity to find new ways of interacting with my friends. Maybe that's getting too far into the why, though. I grabbed another VPS (total VPS count: 3) and decided to set up a &lt;a href="https://docs.joinmastodon.org/"&gt;mastodon&lt;/a&gt; server running the &lt;a href="https://github.com/hometown-fork/hometown"&gt;hometown&lt;/a&gt; fork.&lt;/p&gt;
&lt;p&gt;The basic gist of running mastodon with hometown is to follow two sets of instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.joinmastodon.org/admin/prerequisites/"&gt;the mastodon server installation guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hometown-fork/hometown/wiki/Initial-migration"&gt;the migrating from mastodon to hometown doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is basically comprehensive but I still messed it up the first time, so in this post I will walk through the steps with some inane recipe-tier flavour-text on the side.
Also, the hometown migration doc warns that it's not for beginners, so maybe I can help pseudo-beginners? Semi-nerds like myself still deserve autonomous social networks.&lt;/p&gt;
&lt;h3&gt;Step 1: Procure a server&lt;/h3&gt;
&lt;p&gt;My existing VPSes are with &lt;a href="https://www.heficed.com/"&gt;Heficed&lt;/a&gt;, which I knew as Host1Plus. I originally got a VPS with them many years ago because I wanted to run a Tor exit out of an African country, and they had a data centre in Johannesburg and didn't seem to care about the Tor exit thing.&lt;/p&gt;
&lt;p&gt;For reasons I price I opted for &lt;a href="https://www.ovh.co.uk/"&gt;OVH&lt;/a&gt; this time. In case it's useful, my VPS has 8GB of RAM and 160 GB of storage, and is running Ubuntu 18.04 (this is the recommended OS from the Mastodon guide). I put it in a London data centre, maybe one day I can visit it.&lt;/p&gt;
&lt;p&gt;Full disclosure: I initally only paid for one month of the VPS, then ignored 2 weeks worth of warning emails, and everything from I T E R A T I O N 1 got wiped. There was nothing I could do, in such an unprecedented time. In the current iteration I've paid for a year up front. This isn't immensely cheap (it cost about 200 pounds), but I'm one of the lucky ones right now because I have a job. Maybe if it takes off I'll ask for donations from the users.&lt;/p&gt;
&lt;h3&gt;Step 2: Install Mastodon version 2.9.3&lt;/h3&gt;
&lt;p&gt;This is where we approximately follow the &lt;a href="https://docs.joinmastodon.org/admin/prerequisites/"&gt;mastodon installation instructions&lt;/a&gt;, but make sure to end up with version 2.9.3 installed. This is (at time of writing) necessary for the subsequent hometown migration*. &lt;em&gt;update from 30 minutes after I wrote that: the hometown wiki looks a bit out of date, there are versions of hometown that are aligned with recent versions of Mastodon e.g. &lt;a href="https://github.com/hometown-fork/hometown/releases/tag/v1.0.3%2B3.1.2"&gt;v3.1.2&lt;/a&gt;, but I don't want to rewrite this entire blog post so ???&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This was the hardest part because you have to deviate slightly from the provided instructions. I'm assuming you're looking at them (time of writing: 7th May 2020) and basically following along, so I don't have to reproduce all the steps here. By the time that page is out of date, this one will be too.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.joinmastodon.org/admin/prerequisites/"&gt;Preparing the machine&lt;/a&gt; is all fine.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.joinmastodon.org/admin/install/"&gt;Installing from source&lt;/a&gt; is fine &lt;em&gt;until&lt;/em&gt; it comes to installing Ruby.
They recommend this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; RUBY_CONFIGURE_OPTS=--with-jemalloc rbenv install 2.6.6
&amp;gt; rbenv global 2.6.6
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This version of Ruby is too new! I went for version 2.6.1. I don't remember how I figured out this version would work, a dream maybe?&lt;/p&gt;
&lt;p&gt;Postgres installation instructions are all fine. Be careful if you already have the remnants of failed installations lying around (who would have that?!) with misleading table names. In a way, losing everything to the big VPS wipe was a gift.&lt;/p&gt;
&lt;p&gt;Vigilance is once again required while checking out the Mastodon codebase. They tell you to do this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/tootsuite/mastodon.git live &amp;amp;&amp;amp; cd live
git checkout $(git tag -l | grep -v &amp;#39;rc[0-9]*$&amp;#39; | sort -V | tail -n 1)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That second line is trying to check out the most recent version of Mastodon, and we're not here for that. I cheated by taking the commit hash for v2.9.3 from the Hometown instructions, which is gonna give us this instead:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/tootsuite/mastodon.git live &amp;amp;&amp;amp; cd live
git checkout 06f906acace5770fc10f333a203b036c5b72c849
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can sanity-check this has produced the correct version with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git describe --tags
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where it will dutifully give you &lt;code&gt;v2.9.3&lt;/code&gt;, probably.&lt;/p&gt;
&lt;p&gt;The rest of the Mastodon instructions are basically as-is. &lt;/p&gt;
&lt;p&gt;Before you do the interactive setup wizard&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;RAILS_ENV=production bundle exec rake mastodon:setup
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;you should probably do step 1.5, though.&lt;/p&gt;
&lt;h3&gt;Step 1.5: Get a domain and mail server&lt;/h3&gt;
&lt;p&gt;The Mastodon server running on the VPS needs some way of being known to the outside world, and domain names are a recent technology purporting to improve on the otherwise-flawless system of memorising ipv4 addresses.&lt;/p&gt;
&lt;p&gt;I use &lt;a href="https://www.namecheap.com/"&gt;Namecheap&lt;/a&gt; for my domains (including apeiroto.pe and my Mastodon server). The basic idea is you rent a domain from them, because they're internet landlords I guess. Once you own your domain you can update its A Record (for me this is in the domain management page) to include the IP address of your VPS.&lt;/p&gt;
&lt;p&gt;I'm including 'get a mail server' in here because I did that with Namecheap as well. I briefly considered running my own mail server on the VPS, but I'm not at that level of running my own things. Not yet.
During the mastodon setup wizard it will ask for various details of your mail server, which should be available from whatever provider you go with (e.g SMTP address, username and password and such). I'm like 30% sure the way I set this up has left it open to harvesting by spambots, so I'm not going to give any further advice in case the spambots find me.&lt;/p&gt;
&lt;p&gt;The reason you are giving it an email address is so it can send things like Forgot Password? emails to your users.&lt;/p&gt;
&lt;h3&gt;Step 2.05: Finish step 2 and then check&lt;/h3&gt;
&lt;p&gt;Before proceeding, go to your new domain in the browser and check that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mastodon is working&lt;/li&gt;
&lt;li&gt;mastodon is version 2.9.3 (near the bottom of the page)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You could stop here and be content with your Mastodon server running a slightly out of date version of Mastodon. You could be happy.&lt;/p&gt;
&lt;h3&gt;Step 3: Migrate to hometown&lt;/h3&gt;
&lt;p&gt;Now we refer back to &lt;a href="https://github.com/hometown-fork/hometown/wiki/Initial-migration"&gt;the hometown wiki page on migrating from mastodon&lt;/a&gt; mentioned before.&lt;/p&gt;
&lt;p&gt;Because we ensured to install version 2.9.3 before, this should mostly Just Work.
The only slightly odd thing for me was getting this error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; git fetch --tags
&amp;gt; git merge v1.0.1+2.9.3
merge: v1.0.1+2.9.3 - not something we can merge
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This might be some git config issue but changing to&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git fetch --tags -all
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and then merging as described fixed it.&lt;/p&gt;
&lt;p&gt;Following the rest of instructions as they are should result in your Mastodon server now including a mention of hometown at the bottom, indicating that it works. &lt;/p&gt;
&lt;h3&gt;Step 4 (optional): Make local-only posting default&lt;/h3&gt;
&lt;p&gt;For me, the reason to use hometown was that sweet local-only posting, but it's still possible for users to post publicly. Following all the above steps, it ended up that public (or &lt;em&gt;federated&lt;/em&gt; to be more specific) posting was the default. Having to toggle local-only on every tweet (I refuse to say toot because it sounds stupid as hell) was a pain and could easily result in accidental public posts. Thus began a saga of trying to figure out how to change this behaviour.&lt;/p&gt;
&lt;p&gt;I don't know much Javascript and I know even less Ruby, so after some fruitless attempts to reverse-engineer the codebase I got some friends involved, who are faster at reverse-engineering and who did not repeatedly pause their progress with games of starcraft 2. Eventually (like, four hours later) Paddy realised that this is just an option you can set in a config file:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/hometown-fork/hometown/blob/hometown-dev/config/settings.yml#L20"&gt;here is the line&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In case of link rot, this is the &lt;code&gt;settings.yml&lt;/code&gt; file in the &lt;code&gt;config&lt;/code&gt; subfolder of the repository. The config option is &lt;code&gt;default_federation&lt;/code&gt;, and you want it to be &lt;code&gt;false&lt;/code&gt; for local-only to be default. I'm looking forward to discovering what the other options do.&lt;/p&gt;
&lt;h3&gt;Step 5: Give back to community&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;... by using acquired wisdom to update their documentation (maybe)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;While double-checking things for this post I realised that the hometown installation document seems to be out of date.&lt;/p&gt;
&lt;p&gt;Remember how we painstakingly made sure we had v2.9.3 of Mastodon installed, so we could use hometown? It looks like there's already &lt;a href="https://github.com/hometown-fork/hometown/releases/tag/v1.0.3%2B3.1.2"&gt;hometown v1.0.3 for mastodon v3.1.2&lt;/a&gt; so that was actually unnecessary and we could have just installed whatever Mastodon version we wanted. Anyway I hope you enjoyed this blog post, I'm going to go mass queens &lt;a href="https://starcraft2.com/en-gb/news/23405326"&gt;before they get nerfed&lt;/a&gt;.&lt;/p&gt;</content><category term="sysadmin"></category><category term="mastodon"></category><category term="social media"></category><category term="vps"></category></entry><entry><title>the urgency of slowness</title><link href="/life/2019-05-19-the-urgency-of-slowness.html" rel="alternate"></link><published>2019-05-19T00:00:00+01:00</published><updated>2019-05-19T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2019-05-19:/life/2019-05-19-the-urgency-of-slowness.html</id><summary type="html">&lt;p&gt;My parents are not originally from Dublin. My father came to Ireland from Chile in the 70s, and my mother came from Wicklow a bit later. In both cases the rest of their family stayed behind for the most part, so visiting family has always meant travelling. My family regularly …&lt;/p&gt;</summary><content type="html">&lt;p&gt;My parents are not originally from Dublin. My father came to Ireland from Chile in the 70s, and my mother came from Wicklow a bit later. In both cases the rest of their family stayed behind for the most part, so visiting family has always meant travelling. My family regularly made the 18-odd hour journey from Dublin to Santiago during my youth. I've since learned that regular transcontinental travel is not the norm, and you don't get free food on short-haul flights.&lt;/p&gt;
&lt;p&gt;Driving down to Wicklow we used to pass through the Glen of the Downs. In the Glen, environmental and political activists demonstrated in the forest and painted messages on the wall overlooking the valley. I grew up wanting to be an eco-warrior like them. I wanted to live in a tree in the hills, with a valley to protect. Along the way I forgot about living in a tree and I got caught up in normal life, a life lived safely within the boundaries of social acceptability. Only once have I appeared on television during a protest about anything. In the light of the (semi-)recent IPCC report about the increasingly catastrophic outlook for the planet's climate, I wonder if I and others like me will be looked back at with scorn. Scorn for our inaction, our unwillingness to sacrifice anything, our complacency in the face of unfolding disaster.&lt;/p&gt;
&lt;p&gt;I try not to be complacent. In the calculus of personal responsibility for climate change, I think I fare better than others in my situation. I'm going on 8 and a half years of vegetarianism. I don't drive: I don't actually know how to drive. Privileged as I am to live in cities, I cycle, walk, or take public transport everywhere. I donate to environmental charities. I don't buy new things very often, I avoid plastic. Every time I brush my teeth, every time I wash dishes, I think about water conservation. I don't leave the light on. I hang my clothes out to dry. But I am still a (north-western) European, and I'm well-off. The comforts of my life come at a premium that I will barely have to pay. I take warm showers, I enjoy continuous electricity and high-end consumer devices. And I fly. A lot.&lt;/p&gt;
&lt;p&gt;In June of 2013, I thought of myself as well-travelled. I had been to Chile countless times; once to Easter Island, and another time to the Atacama desert. I had been to California and Florida, Turkey and Vietnam. When I decided to move to the USA for a PhD I knew it was a more serious kind of emigration than moving to England had been. But at the time I had not deeply thought about being a scientist as well as an emigrant. Between conference travel and visiting home and family in Europe, over those 5.4 years my propensity for air travel increased precipitously. I started writing this en route to South Africa in January 2019 (it's been in the outbox for a while). Between November 2018 and February 2019 I will have been in Zurich (home), San Francisco, New York, Montreal (via Geneva), Dublin (other home), London, Cape Town (via Johannesburg, Paris, George, and Amsterdam), and Mexico City. That's four transcontinental trips in as many months. This would have been unthinkable to a teenage version of myself. One acclimates quickly.&lt;/p&gt;
&lt;p&gt;The problem is that flying is very convenient. Flying is unbelievably fast. Flying allows you to convert money into time, in what is so obviously an excellent trade that anyone with the means would surely be foolish to turn it down. This speed opens up the possibility of spending the weekend in another country and returning for work as usual on Monday. Going to the other side of the planet becomes inconvenient only because 12 hours on a plane can become uncomfortable. The entertainment system might stop working, the crying of a child may interrupt already fitful sleep at 35,000 feet, and the food may be bland or unrecognisable. The idea that air travel is damaging in some abstract way to the environment does not practically feature into the considerations one makes around time, money, layovers, legroom, screaming children, discarded water bottles, city views, packing. There is a sense that flying might be contributing somehow to climate change, but for an extra 10 euro you can offset your carbon, and climate change is sort of not obviously happening right now, and there's still some time left, and anyway it's those top 100 companies that are really causing it, so it's probably fine. You move on to seat selection: windows are good for long trips because you can lean against the wall, but what if you want to go to the bathroom?&lt;/p&gt;
&lt;p&gt;And so it goes.&lt;/p&gt;
&lt;p&gt;In the winter of 2018 I finally got to take the "Adirondack" between Montreal and New York. This is a passenger train route which travels through the Hudson valley and the Adirondack mountains, and it takes about 11 hours end to end. I had previously thought about it in 2015 when I was still living in New York and Montreal was hosting a popular conference in my field. In the end I decided I couldn't afford to add two days of travel to the trip, even if those days could be spent preparing for the conference. I figured any such train-based preparation would be inferior to what I could do from my office, so the most efficient solution would be to minimise travel time. Besides, the money wasn't mine, and everyone else was flying. In December 2018 the conference came back to Montreal, and my PhD defense required me to return to New York, so I killed two non-metaphorical endangered species with one stone and made the transatlantic trip. This time, when the opportunity arose, I took the train.&lt;/p&gt;
&lt;p&gt;I'm not a stranger to long rail journeys. Like many middle-class Europeans, I spent one intense summer travelling our small continent on trains of varying speed and quality. These at-times ten-hour journeys were spent eating illicit sandwiches smuggled from hostel breakfasts, debating still-intractable philosophical positions, and otherwise smoothly blurring into the rest of the trip. So I was not worried - I knew I could entertain myself, even without friends on hand to practice Hungarian with. We left Montreal at 10am and arrived in New York's Penn station after 9pm. Over eleven hours I watched the landscape change almost imperceptibly slowly. The urban area of Montreal slipped away as the snow receded. Frozen lakes cracked and melted, and bare trees gave way to the distant brown-green valleys of Vermont. I saw tree swings and tree houses and a pair of wooden chairs on a hill overlooking the river. I thought about life in these places. I speculated. After five years under the abstract sense of panic I attribute to doing a PhD, I basked in the luxury of time. For eleven hours, I didn't really do anything. I didn't try to pass the time, nor did I clamour to use it. I sat, mostly, gazing out the window as the sun moved across the sky.&lt;/p&gt;
&lt;p&gt;Choosing to fly is a complex decision. Due to economic and political forces I have yet to understand, it's often the cheaper option, and not everyone has the same abundance of time as an academic who just finished her PhD, nor the same willingness to spend ten hours looking at variations on wintry landscapes. I can't ask people to stop flying, I wouldn't even ask it of myself. But I have come to realise that the idea that flying turns money into time is an elaborate self-deception. You can't buy time. You can only (if you are lucky) choose how to spend it. Spending it in paradoxically-monotonous bursts of speed seems to be the default choice for anyone with the money and the inclination. It was for me, but I'm done. I'm done with priding myself on my tolerance for complicated itineraries, I'm done with "at least you can watch movies!", with habitually storing my toiletries in a plastic bag. I'm done with finding the view above the clouds unremarkable. I'm done with the urgency of flying, the devaluation of time spent doing little. I'm done with a culture of treating environmental issues as sources of guilt and little else, a carbon offset to be purchased, and a hope that everything will get better by someone else's hand. Moving slowly might not save the world, but at least I'll see the trees while they last.&lt;/p&gt;</content><category term="environment"></category><category term="travel"></category><category term="phd"></category><category term="trains"></category></entry><entry><title>yes, but did it work? evaluating variational inference</title><link href="/ml/2018-06-03-yes-but-did-it-work-vi.html" rel="alternate"></link><published>2018-06-03T00:00:00+01:00</published><updated>2018-06-03T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2018-06-03:/ml/2018-06-03-yes-but-did-it-work-vi.html</id><summary type="html">&lt;p&gt;This post is about the paper &lt;a href="https://arxiv.org/abs/1802.02538"&gt;Yes, but Did It Work?: Evaluating Variational Inference&lt;/a&gt; by &lt;em&gt;Yuling Yao, Aki Vehtari, Daniel Simpson, Andrew Gelman&lt;/em&gt;, which will appear at ICML 2018. I'm going to try to summarise/explain the paper in my own words, largely for my own benefit. I'm also going …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is about the paper &lt;a href="https://arxiv.org/abs/1802.02538"&gt;Yes, but Did It Work?: Evaluating Variational Inference&lt;/a&gt; by &lt;em&gt;Yuling Yao, Aki Vehtari, Daniel Simpson, Andrew Gelman&lt;/em&gt;, which will appear at ICML 2018. I'm going to try to summarise/explain the paper in my own words, largely for my own benefit. I'm also going to do this without writing any mathematical formulae, because I don't remember how to do LaTeX with my website, and I don't feel like shaving that particular yak right now.&lt;/p&gt;
&lt;p&gt;After the accepted ICML papers were announced, I went through it hunting for relevant work. I've decided it's a better use of my time to read papers that have been accepted somewhere, rather than drowning under the firehouse of my arXiv RSS feed. This paper ticked two boxes: variational inference, and knowing if it worked. It also ticked a third, secret box of "titles that make it sound like the paper will have been written in a casual, conversational style, eschewing the tradition of appearing smarter by obfuscating the point".&lt;/p&gt;
&lt;p&gt;Single-line summary: &lt;strong&gt;they describe two diagnostics for evaluating the variational posterior, with different properties and use-cases&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So let's talk about these two diagnostics.&lt;/p&gt;
&lt;h3&gt;Pareto Smoothed Importance Sampling (PSIS)&lt;/h3&gt;
&lt;p&gt;At this point I realised the auther overlap between this paper and &lt;a href="https://arxiv.org/abs/1507.04544"&gt;Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC&lt;/a&gt;, which in turn builds on &lt;a href="https://arxiv.org/abs/1507.02646"&gt;Pareto Smoothed Importance Sampling&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So what is PSIS and how is it useful for evaluating VI?&lt;/p&gt;
&lt;p&gt;Importance sampling is a technique which enables us to estimate expectations under a distribution which is difficult to sample from (the target distribution), using an approximating &lt;em&gt;proposal distribution&lt;/em&gt;. You sample from your proposal distribution (which is easy to sample from), then &lt;em&gt;weight&lt;/em&gt; those samples by the ratio of target distribution to the proposal distribution (evaluated at the sample point). These weights are called importance ratios.&lt;/p&gt;
&lt;p&gt;Pareto smoothing comes in because in the event that the proposal distribution is a poor fit to the target distribution, these weights can have a very high variance. The proposal distribution is the denominator in the importance ratio, so if you imagine that this distribution is a lot thinner than the target distribution - that is, it's near zero in regions where the target distribution is not, you can end up with some very large importance ratios - high variance. This means that you would need a &lt;em&gt;lot&lt;/em&gt; of samples to estimate the expectation value of interest. Pareto smoothing is a way to control this variance. It builds on the idea of simply truncating the importance ratios (&lt;a href="https://experts.umich.edu/en/publications/truncated-importance-sampling"&gt;Truncated Importance Sampling, Ionides 2008&lt;/a&gt;) by instead fitting a Pareto distribution to them.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Side note: Part of the motivation of using the Pareto distribution at this point, I think, is to use its fitted parameters to do&lt;/em&gt; diagnostics &lt;em&gt;on the proposal distribution. This is exactly what "Yes, But Did It Work?" is doing, but they already talk about it in the original PSIS paper, so I guess part of the novelty of this ICML paper is bringing it explicitly to the VI area. More about VI when I'm done with this Pareto stuff.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So how does fitting a Pareto distribution to the importance ratios help? In practice, you fit the Pareto distribution, and then instead of simply &lt;em&gt;truncating&lt;/em&gt; the top M importance ratios (M is chosen empirically/arbitrarily) you &lt;em&gt;replace&lt;/em&gt; them using the inverse cumulative density function of the Pareto distribution you fit. This replacement operates on the ranks of these importance ratios (so the smallest of the M, the second-smallest and so on), replacing those with what you'd get in a Pareto distribution ranked by CDF. This reminds me of rank-based inverse-normal transformations I've seen used in genetics (weirdly difficult to find papers about this, &lt;a href="https://cran.r-project.org/web/packages/RNOmni/vignettes/RNOmni.html"&gt;here&lt;/a&gt; is an R vignette). They argue that this produces an IS estimate that is less biased than what you get using truncated IS. Moreover, you can inspect the parameters of the fitted Pareto distribution to do diagnostics.&lt;/p&gt;
&lt;p&gt;The reason they use a &lt;em&gt;Pareto&lt;/em&gt; distribution to model the top M importance ratios is because It Is Known. Rather, it is shown in &lt;a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1176343003"&gt;Pickands, 1975&lt;/a&gt; to be an appropriate choice. To be specific, they use a &lt;em&gt;generalised&lt;/em&gt; Pareto distribution. This distribution has three parameters (location, scale, shape), and it has the property that it has finite moments up to order 1/k, where k is the shape parameter. That means that if k &amp;gt; 0.5, the variance of the importance ratios is infinite, but if k &amp;lt; 1 at least the mean exists. They point to 0.5 &amp;lt; k &amp;lt; 0.7 as a regime where the importance sampling procedure exhibits a practicaly useful convergence rate.
Side note: I don't quite see where the jump from modelling the variance of the &lt;em&gt;tail&lt;/em&gt; of the importance ratios to modelling &lt;em&gt;all&lt;/em&gt; the importance ratios happened. I suppose if you observe that your tail has a finite variance, then you must have finite variance in the rest of the values, but I would have expected an additional step to extend the conclusions made about the fit of the Pareto distribution to the rest of the importance ratios.&lt;/p&gt;
&lt;p&gt;Now, relating this back to variational inference is straight forward: replace "target distribution" with "variational posterior". PSIS, via the shape parameter of the fitted Pareto distribution, gives us a diagnostic for how well the variational posterior fits with the true posterior.&lt;/p&gt;
&lt;p&gt;But wait... don't we need the true posterior to calculate the importance ratios? Isn't this circular? The answer is that you can use the &lt;em&gt;joint&lt;/em&gt; distribution (p(z, x) rather than p(z|x)) because the estimate of k is invariant to a constant multiplicative factor, which will be p(x).&lt;/p&gt;
&lt;p&gt;The diagnostic approach is thus:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run VI, get variational distribution q(z) approximating p(z|x).&lt;/li&gt;
&lt;li&gt;Sample a bunch of zs from q(z)&lt;/li&gt;
&lt;li&gt;Calculate p(z, x) for all the zs (remember, x is known - it is a specific dataset), and get the importance ratios p(z, x)/q(z)&lt;/li&gt;
&lt;li&gt;Fit a generalised Pareto distribution to the largest M importance ratios&lt;/li&gt;
&lt;li&gt;Report the (estimated) shape parameter k&lt;/li&gt;
&lt;li&gt;If k &amp;gt; 0.7, the VI approximation is not reliable&lt;/li&gt;
&lt;li&gt;If k &amp;lt; 0.5, the VI approximation is good, and PSIS can additionally be used to calculate further divergence measures&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;They touch on two other points in this paper, regarding PSIS:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The shape parameter k is invariant under reparametrisation, but reparametrisation can influence the VI procedure and produce better/worse proposal distributions. So looking at k can help guide reparametrisation efforts&lt;/li&gt;
&lt;li&gt;Marginal PSIS diagnostics are not useful. These marginal diagnostics would be doing the above procedure, but instead of sampling full zs, sampling only 1 dimension at a time. Compared to PSIS diagnostic evaluated from the joint distribution, these marginal ks are never larger (usually smaller) than k, and can be misleading. Also, this means you need access to the marginal distribution p(z_i, x) (or p(z_i | x)) to get the importance ratios, which may be unavailable. So don't do it.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Variational Simulation-Based Calibration Diagnostic (VSBC)&lt;/h3&gt;
&lt;p&gt;The PSIS diagnostic looks at the &lt;em&gt;full&lt;/em&gt; approximate posterior. However, sometimes you don't need to properly approximate the full posterior, and can get away with producing useful point estimates. VSBC evaluates the quality of point estimates. It is based on &lt;a href="http://www.stat.columbia.edu/~gelman/research/published/Cook_Software_Validation.pdf"&gt;Validation of Software for Bayesian Models Using Posterior Quantiles&lt;/a&gt; (Cook, 2006).&lt;/p&gt;
&lt;p&gt;The key observation from (Cook et al., 2006) is going to be fun to explain with no proper equations. Let's try: suppose we have access to p(z), p(x|z) and p(z|x) (this will be approximated by p(z) shortly). We simulate an x by first sampling from z, then p(x|z). Now, we then sample multiple z's from p(z|x). We can ask what fraction of those sampled z's are smaller than the original z - we call this the calibration probability. Now, if we were to do this multiple times (picking a z, then x, then multiple resampled z's) we would get a distribution of calibration probabilities. And that should be uniform. I &lt;em&gt;think&lt;/em&gt; this is Cook's observation.&lt;/p&gt;
&lt;p&gt;So to relate this to VI, we can perform the above procedure, replacing the true posterior p(z|x) with the approximate posterior q(x). (This means we have to do a full VI step for each dataset x we sample!) We could then in principle ask how far the distribution of calibration probabilities deviates from normal, but in this paper they suggest (following on from other literature) to instead measure how &lt;em&gt;asymmetric&lt;/em&gt; this probability is.&lt;/p&gt;
&lt;p&gt;Thus, the VSBC diagnostic is to test for asymmetry in the distribution of calibration probabilities. They do this using a Kolmogorov-Smirnov test between the distribution of probabilities and one minus that distribution. More specifically, they actually focus on &lt;em&gt;marginal&lt;/em&gt; probabilities - so where I said 'z' above, imagine this is one dimension of z. Thus, they look at marginal calibration probabilities. This is necessary because z &amp;lt; z' only makes sense for scalars.&lt;/p&gt;
&lt;p&gt;So running the diagnostic means running VI multiple times over simulated datasets. If your generative model of the data is poor, this diagnostic won't tell you much about how your VI scheme will work on real data, or indeed on a given instance of real data, since VSBC gives average performance. An advantage of VSBC over PSIS is that it looks at marginals, so you can potentially identify &lt;em&gt;which&lt;/em&gt; dimensions in z are problematic during fitting.&lt;/p&gt;
&lt;h3&gt;Applications, etc.&lt;/h3&gt;
&lt;p&gt;Given these two diagnostics, they then show how they can be used in a couple of different settings - Bayesian linear regression, logistic regression, a hierarchical model (the famous Eight-School model), and a cancer classification application. In all cases, they use mean-field Gaussian automatic differentiation variational inference.&lt;/p&gt;
&lt;p&gt;The big question for me and probably a lot of other users of variational inference is how well these can be applied to the types of posteriors we try to approximate using hideous neural networks. VSBC may be computationally impractical because it puts the whole VI procedure inside an inner loop, although it's easily parallelisable. High-dimensional posteriors are problematic for importance sampling and thus PSIS, although I don't know what "high" is - 10, 100? 1000??  Multimodality in the posterior is also a challenge, as they point out in the discussion - the VI approximation could completely miss a mode, but the PSIS diagnostic would nonetheless indicate all is well. They suggest to use PSIS to evaluate some other divergence (such as a KL divergence) to diagnose this case.&lt;/p&gt;
&lt;p&gt;In summary, this has been a post about evaluating variational inference using two diagnostics - Pareto-smoothed importance sampling, and variational simulation-based calibration. At its core this paper feels like an application of previous/existing work to a slightly new domain (variational inference). I'm curious to try these diagnostics on my own variational posteriors. Code is seemingly available (maybe just for PSIS) - R package (&lt;a href="https://cran.r-project.org/web/packages/loo/index.html"&gt;loo&lt;/a&gt;), and also a &lt;a href="https://github.com/avehtari/PSIS"&gt;Python/Matlab port&lt;/a&gt;.&lt;/p&gt;</content><category term="papers"></category><category term="variational inference"></category><category term="evaluation"></category></entry><entry><title>NIPS 2017</title><link href="/ml/2017-12-20-nips2017.html" rel="alternate"></link><published>2017-12-20T00:00:00+00:00</published><updated>2017-12-20T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2017-12-20:/ml/2017-12-20-nips2017.html</id><summary type="html">&lt;p&gt;&lt;em&gt;I'm continuing my tradition of summarising conferences I attend. Previous posts: &lt;a href="/ml/2016-12-16-nips2016_b.html"&gt;NIPS 2016&lt;/a&gt;, &lt;a href="/ml/2015-12-14-nips2015.html"&gt;NIPS 2015&lt;/a&gt;, &lt;a href="/ml/2016-02-17-aaai2016.html"&gt;AAAI 2016&lt;/a&gt;, &lt;a href="/ml/2016-07-05-icml2016.html"&gt;ICML 2016&lt;/a&gt;. I also went to AAAI 2017 to present my &lt;a href="https://arxiv.org/abs/1607.04903"&gt;work on unitary recurrent neural networks&lt;/a&gt;, but didn't write a summary.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This was my third time attending NIPS, but my first time …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;I'm continuing my tradition of summarising conferences I attend. Previous posts: &lt;a href="/ml/2016-12-16-nips2016_b.html"&gt;NIPS 2016&lt;/a&gt;, &lt;a href="/ml/2015-12-14-nips2015.html"&gt;NIPS 2015&lt;/a&gt;, &lt;a href="/ml/2016-02-17-aaai2016.html"&gt;AAAI 2016&lt;/a&gt;, &lt;a href="/ml/2016-07-05-icml2016.html"&gt;ICML 2016&lt;/a&gt;. I also went to AAAI 2017 to present my &lt;a href="https://arxiv.org/abs/1607.04903"&gt;work on unitary recurrent neural networks&lt;/a&gt;, but didn't write a summary.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This was my third time attending NIPS, but my first time attending NIPS with jetlag. The advantage of jetlag is that it provides a topic of small talk less agonisingly self aware than the weather (weather readily avoided by waking up at 6am). The downside of jetlag is me standing glassy-eyed in front of a poster, trying to formulate intelligent thoughts but just yawning really, really obviously. &lt;/p&gt;
&lt;p&gt;After a few days of complaining about the jetlag I realised I was probably exhausted because &lt;em&gt;NIPS is exhausting&lt;/em&gt;. The problem is early mornings, listening to talks, bumping into people I know, talking to people I don't know, having meetings, talking to recruiters, talking over dinner, going to poster sessions, talking at posters, finding people who I had previously talked to as strangers but who are now acquaintances and talking to them again, and so on. Having gone twice before did not teach me moderation, and I was hoarse by Thursday. I also experienced an interesting fluctuation in my desire to do research, which I have depicted in the following graph: &lt;em&gt;(enthusiasm has since returned, luckily)&lt;/em&gt;&lt;/p&gt;
&lt;figure style="display: block; float: none; margin: auto; width: 50vw;"&gt;
&lt;img src="/images/2017/nips/graph.jpg" style="width: 35vw;"&gt;
&lt;figcaption&gt;
Figure 1: We observe that research enthusiasm of the PhD student is a nonlinear function of Days of NIPS (dNIPS), with two local maxima attained towards the ends of day 3 and 5. Data beyond day 6 could not be reliably collected due to hostility from the test subject.
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This analysis clearly indicates that the optimal length of NIPS (including tutorials and workshops) is three days. Recent work (private communication) suggests that "taking breaks" can prolong the research-excitement peak, sustaining the individual beyond the end of the conference, providing hope for 2018. When I got back to Zurich I slept for 7 hours, arose for 7 (during which time I did a roller derby exam, but that's another blog post), then went back to bed for another 10. My body had no idea what was going on.&lt;/p&gt;
&lt;p&gt;As in 2016, I'll organise this by topic. This post is rather long, but each section is largely independent so feel free to pick and choose.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#tutorials"&gt;Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#wiml"&gt;Women in Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Main Conference&lt;ul&gt;
&lt;li&gt;&lt;a href="#invited"&gt;Invited Talks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#spotlights"&gt;Spotlights and Orals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#posters"&gt;Posters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#ml4h"&gt;Machine Learning for Health&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a name="tutorials"&gt;&lt;/a&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;The first day of WiML actually coincided with the tutorials, so I was only able to attend those in the morning. I went to &lt;a href="https://optimaltransport.github.io/"&gt;A Primer on Optimal Transport&lt;/a&gt;. I then got &lt;a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases#Frequency_illusion"&gt;Baader-Meinhof'd&lt;/a&gt; about it for the rest of the conference.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/nips/skating.png" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;I was twenty minutes late to the tutorial. This is decided to commute to the conference on roller skates (see frame from snapchat selfie video, right), and on the first day I misjudged how long it would take (my Airbnb was about 3 miles away). &lt;/p&gt;
&lt;p&gt;Unfortunately missing the start of a tutorial, especially a mathematical tutorial, can be fatal. I arrived in the middle of an explanation of how Kantorovich's formulation of optimal transport relates to Monge's formulation and I had no reference for what was going on. I tried to sneakily look things up on Wikipedia to catch up, but all was lost and I came away from the tutorial with only an intuitive understanding of the optimal transport problem, and that Wasserstein barycentres are better than l2 averages for combining distributions, usually. In case you missed it, here are the &lt;a href="https://www.dropbox.com/s/55tb2cf3zipl6xu/aprimeronOT.pdf?dl=0"&gt;slides (pdf)&lt;/a&gt;. I said to myself that I'd go and learn optimal transport real quick and give a coherent summary of it here, but I also want to write this post before I graduate.&lt;/p&gt;
&lt;h2&gt;&lt;a name="wiml"&gt;&lt;/a&gt;Women in Machine Learning Workshop&lt;/h2&gt;
&lt;p&gt;WiML took place on the tutorial day (Monday), and also on symposia day (Thursday). I am not sure why they split it up like this.&lt;/p&gt;
&lt;p&gt;Last year I said that 15% of the 6000 NIPS attendees were women. I don't recall them releasing statistics about attendee demographics this year, but apparently 10% of unique authors amongst submissions were women (amongst accepted submissions? unknown), so the gender situation in ML is still pretty dire. Fixing this is a hard problem and not really my area of expertise (except for what I know from invariably being involved in conversations about Women in STEM), but I'm pretty sure events like this help. Why do I think that? Well, this year was the first instance of the &lt;a href="https://blackinai.github.io/"&gt;Black in AI&lt;/a&gt; workshop, and while I didn't attend (I was at the healthcare workshop), even seeing people tweeting about it made me way more aware of the work being done by Black researchers. So hopefully WiML also alerts people to the existence of good work getting done by women. Oh, and travel grants! I could imagine in this era of NIPS-selling-out-rapidly that pre-purchasing tickets to redistribute to minority groups could also play a part in promoting diversity. Weird to think of women as minority group, but apparently we only comprise &lt;a href="https://data.worldbank.org/indicator/SP.POP.TOTL.FE.ZS"&gt;49.58%&lt;/a&gt; of the world's population these days.&lt;/p&gt;
&lt;h3&gt;Interesting talks/posters:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(contributed talk) &lt;a href="https://www.linkedin.com/in/peyton-greenside-298a5881"&gt;Peyton Greenside&lt;/a&gt; spoke about using graph convolutional networks to model Hi-C and also ATAC-seq data. I wanted to talk to her at the poster session, and once again (&lt;a href="http://apeiroto.pe/ml/nips-2016.html"&gt;this happened last year too&lt;/a&gt;) her poster was on the other side of the board to mine. You can find her talk at 1:14 into &lt;a href="https://www.facebook.com/WomenInMachineLearning/videos/1956846487664316/"&gt;this video&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(invited talk) &lt;a href="http://www.cs.mcgill.ca/~jpineau/"&gt;Joelle Pineau&lt;/a&gt; spoke about &lt;strong&gt;Improving health-care: challenges and opportunities for reinforcement learning&lt;/strong&gt;. The talk focused on &lt;em&gt;slow and small research&lt;/em&gt;: research with small sample sizes, acquired slowly. She spoke about designing treatment strategies for epilepsy, probably referencing this paper: &lt;a href="http://www.cs.mcgill.ca/~jpineau/files/panuccio-expneur13.pdf"&gt;Adaptive Control of Epileptiform Excitability in an &lt;em&gt;in vivo&lt;/em&gt; Model of Limbic Seizures&lt;/a&gt; (or &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4884089/"&gt;this one&lt;/a&gt; but I can't find the PDF). The idea is that brain stimulation can prevent seizures (cool!), and you can use reinforcement learning to build a controller (controlling the frequency of applied stimulation) to achieve the same level of seizure control while minimising the required amount of stimulation. One lesson she highlighted from this work is that models (in the 'animal model' sense) are important (they use a seizure model from mouse brain cells, I think), and having existing baselines to build from also helps. She also described some work on bandits to do cancer treatment optimisation, which I think I actually already wrote about in my &lt;a href="http://apeiroto.pe/ml/icml-2016-not-by-the-day.html"&gt;ICML 2016 post&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(invited talk) &lt;a href="http://theory.stanford.edu/~nmishra/"&gt;Nina Mishra&lt;/a&gt; spoke about &lt;strong&gt;Time-Critical Machine Learning&lt;/strong&gt;. She spoke about anomaly detection on huge streams of data, using &lt;a href="http://proceedings.mlr.press/v48/guha16.html"&gt;Random Cut Forests&lt;/a&gt;, and she spoke about machine learning in medical emergencies (probably this paper: &lt;a href="http://theory.stanford.edu/~nmishra/Papers/timeCriticalSearch.pdf"&gt;Time-Critical Search&lt;/a&gt;). When faced with a medical emergency, people will ring the relevant emergency number, and then, a lot of people will turn to Google for help. This isn't always the most efficient way to get useful information, so they did some work on trying to detect (using search query and other metadata such as time, location, query history) whether or not a person was in an emergency situation, with the intention to give more relevant results. Someone asked if it wouldn't be easier to just make a special emergency-search app, but Nina pointed out that nobody wants to download an app in an emergency situation. (I do wonder if phones could come with such an app by default, but making that standard is a whole other challenge). She &lt;em&gt;did&lt;/em&gt; however describe a possible emergency app, which I think was called Samaritan (reminding me of the very cool &lt;a href="https://www.goodsamapp.org/"&gt;GoodSAM app&lt;/a&gt;), that guides a user through performing CPR. Part of the procedure involves putting the phone on the person's chest and using its accelerometer to guide CPR compressions. Nice use of ubiquitous smartphone tech.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Regarding the poster sessions, I spent all of the Monday session presenting my poster (see the Healthcare workshop below), and much of the Thursday session talking at my friend's poster (&lt;a href="https://arxiv.org/abs/1712.00643"&gt;Learning the Probability of Activation in the Presence of Latent Spreaders&lt;/a&gt;) and sneaking peeks at the &lt;em&gt;Interpretable Machine Learning&lt;/em&gt; symposium - video of a panel session &lt;a href="https://www.youtube.com/watch?v=kruwzfvKt3w"&gt;here&lt;/a&gt;, and video of the debate &lt;a href="https://www.youtube.com/watch?v=2hW05ZfsUUo"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Roundtables&lt;/h3&gt;
&lt;p&gt;As in previous years, the roundtables were one of the highlights of WiML for me. It's a great opportunity to meet senior scientists I might not otherwise be able to, and also to get to know some of the other WiML attendees.&lt;/p&gt;
&lt;p&gt;I ended up going to four tables - two career-based, two topic-based:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Choosing between academia and industry&lt;/em&gt; - I went to the same topic last year, but this time the table mentors were both in academia, so I got a somewhat different perspective. This is also a question I've spoken to people about and thought about, so I didn't learn much, but it's useful to have one's thoughts externally validated. The gist is that academia gives more freedom, at the cost of stability, potentially having to teach, having to supervise students, and having to &lt;s&gt;beg for money&lt;/s&gt; write grants. Not all of these are necessarily negatives - some people like teaching and supervising (nobody likes writing grants). Meanwhile, industry may limit research freedom, but provides &lt;em&gt;more&lt;/em&gt; stability, and (usually) freedom from having to run your own lab with all that entails.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Establishing collaborators/long-term career planning&lt;/em&gt; - the roundtable I attended wasn't especially enlightening on this topic, but the talk from &lt;a href="http://raiahadsell.com/index.html"&gt;Raia Hadsell&lt;/a&gt; touched on it, and gave some good long-term career advice. The advice was this (taken from one of her slides):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you like to go deep, make some room for novelty and risk.&lt;/li&gt;
&lt;li&gt;If you are a renaissance woman, try going deep.&lt;/li&gt;
&lt;li&gt;NIPS and WiML are &lt;em&gt;your&lt;/em&gt; community - be a participant.&lt;/li&gt;
&lt;li&gt;speak loudly. ask questions. be strong&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'd not self-identify as a 'renaissance woman' (I go for 'attempted polymath'), but I tend to aim for &lt;em&gt;multifaceted&lt;/em&gt; (see the name of this website), so the advice to go deep was hard to hear, and therefore useful. (I just love when people tell me things I don't want to hear, it's why I use twitter.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Generative models&lt;/em&gt; - a lot of this roundtable consisted of me discussing evaluation of GANs with Ian Goodfellow. This was a bit selfish because it's a topic of direct relevance to my &lt;a href="https://arxiv.org/abs/1706.02633"&gt;current work on recurrent GANs for medical data&lt;/a&gt; (see also below) and maybe less interesting to others. However, I also think evaluation is one of the most interesting GAN-related questions right now. There's understandably a lot of focus on the GAN objective and optimisation procedure, thinking about convergence and stability and so on, but optimisation without evaluation seems foolish.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Machine learning for healthcare&lt;/em&gt; - we discussed some of the big challenges facing MLHC, like data sharing, causality, and something else I've forgotten but lists should always contain three elements. I've not worked on causality before, but I'm increasingly aware of how causal reasoning (especially counterfactual reasoning) plays a role in trying to understand observational medical data. More about healthcare in the section on the healthcare workshop.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;The Main Conference&lt;/h1&gt;
&lt;h2&gt;&lt;a name="invited"&gt;&lt;/a&gt;Invited Talks&lt;/h2&gt;
&lt;figure&gt;
&lt;img src="/images/2017/nips/longbeach.jpg"&gt;
&lt;figcaption&gt;
Long Beach in December: not bad
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a href="https://research.google.com/pubs/JohnPlatt.html"&gt;John Platt&lt;/a&gt; spoke about &lt;strong&gt;Powering the next 100 years&lt;/strong&gt; (&lt;a href="https://youtu.be/L1jLpkvKPh0?t=1727"&gt;video&lt;/a&gt;), which was less environmentalist than I was hoping, and more about economics (also important, less exciting). He also spoke about nuclear fusion, which is very exciting, and possibly important (in the future). One issue I had with the premise of this talk is that I don't think we should be &lt;em&gt;trying&lt;/em&gt; to expand US power usage to the rest of the world - the US uses disproportionately much energy relative to other developed nations  (even with high standards of living, see also the &lt;a href="https://en.wikipedia.org/wiki/2000-watt_society"&gt;2000-watt society&lt;/a&gt;), so while it would be nice if we &lt;em&gt;could&lt;/em&gt;, I would personally rather focus on minimising our energy consumption until it is sustainable to consume more. But anyway, assuming the premise, they use machine learning to optimise both the economics of power usage, and for identifying promising (and safe) experiments to run on fusion reactors.&lt;/p&gt;
&lt;p&gt;I missed &lt;a href="http://www.psi.toronto.edu/~frey/"&gt;Brendan Frey&lt;/a&gt;'s talk about reprogramming the human genome, and &lt;em&gt;also&lt;/em&gt; &lt;a href="https://keysduplicated.com/~ali/"&gt;Ali Rahimi&lt;/a&gt;'s talk for the &lt;strong&gt;Test of Time Award&lt;/strong&gt;. I sorely regret missing the latter talk because people kept asking me about it. I had to wait until I got back to Zurich to rectify the matter, but having now watched it (available &lt;a href="https://www.youtube.com/watch?v=ORHFOnaEzPc"&gt;here&lt;/a&gt;), I get the fuss. &lt;/p&gt;
&lt;p&gt;So, regarding &lt;strong&gt;Rahimi's talk&lt;/strong&gt;: Yann LeCun quickly posted &lt;a href="https://www.facebook.com/yann.lecun/posts/10154938130592143"&gt;a response&lt;/a&gt;, and Ferenc Huszár posted &lt;a href="http://www.inference.vc/my-thoughts-on-alchemy/"&gt;another response&lt;/a&gt;, and I should make a separate blog post to add my incredibly important opinions on the matter, but I'll just cram them right in here. Ali Rahimi's talk claimed that much of machine learning these days is alchemy - people are building models that work, seemingly by magic, which we don't quite understand. As a relative newcomer (remember, only my third NIPS) I can't hark back to any golden days of rigour and understanding, but I can certainly say that the things he suggested - simple experiments, simple theorems - are appealing.&lt;/p&gt;
&lt;p&gt;My take: We should not make unsubstantiated claims in science. We should design experiments to test claims we make about our models, and we should not accept speculatory claims from others as fact. How often do papers today fail by these measures? Rahimi's talk implies this happens often enough to be worth calling out. I &lt;em&gt;feel&lt;/em&gt; like I have read papers which make unsubstantiated claims, or over-explain their results, or introduce poorly-defined concepts, but I can't recall any to mind, so my claim must remain purely &lt;em&gt;speculative&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;What really resonated with me from Rahimi and also Huszár's points is that &lt;em&gt;empiricism does not imply lack of rigour&lt;/em&gt;. A lot of what I do is quite empirical. A lot of what I do is somewhat applied. I've struggled with feeling like it's less scientific as a result. I've felt like I am "just" doing engineering. But the best way I have come to understand this work, which was captured in this point about empiricism, is that rigour does not need to be &lt;em&gt;mathematical&lt;/em&gt; (forgive me, I am a former theoretical physicist, so this has taken me some time to realise). Experimental design is also rigorous when done well. Building a model to solve a problem may be a kind of engineering, but trying to &lt;em&gt;understand&lt;/em&gt; it afterwards, forming hypotheses about its behaviour and then testing them - this can, and indeed should, be done rigorously. Otherwise, you show that a model exists which can achieve a certain performance on a certain task on a certain dataset, and little else.&lt;/p&gt;
&lt;p&gt;The next talk I actually attended was &lt;strong&gt;The Trouble with Bias&lt;/strong&gt; from &lt;a href="http://www.katecrawford.net/"&gt;Kate Crawford&lt;/a&gt; (video &lt;a href="https://www.youtube.com/watch?v=fMym_BKWQzk"&gt;here&lt;/a&gt;). This was a great talk, and I'm glad it got a prime spot in the program. Not only was her public speaking skill commendable (the slides just vanished near the end and she barely skipped a beat), but the talk was really interesting. I admit I was worried I'd already know most of the contents, since I read things about bias on a semi-regular basis (somehow). Even if I'd known everything she was going to say (which I didn't), I'd consider this talk a good distillation and overview of the pressing issues. She made an illuminating distinction which I shall now paraphrase.&lt;/p&gt;
&lt;p&gt;When it comes to bias, there are harms of &lt;em&gt;allocation&lt;/em&gt; and harms of &lt;em&gt;representation&lt;/em&gt;. Biased allocation is easy to see - someone got a loan someone else didn't, someone got bail and someone else didn't, etc. These are concrete and tangible, immediate, and easy to quantify. &lt;em&gt;Representation&lt;/em&gt; on the other hand relates to impressions and stereotypes. Google searches for 'CEO' returning all white men is a &lt;em&gt;representational&lt;/em&gt; bias, and its effect is much harder to measure. Images of Black people being labelled as 'gorillas' is &lt;em&gt;representational&lt;/em&gt; bias and while clearly hurtful, the impact of &lt;em&gt;allocation&lt;/em&gt; is not immediately obvious. Many people generally accept that this kind of representation is bad, but can we blame it for any &lt;em&gt;particular&lt;/em&gt; instance of allocation bias? Usually not. Representational bias is diffuse across culture, difficult to measure, and may not have any immediately obvious impacts. An example from me: We as a society are starting to suspect that something about how women are represented in society may be influencing the rates of women going on to study STEM subjects. This representational bias may be slowly manifesting as a tangible absence of female engineers, but it is difficult to formalise or prove that these observations are causally related. And of course, machine learning algorithms (like literally any algorithm) can be biased in either of these ways (and presumably more). Once again: &lt;a href="https://www.youtube.com/watch?v=fMym_BKWQzk"&gt;watch the talk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://people.eecs.berkeley.edu/~pabbeel/"&gt;Pieter Abbeel&lt;/a&gt; spoke about &lt;strong&gt;Deep Learning for Robotics&lt;/strong&gt; - really, (deep) reinforcement learning for robotics. Probably the most important takeaway from this talk was the 1 second clip of Dota 2 1v1 mid he showed, establishing an important moment in both Dota 2 and NIPS keynote history. The non-Dota content of the talk was largely focused about meta-reinforcement learning, or 'learning to reinforcement learn', and architectures to achieve this. The idea is that you want to build agents which can adapt quickly to new environments, as humans do. One interesting idea was 'Hindsight Experience Replay', which assumes whatever ended up happening was actually the goal, and deriving reward from that. &lt;/p&gt;
&lt;figure style="width: 20vw;"&gt;
&lt;img src="/images/2017/nips/skinner.jpg"&gt;
&lt;figcaption&gt;
Reinforcement learning agent re-evaluating its experience.
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This converts the usually sparse reward in RL to plentiful reward signals, given the Q-function is augmented with a notion of a goal. He used the cake metaphor that everyone loved from Yann LeCun's keynote at NIPS last year, converting the cherry on top of a cake to multiple cherries on a cake. People can't get enough of the cake joke. It's Portal all over again.&lt;/p&gt;
&lt;p&gt;I missed the talks from &lt;a href="https://getoor.soe.ucsc.edu/"&gt;Lise Getoor&lt;/a&gt;, &lt;a href="http://www.princeton.edu/~yael/"&gt;Yael Niv&lt;/a&gt;, and &lt;a href="https://www.stats.ox.ac.uk/~teh/"&gt;Yee Whye Teh&lt;/a&gt; because there is only so much time in a day.&lt;/p&gt;
&lt;h2&gt;&lt;a name="spotlights"&gt;&lt;/a&gt; Spotlights and Orals&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;First, a brief rant.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I was quite impressed by the quality of the spotlights and orals this year. Coming from the rather low bar of 'mumbling at a slide covered in equations' of previous years, I was glad to see that many presenters really put time into preparing their talk. These talks give people the opportunity to explain their work to potentially &lt;em&gt;thousands&lt;/em&gt; of fellow researchers, so giving a terrible talk is insulting both to the audience and to the people who didn't get that opportunity.&lt;/p&gt;
&lt;p&gt;I've thought about the implications of having an additional selection process for determining orals and spotlights. There's a trade-off between highlighting really good papers (with possibly terrible speakers) and highlighting less meritorious work (with a good communicator). There's also a challenge of being fair to non-native English speakers when assessing presentation quality - it would not be acceptable to condemn a talk on the basis of the speaker's command of English. &lt;/p&gt;
&lt;p&gt;I try to assess talks by how much they have considered the audience - considering what the audience already knows, what may be obvious (or not, usually), what the really important things in the work are, and what can be skipped without degrading the story. But how to do this without (subconsciously) judging the fluency of the speaker's language and delivery is not entirely clear. I'm sure there is already bias in how the quality of one's English influences paper acceptance (either through clarity or unknowingly discriminatory reviewers), so adding an additional layer on the presentation quality may exacerbate the issue. On the other hand, communication is really important for scientists, and the conference should do what they can to ensure the content is high quality. Maybe some sort of (optional) pre-conference speaking workshop for those invited to give orals and spotlights?&lt;/p&gt;
&lt;p&gt;Ranting aside, a selection of talks I took note of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1703.04389"&gt;Bayesian Optimisation with Gradients&lt;/a&gt; - &lt;em&gt;Jian Wu, Matthias Poloczek, Andrew Gordon Wilson, Peter I. Frazier&lt;/em&gt;.  Augment Bayesian optimisation using gradient information - 'derivative-enabled knowledge-gradient (dKG)'. They put a Gaussian process prior over the function to be optimised, resulting in a multi-output GP for both function and gradient (the gradient of a GP is a GP). It works better than methods not using derivatives, but I rarely have access to derivatives when I'm doing hyperparameter optimisation in deep networks, so I'm not sure how useful it would be for me.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1705.07874"&gt;A Unified Approach to Interpreting Model Predictions&lt;/a&gt; - &lt;em&gt;Scott Lundberg, Su-In Lee&lt;/em&gt;. The framework is called 'SHAP' (SHapley Additive exPlanations). The idea is to interpret the model by assigning features importance values for a given prediction. This work unifies six existing methods by proposing a notion of a 'additive feature attribution method'. They also find that their approach agrees well with human-derived feature attribution scores.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1709.01894"&gt;Convolutional Gaussian Processes&lt;/a&gt; - &lt;em&gt;Mark van der Wilk, Carl Edward Rasmussen, James Hensman&lt;/em&gt;. They consider a &lt;em&gt;patch-response&lt;/em&gt; function, which maps from image patches to real values, and place a Gaussian process prior on this function. Considering the sum of the patch-response function on all patches of the image as another function, its prior is also a Gaussian process. Computational complexity is a huge barrier here, which they address by using inducing points in the &lt;em&gt;patch&lt;/em&gt; space, corresponding to using inter-domain inducing points (an idea which is already understood, if not by me).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1703.06856"&gt;Counterfactual Fairness&lt;/a&gt; - &lt;em&gt;Matt J. Kusner, Joshua R. Loftus, Chris Russell, Ricardo Silva&lt;/em&gt;. Consider predictors as counterfactually fair if they produce the same result if a sensitive attribute were different. This means that any nodes downstream (in the causal graph) of that sensitive attribute may also be different. This implies that a predictor will necessarily be counterfactually fair if it is only a function of nodes which are &lt;em&gt;not&lt;/em&gt; descendants of the sensitive attribute, unsurprisingly enough. They address the fact that this is rarely feasible (almost everything in a person's life may be affected by their race, for example), by considering other models. For example, using residuals of variables, after accounting for (using a linear model) the sensitive attributes. One nitpick: I take issue with the example they give in Figure 2 (level 2). They introduce a latent variable which is predictive of success (GPA, LSAT, first year law school average grade) independent of sex and race, and call this &lt;em&gt;knowledge&lt;/em&gt;. I think this is a weird choice - &lt;em&gt;surely&lt;/em&gt; knowledge is affected by sex/race, if only by influencing available educational opportunities and ability to study unimpeded (for example, the need to work during school/college, the need to look after family members). I am trying to think of another name for this node which is &lt;em&gt;not&lt;/em&gt; plausibly influenced by sex or race, some sort of intrinsic attribute of the person - 'grit'? 'general intelligence'? 'luck'? (But who wants to base law school admissions on luck?) I can't imagine the authors were intending to make any kind of political statement about the nature of knowledge here, but it seems like a weird error(?) in a paper dealing with social issues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1708.02183"&gt;Multiresolution Kernel Approximation for Gaussian Process Regression&lt;/a&gt; - &lt;em&gt;Yi Ding, Risi Kondor, Jonathan Eskreis-Winkler&lt;/em&gt;. The popular method for scaling GPs is to approximate the kernel function using a low-rank approximation (the Nyström approximation). There are some issues with that: is a low-rank approximation reasonable? Which part of the eigenvalue spectrum of K' (that is, K + sigma I, which appears in the MAP estimate of the function) is the most important? This work proposes and develops a different kind of kernel approximation, depending on the data, where local factorisations are used, and it can be assumed that 'distant clusters [of data] only interact in a low rank fashion'. My cursory skim of the paper wasn't enough to get exactly what they're doing, but I love to see work questioning common practices and trying to understand/improve on them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1705.08933"&gt;Doubly Stochastic Variational Inference for Deep Gaussian Processes&lt;/a&gt; - &lt;em&gt;Hugh Salimbeni, Marc Deisenroth&lt;/em&gt;. Why do I always end up reading about GPs? I'm not even using them (right now?!). The tl;dr on this paper is that they got deep (that is, multi-layer generalisations of) GPs to work. Previously they didn't work particularly well because the variational posterior required each layer to be independent, an assumption which this work drops by introducing a new variational inference procedure (hence the title). They show that this model works even on datasets with a billion examples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1705.09655"&gt;Style Transfer from Non-Parallel Text by Cross-Alignment&lt;/a&gt; - &lt;em&gt;Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakola&lt;/em&gt;. Separate content from style, in text. This is interesting to me because, like &lt;em&gt;years ago&lt;/em&gt;, (2014) we had discussed using language embeddings to remove stylistic choices from the language of doctors, to try to standardise text across multiple authors. I'm not saying we have any claim whatsoever to the idea - ideas are cheap, implementation matters - but I'm interested to see that someone has - sort of - achieved something like what we wanted. They assume they have corpora with roughly the same &lt;em&gt;content&lt;/em&gt; distribution but different &lt;em&gt;style&lt;/em&gt; distributions, and try to learn a latent representation (which they formulate using a probabilistic model).  I have a big armchair-linguist issue with the idea that style is independent of content, because if you consider content as meaning then a &lt;em&gt;lot&lt;/em&gt; of meaning is conveyed through &lt;em&gt;how&lt;/em&gt; someone says something, and indeed even in their examples, they consider 'sentiment' as style, in which case I actually don't know what they mean by content. They actually mention in the introduction that one can only hope to approximately separate style and content even with parallel data, but they never really clearly define what they mean by 'content' of a sentence.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/6827-deep-multi-task-gaussian-processes-for-survival-analysis-with-competing-risks"&gt;Deep Multi-task Gaussian Processes for Survival Analysis with Competing Risks&lt;/a&gt; - &lt;em&gt;Ahmed M. Alaa · Mihaela van der Schaar&lt;/em&gt;. The risks are competing because the patient can only die from one thing. The model attempts to produce survival times (time-to-event) using a deep, multi-task (since multiple risks) Gaussian process. They use an intrinsic coregionalisation model for the kernel functions to account for multiple outputs, which models task(=output) dependence independently of input dependence, but simplifies calculations a lot (I tried to build a more complicated multi-task kernel function once and it was a big mess). They also point out that using a deep GP alleviates some dependence on the exact form of the kernel function. This work (unsurprisingly) uses the 'old' (2013) work on deep GPs, so I wonder how much it would benefit from the improved deep GPs (see above).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1705.10915"&gt;Unsupervised Learning of Disentangled Representations from Video&lt;/a&gt; - &lt;em&gt;Emily Denton, Vighnesh Birodkar&lt;/em&gt;. They want to separate time-varying and stationary parts of a video. Then you can predict future frames by applying a LSTM to the time-varying components. That's pretty neat! How do they achieve this? They use four networks - two encoders (one for scene (stationary information), one for pose (time-varying)), a decoder which maps pose and scene vectors to produce a frame, and a scene discriminator which tries to tell if pose vectors came from the same video. They construct loss terms to impose their constraints (separating time-varying and static elements), including some interesting adversarial loss terms.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a name="posters"&gt;&lt;/a&gt; Posters&lt;/h2&gt;
&lt;figure style="width: 40vw; float: none; display: block; margin: auto;"&gt;
&lt;img src="/images/2017/nips/posters.jpg"&gt;
&lt;figcaption&gt;
The quiet poster hall on Monday morning.
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;My experience of the poster sessions suffered the most as a result of jetlag, so I ended up looking at far fewer posters than I would have liked (even accounting for my eternally overambitious plans for poster sessions). This was also the first year where I got invited to ~cool parties~, so I went to some of those, too.&lt;/p&gt;
&lt;p&gt;The hall for the posters included what seemed like gratuitous space between rows, but it filled up rapidly (the crowd at the Capsules poster was sizeable). I admit I always think about roller derby these days when I'm trying to get past crowds of people, but hip checking strangers isn't a great way to do poster sessions (I &lt;em&gt;assume&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;My poster session strategy is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;before the conference: go through the list of papers and note the interesting ones&lt;/li&gt;
&lt;li&gt;don't leave any time to actually read the papers&lt;/li&gt;
&lt;li&gt;forget about the list, fight through crowds of large men to peer at poster titles&lt;/li&gt;
&lt;li&gt;eventually, learn things&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A humble plea to poster presenters: please don't stand directly in front of your poster while you're talking about it, I can't see and I don't want to get so close to you that you start talking to me.&lt;/p&gt;
&lt;p&gt;Here's a little caveat about this part of the blog post: I didn't visit all these posters. I'm just taking the opportunity to mention more interesting papers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1612.09328"&gt;The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process&lt;/a&gt; - &lt;em&gt;Hongyuan Mei, Jason Eisner&lt;/em&gt;. Alongside optimal transport, Hawkes processes appeared in my radar of possibly-interesting terms this NIPS, so I decided to take a look at this paper. I got so engrossed that I realised I was actually &lt;em&gt;reading&lt;/em&gt; the paper (I usually do a cursory skim to produce these summaries), so I've had to stop myself in the interest of giving other papers a chance. In short: a Hawkes process is a kind of non-homogeneous Poisson process (the rate of the process can vary in time) where events can &lt;em&gt;increase&lt;/em&gt; the probability of future events (the events are self-exciting). In this work they generalise the Hawkes process (allowing for inhibitory events, for example) and use a &lt;em&gt;continuous-time LSTM&lt;/em&gt; to model the intensity functions of the given events. Also, they use a &lt;em&gt;meme dataset&lt;/em&gt; (amongst others) to train the model, so the paper includes amusing lines like&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"We attribute the poor performance of the [non-neural] Hawkes process to its failure to capture the latent properties of memes, such as their topic, political stance, or interestingness".&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The idea of trying to study memes computationally is funny, because even humans barely understand memes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style="display: block; width: 40vw; margin: auto; float: none;"&gt;
&lt;img src="/images/2017/nips/rqadsnzazfqz.jpg" 
&lt;figcaption&gt;
Example of a typical "meme"
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1710.02224"&gt;Dilated Recurrent Neural Networks&lt;/a&gt; - &lt;em&gt;Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark Hasegawa-Johnson, Thomas S. Huang&lt;/em&gt;. Like a dilated CNN, but... an RNN. They achieve this using dilated recurrent skip connections. This is different to the usual skip connection (which takes information from some previous state of the RNN) in that it &lt;em&gt;doesn't&lt;/em&gt; rely on the immediately previous state. That's what makes it a dilation. You can stack layers with different dilation lengths to get a sort of 'multiresolution' RNN. If this sounds similar to the &lt;a href="https://arxiv.org/abs/1402.3511"&gt;Clockwork RNN&lt;/a&gt;, you're right, but see section 3.4.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1711.05411"&gt;Z-Forcing: Training Stochastic Recurrent Networks&lt;/a&gt; - &lt;em&gt;Anirudh Goyal, Alessandro Sordoni, Marc-Alexandre Côté, Nan Rosemary Ke, Yoshua Bengio&lt;/em&gt;. Yes, I care a lot about RNNs. I work on (medical) time series data, if that wasn't already apparent. This paper adds to the growing work on combining deterministic RNN architectures with stochastic elements (like state space models), hitting an intractable inference problem, and using variational inference with a RNN-parametrised posterior approximation. So what's new here? They observe that these models can often neglect to use the 'latent' part (the stochastic elements), so they add a regularisation term to the ELBO which 'forces' the latent state at time &lt;em&gt;t&lt;/em&gt; to be predictive of the hidden state of the backwards-running inference network. And this works better, empirically. When I first saw this paper I panicked because the title makes it sound very similar to an idea I have been cooking up, an idea which I got stuck on because I was trying to explain an additional regularisation term in terms of a prior (on &lt;em&gt;something&lt;/em&gt;). But these authors just go ahead and use a regulariser without any probabilistic interpretation, so it's probably fine to do that. Note to self: not everything has to be mathematically beautiful.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1703.04977"&gt;What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?&lt;/a&gt; - &lt;em&gt;Alex Kendall, Yarin Gal&lt;/em&gt;. I first learned about (and immediately loved) aleatoric and epistemic uncertainty in my applied Bayesian statistics class back in Cambridge, so despite not featuring RNNs, I was interested in this work. In this context, aleatoric uncertainty is the uncertainty inherent to the observations, whereas epistemic uncertainty arises from uncertainty about the model parameters (which could in principle be reduced with more training). So this work studies epistemic and aleatoric uncertainty in deep networks (for computer vision), and shows that modelling aleatoric uncertainty improves performance in semantic segmentation and depth regression.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1705.08639"&gt;Fast-Slow Recurrent Neural Networks&lt;/a&gt; - &lt;em&gt;Asier Mujika, Florian Meier, Angelika Steger&lt;/em&gt;. Phew, back to RNNs. This work proposes a RNN architecture attempting to combine the advantages of multiscale RNNs and deep transition RNNs. Basically, it's a 'new model architecture' paper. They show good results on two language modelling tasks, and do further analyses of the properties of their model. Multiscale (temporally speaking) data is extremely common in medicine, so something like MIMIC-III would have been a great test-case for this model as well. Maybe I'll find a masters student to explore this (I obviously don't have time because I spend all my time writing blog posts).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1705.10888"&gt;Identification of Gaussian Process State Space Models&lt;/a&gt; - &lt;em&gt;Stefanos Eleftheriadis, Thomas F.W. Nicholson, Marc Peter Deisenroth, James Hensman&lt;/em&gt;. A lot of work focuses on inferring the latent states of a GP state space model. Here, they (also) look at learning the model itself. An important difference between your typical GP setting and the GP-SSM is that the &lt;em&gt;inputs&lt;/em&gt; to the GP in the latter case are &lt;em&gt;latent&lt;/em&gt; states (of the state space model), so they have to infer &lt;em&gt;both&lt;/em&gt; the latent states &lt;em&gt;and&lt;/em&gt; the transition dynamics (that's the model). They use variational inference with a bidirectional RNN as the recognition network, so you know I'm on board.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1709.02012"&gt;On Fairness and Calibration&lt;/a&gt; - &lt;em&gt;Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, Kilian Q. Weinberger&lt;/em&gt;.  This work seems to be a follow-up to &lt;a href="https://arxiv.org/abs/1609.05807"&gt;this paper&lt;/a&gt; which was written to analyse &lt;a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing"&gt;this piece on bias in criminal sentencing from ProPublica&lt;/a&gt; (ProPublica also &lt;a href="https://www.propublica.org/article/bias-in-criminal-risk-scores-is-mathematically-inevitable-researchers-say"&gt;followed up&lt;/a&gt; on this and other research following their investigation). So first up: it's awesome to see academic research and investigative journalism interacting in this way. In the precursor paper they provide an impossibility proof (which is given a simplified geometric proof in this paper) for simultaneously satisfying calibration and equalized odds (equal false positive and false negative rates between groups). As hinted in the precursor paper, relaxing the notion of equalized odds (for example, sacrificing equal false positive rates) may allow you to keep calibration, and that's what they show in this paper.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1705.08821"&gt;Causal Effect Inference with Deep Latent-Variable Models&lt;/a&gt; - &lt;em&gt;Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel, Max Welling&lt;/em&gt;. The focus of this work is in account for &lt;em&gt;confounders&lt;/em&gt; (by modelling them as latent variables) while doing effect inference, particularly in the presence of noisy proxies of true confounders. They achieve this using a 'causal effect variational autoencoder' (CEVAE).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a name="ml4h"&gt;&lt;/a&gt;Machine Learning for Health (&lt;a href="https://ml4health.github.io/2017/"&gt;ML4H&lt;/a&gt;)&lt;/h1&gt;
&lt;p&gt;I speculate that they're moving away from the previous acronym (MLHC - machine learning for health care) due to a collision with the &lt;a href="http://mucmd.org/"&gt;MLHC conference&lt;/a&gt; (previously abbreviated MUCMD). Apparently MLHC (the conference) will be in Stanford in 2018, which is a shame because I feel I should attend it, but I really didn't enjoy travelling to/from California for NIPS. Also, I think conference organisers should be avoiding the USA (or any other country with restrictive or racist visa policies) if at all possible right now. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Anyway&lt;/em&gt;. The workshop, unrelated to the MLHC conference, was an all-day affair on the Friday of NIPS. There were all the usual things: invited talks, spotlight talks, (frustratingly short) poster sessions, and people crammed into one room for 8 hours. I missed the panel because I was stuck at lunch, and I missed Jure Leskovec's talk because I was ~ networking ~. For the rest, I took some notes.&lt;/p&gt;
&lt;p&gt;Talks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://dbmi.hms.harvard.edu/person/faculty/zak-kohane"&gt;Zak Kohane&lt;/a&gt; - &lt;strong&gt;AI in Medicine That Counts&lt;/strong&gt;. He distinguished between AI that does what doctors do, AI that does what doctors &lt;em&gt;could&lt;/em&gt; do (but don't), and AI that does what doctors can't do. I am reminded of &lt;a href="https://lukeoakdenrayner.wordpress.com/2016/11/27/do-computers-already-outperform-doctors/"&gt;this post&lt;/a&gt; from Luke Oakden-Rayner which distinguishes between tasks we're building ML systems to solve, and tasks which doctors actually do. They're not the same, and Kohane made the point that they need not be, in general. We can see gains in outperforming doctors on e.g. diagnostics, but we can also see gains in doing analyses doctors simply can't do (because they're not computers). Kohane gave an example of a child with ulcerative colitis who was saved from colonectomy after they ran a gene expression analysis on children with similar irritable bowel disease and identified an effective drug (indirubin). He also provided a good comparison between medicine and what Deepmind has been achieving with AlphaZero (on Go and other games). Achievements like AlphaZero make people think AI is about to take over the world (from what I can tell), but medicine is far from AI-led mastery:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it's non-deterministic&lt;/li&gt;
&lt;li&gt;it's not fully observable&lt;/li&gt;
&lt;li&gt;the action space is not discrete&lt;/li&gt;
&lt;li&gt;we have no perfect simulators&lt;/li&gt;
&lt;li&gt;'episodes' in medicine are not short (consider the number of seconds in a typical ICU stay, consider a person's entire life...)&lt;/li&gt;
&lt;li&gt;evaluation is unclear and slow&lt;/li&gt;
&lt;li&gt;trial and error is not an option (outside of controlled trials, and even then the trial is highly constrained)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In his list he also included that we have huge datasets of human play (for games like Go), but I think medicine is getting there towards having large datasets (at least locally), so I don't count this as a &lt;em&gt;fundamental&lt;/em&gt; limitation. He then went on to discuss the money end of medicine, which I'm &lt;em&gt;not&lt;/em&gt; a fan of, but if you're to be pragmatic, you gotta understand the game you're playing. He made a point that we may come up with cool technology to improve medicine in different ways, but unless a business argument can be made for it, it likely won't be adopted. This is more clearly true in the US where healthcare is more of profit-oriented than in other countries (e.g. those with socialised healthcare systems) - ML4H @ Socialised Healthcare edition, anyone? We can have it in a neutral country! &lt;em&gt;(Joking aside, I am legitimately interested in the opportunities for ML to benefit from and improve socialised healthcare systems - data centralisation is an obvious point, but perhaps other types of problems are more immediately pressing in systems like the NHS, than they would be in the USA...)&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.microsoft.com/en-us/research/people/jchayes/"&gt;Jennifer Chayes&lt;/a&gt; - &lt;strong&gt;Opportunities for Machine Learning in Cancer Immunotherapy&lt;/strong&gt;. The immune system is an incredibly complicated and therefore cool system, and cancer immunotherapy is a very very cool use of the immune system. With the caveat that I'm &lt;em&gt;not&lt;/em&gt; an immunologist, the tl;dr of cancer immunotherapy is: tell your immune system to target and kill cancer cells. This may be what the immune system does already, to some extent. T-cells identify specific antigens, and direct the rest of the immune system to kill cells presenting those antigens. (How do T-cells know what to identify? &lt;a href="https://en.wikipedia.org/wiki/Thymus"&gt;The thymus is the coolest organ you've never heard of.&lt;/a&gt;) So the challenge is to train T-cells to specifically recognise your cancer cells, but there are &lt;em&gt;lots&lt;/em&gt; of possible (neo)antigens. You can formulate this as a matrix completion problem (T-cells v. antigens) to predict the response of new T-cells. She also described work they did for predicting response to checkpoint inhibitors (a type of cancer immunotherapy), highlighting the value of building relatively simple models on small data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.seas.harvard.edu/directory/samurphy"&gt;Susan Murphy&lt;/a&gt; - &lt;strong&gt;Challenges in Developinging Learning Algorithms to Personalise mHealth Treatments&lt;/strong&gt;. This was about the &lt;a href="http://www.heartsteps.net/"&gt;HeartSteps&lt;/a&gt; project, which tries to encourage physical activity in people who have completed cardiac rehabilitation. That is, it's an app that encourages you to go for a walk. This is a problem of sequential decision making. To maximise positive outcome (more time walking), what sort of notifications should the app send, and when? If someone is driving, you shouldn't bother them. If they just walked somewhere, or are in the middle of walking, you shouldn't tell them to go for a walk. They model it as a (contextual) bandit problem, and have to deal with noise in the data, nonstationarity (the expected reward function changes over time), and that there are longer-term delayed effects from actions. Unsurprisingly (to anyone who's used apps that send them push notifications), after a while people just start ignoring them, and the result of interventions diminish. While the intentions in this work are noble, I can see creepy unintended uses of research like this into user engagement (like &lt;a href="https://usedopamine.com/"&gt;this horrible startup&lt;/a&gt;). Technology is always a double-edged sword, but if we have to be subjected to personalised advertising and addiction mechanics in games, and so on, at least fewer people should die of heart disease, right?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://vision.stanford.edu/feifeili/"&gt;Fei-Fei Li&lt;/a&gt; - &lt;strong&gt;Illuminating the Dark Spaces of Healthcare&lt;/strong&gt;. I think that was the title. She spoke about three projects in healthcare that use computer vision, and the room was &lt;em&gt;packed&lt;/em&gt;. At first I thought everyone suddenly loves healthcare, but then I remembered that Fei-Fei Li is famous. The projects were all about activity recognition from non-RGB video (they had depth sensors and IR video if I recall - these alleviate &lt;em&gt;some&lt;/em&gt; privacy concerns). First she spoke about identifying hand-washing to tackle hospital acquired infection. One challenge was in activity recognition given unusual (for research) viewpoints, e.g. cameras on ceilings looking directly down. The second project was about ICU activity recognition, to better understand what people spend time doing in the ICU. The priority here was &lt;em&gt;efficiency&lt;/em&gt;, so they developed methods to analyse video which don't require analysis of every single frame, saving a lot of compute while still achieving high performance (on standard video understanding datasets). Finally, she spoke about applications in independent senior living, such as fall detection. This in particular is challenging due to limited training data and rare events (thankfully). They propose to use domain transfer to aid in the data scarcity issues, but she pointed out that much of this work is still in progress.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://healthsciences.ucsd.edu/som/medicine/divisions/med-genetics/research/Pages/Mesirov-Lab.aspx"&gt;Jill Mesirov&lt;/a&gt; - &lt;strong&gt;From Data to Knowledge&lt;/strong&gt;. I am doubtful this was the title of her talk, but we'll run with it. The topic was medulloblastoma, which is one of the most common forms of paediatric brain tumour. 70% of children &lt;em&gt;survive&lt;/em&gt;, but only 10% go on to leave independent lives. Their focus is in predicting relapse, which they achieve using a probabilistic model incorporating various clinical and genomic features. She then went on to describe a project to identify novel therapeutics for an aggressive subtype of medulloblastoma driven by Myc (this is a gene). Through mouse xenograft experiments and expression profiling, they found this subtype is likely sensitive to CDK-inhibitors, and found they could extend survival (in mice) by 20% with palbociclib, suggesting a candidate treatment. This sort of analysis is sort of 'well known' to me because my lab (alongside machine learning) works on cancer genomics, but I'd also like to pause for a moment to reflect on two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;As with the example from Zak Kohane (about indirubin), a lot of the time (translational) computational biologists are hunting for threads - persistent patterns in the disease which indicate possible vulnerabilities, which they can then follow up by looking for matches in drugs with known targets. If you can optimise any point in that process, you can probably save someone's life, some day.&lt;/li&gt;
&lt;li&gt;A 20% extension in survival is clinically significant, but it's not a cure as we think of it. For mice it's measured in days, for humans probably one or two years if not months. For some cancers, especially brain cancers, this is still where we're at. Fighting cancer is really, really hard.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://research.google.com/pubs/GregCorrado.html"&gt;Greg Corrado&lt;/a&gt;. I just stopped writing down the titles at some point. He spoke about a few different projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Diagnostics: doctors working alongside algorithms to work better/faster. Examples from Google Brain: screening for diabetic retinopathy (on par with ophthamologists), reading breast cancer biopsies.&lt;/li&gt;
&lt;li&gt;Care management/decision support: the idea is to have smart electronic medical records, to help reduce errors and improve care quality. Having observed clinicians interacting with EMRs, I see a lot of potential for improvement here.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;He mentioned challenges with processing medical data because of how messy it is and I just laughed and laughed and then cried (silently). Apparently they built some sort of &lt;a href="https://www.hl7.org/fhir/"&gt;FHIR&lt;/a&gt;-based pipeline to integrate data from six healthcare systems, and it worked well, but I didn't write down what they were doing at the end of the pipeline.
 He also gave a shout-out to Google's newly open-sourced variant caller, &lt;a href="https://www.biorxiv.org/content/early/2016/12/21/092890"&gt;DeepVariant&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.oxford-man.ox.ac.uk/~mvanderschaar/"&gt;Mihaela van der Schaar&lt;/a&gt; - &lt;strong&gt;Dynamical Disease Modelling&lt;/strong&gt;. Her work focuses on &lt;em&gt;dynamical&lt;/em&gt; modelling, assuming some hidden clinical state which informs observable physiological variables. You could approach this using a hidden Markov model, but she observed that transition probabilities typically depend on sojourn times, necessitating a semi-Markov model. Furthermore, patterns of missingness are informative, suggesting to model observation times, e.g. as a Hawkes process. The informativeness of measurements in medicine may not be immediately obvious, but the rationale (at least in the ICU, my area of focus) is that some measurements are only taken when needed, and they're only needed when the doctor suspects something is up. Even if a measurement is routinely performed, the rate of measurement may increase when patients become more critical. So you have a huge case of missing-not-at-random. She also mentioned their work on modelling competing risks, which I described earlier in this blog post.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://buttelab.ucsf.edu/"&gt;Atul Butte&lt;/a&gt; - &lt;strong&gt;Translating a Trillion Points of Data into Diagnostics, Therapies and New Insights in Health and Disease&lt;/strong&gt;. I didn't take notes for this talk, but his slides are &lt;a href="https://www.slideshare.net/atulbutte/atul-butte-nips-2017-ml4h"&gt;here&lt;/a&gt; - I'd recommend slide 29. In case that link at some point goes dead, that slide summarises lessons he's learned in MLHC over the years, and these are (paraphrased):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solve the problems that health care professionals need solved, don't just guess&lt;/li&gt;
&lt;li&gt;Watch out for models limited by bad inputs (e.g. from patients, from doctors)&lt;/li&gt;
&lt;li&gt;Learn what IRB, HIPAA, BAA, ICD-10 codes, CPT codes, CLIA, and CAP are.&lt;/li&gt;
&lt;li&gt;Learn patience.&lt;/li&gt;
&lt;li&gt;Not everything needs deep learning.&lt;/li&gt;
&lt;li&gt;Having all the data on someone is super rare.&lt;/li&gt;
&lt;li&gt;Health care inefficiency is not about friction. (He made a point that everywhere there's a cost, someone is making money and will push back against losing that money.)&lt;/li&gt;
&lt;li&gt;Data integration &lt;em&gt;can&lt;/em&gt; happen, if there's a business reason for it.&lt;/li&gt;
&lt;li&gt;Platforms and companies are commoditized. (As subpoints to that he suggests the ML people should come with some medical knowledge, to demonstrate we care about healthcare, and so we don't cost medical collaborators time training us.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another point he made was that there's a &lt;em&gt;lot&lt;/em&gt; of freely-accessible data out there, which is ripe for analysis. And possibly founding startups.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/ratschlab/RGAN/blob/master/presentations/nips_ml4h.pdf"&gt;&lt;img src="/images/2017/nips/nips_ml4h.png" style="float: right; width: 30vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As I mentioned, there were two poster sessions. I spent the first one presenting my poster, and much of the second one talking to people, so I didn't get to &lt;em&gt;see&lt;/em&gt; too many posters. I've described a lot of work from other people in this post, so let me do the same for myself. At WiML and ML4H I was presenting (variations on) this poster: (right)&lt;/p&gt;
&lt;p&gt;Summary of the related paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1706.02633"&gt;Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs&lt;/a&gt; - &lt;em&gt;Stephanie L. Hyland (that's me) and Cristóbal Esteban, Gunnar Rätsch&lt;/em&gt;. (For disclosure: the version that was accepted by ML4H was a 4-page version of this preprint, focusing on the medical data and aspects. They asked us to give links to the arXiv versions of our work, but I couldn't in good faith link to the full version as it wasn't reviewed by them. In case you noticed and were wondering why there's no link to the paper the workshop page, it's because of my conscience). &lt;/p&gt;
&lt;p&gt;The motivation for this work was that MLHC struggles with data sharing. Medical data is hard to share, with good reason. But it means a &lt;em&gt;lot&lt;/em&gt; of work in MLHC is completely unreproducible, and nobody can directly build on it, because they don't have access to the data/task a model was built for. This stifles our progress, and MLHC is hard enough already. So wouldn't it be great if we had a &lt;em&gt;synthetic&lt;/em&gt; dataset (without privacy concerns) that we could use to benchmark models and approaches? Shoutout to this related paper with similar motivation from Choi et al.: &lt;a href="https://arxiv.org/abs/1703.06490"&gt;Generating Multi-label Discrete Patient Records using Generative Adversarial Networks&lt;/a&gt; (they focus on binary and count-valued data, hence our focus on real-valued time-series data).&lt;/p&gt;
&lt;p&gt;I'd summarise what we did in this work in three points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Devise a GAN architecture to generate real-valued time series. We call this a 'recurrent' GAN, or RGAN, because it uses RNNs for both discriminator and generator networks (yes, RNNs!). We also have a conditional version which takes label information, allowing the RGAN to generate data from labels.&lt;/li&gt;
&lt;li&gt;Devise an evaluation scheme for GANs tailored to our setting. We do this by generating a synthetic training dataset from the RCGAN (labels + features), training a classifier (e.g. CNN, random forest) on it, and reporting its performance on a held-out &lt;em&gt;real&lt;/em&gt; test set. We call this the &lt;em&gt;TSTR&lt;/em&gt; (train on synthetic, test on real) score. Since we want to use the RGAN to generate synthetic medical data, the TSTR score is of particular relevance.&lt;/li&gt;
&lt;li&gt;Analyse empirically whether the RGAN is 'overfitting'. By this I mean, we ask (roughly) if the GAN is more likely to produce samples &lt;em&gt;very similar&lt;/em&gt; to training samples than it is to produce other samples (from the same distribution, e.g. the test set). If it is, then we have a problem. Firstly because reproducing the training set is boring and does &lt;em&gt;not&lt;/em&gt; require a GAN, and secondly (more importantly) because reproducing the training data set would constitute a serious privacy breach in our setting.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On the final point, we also experimented with training the RGAN using differential privacy, &lt;em&gt;just to be extra safe&lt;/em&gt;. If you're willing to sacrifice performance you can get some privacy, but it's a harsh trade-off and requires further research.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I held a small reading group in my lab about interesting contributions from the ML4H workshop, so I'll briefly summarise two papers of interest to me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1712.00164"&gt;Generative Adversarial Networks for Electronic Health Records: A Framework for Exploring and Evaluating Methods for Predicting Drug-Induced Laboratory Test Trajectories&lt;/a&gt; - &lt;em&gt;Alexandre Yahi, Rami Vanguri, Noémie Elhadad, Nicholas P. Tatonetti&lt;/em&gt;. My reason for interest should be obvious. Also, the first author emailed me to get help with &lt;a href="https://github.com/ratschlab/RGAN/"&gt;our code&lt;/a&gt;, which possibly means they used it. I spent some time answering issues on GitHub and responding to emails, and I'm still quite a junior scientist, so it's really exciting for me to see people taking interest in and actually trying to use my work. Anyways, in this paper, &lt;em&gt;as far a I understand it&lt;/em&gt;, they're generating cholesterol time-course data before and during exposure to statins. They do two interesting things: 1) Clustering patients based on a large set of clinical attributes, then training separate GANs on each cluster. 2) Evaluating the performance of the GAN by measuring how well it 'predicts' cholesterol level during statins exposure. They do this by matching generated samples to the closest real (hopefully test-set) sample based on the &lt;em&gt;pre-exposure&lt;/em&gt; part of the sequence, then measuring the similarity of the synthetic and real samples during statins exposure. This evaluation method seems a little brittle - imagine there are multiple real samples that look similar to the synthetic one, but respond to statins quite differently, but it's an interesting idea.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1712.00181"&gt;Personalized Gaussian Processes for Future Prediction of Alzheimer's Disease Progression&lt;/a&gt; - &lt;em&gt;Kelly Peterson, Ognjen (Oggi) Rudovic, Ricardo Guerrero, Rosalind W. Picard&lt;/em&gt;. I haven't spent enough time with this paper to fully understand it, but the most interesting aspects are: fitting a GP model to a source population, and personalising (i.e. tuning) it to an individual based on their observed data to date using domain-adaptive GPs, and using auto-regressive GPs. Various kinds of GPs. No RNNs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a name="conclusion"&gt;&lt;/a&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This has been an exceedingly long blog post and I hope you're not as exhausted as I am, but this is basically an accurate depiction of my experience of NIPS. A lot of stuff, all the time. I have not even mentioned the &lt;a href="http://bayesiandeeplearning.org/"&gt;Bayesian Deep Learning workshop&lt;/a&gt;. During the lunch break on the final day I grabbed a burrito and almost fell asleep. I was not the only one. The convention centre by that point was gradually emptying, with scattered people dozing off in chairs, and a prominent left-luggage zone where the registration tables had been. There was a clear sense of winding down, perhaps because the process had already begun for me. I stayed only briefly at the closing party (missing some unpleasantness, it sounds like), and instead walked/skated thoughtfully back to my Airbnb along the beach, pausing to look at the stars and listen to the Pacific Ocean.&lt;/p&gt;
&lt;p&gt;&lt;Img src="/images/2017/nips/longbeach_2.jpg" style="display: block; width: 45vw; margin: auto;"&gt;&lt;/p&gt;</content><category term="nips"></category><category term="conference"></category><category term="long beach"></category><category term="california"></category><category term="usa"></category><category term="MLHC"></category></entry><entry><title>bloodseeker costume</title><link href="/making/2017-10-31-bloodseeker-costume.html" rel="alternate"></link><published>2017-10-31T00:00:00+00:00</published><updated>2017-10-31T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2017-10-31:/making/2017-10-31-bloodseeker-costume.html</id><summary type="html">&lt;p&gt;&lt;img src="/images/2017/bloodseeker/general_00s.jpg" style="float: left; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;
Here are a bunch of WIP pictures of the Bloodseeker (Dota 2) cosplay I did for MCM London Comic Con on the weekend. This is the first non-Halloween costume I've made, and the first non-trivial sewing project I've done, so it's far from perfect, but I'm pretty happy with how …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img src="/images/2017/bloodseeker/general_00s.jpg" style="float: left; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;
Here are a bunch of WIP pictures of the Bloodseeker (Dota 2) cosplay I did for MCM London Comic Con on the weekend. This is the first non-Halloween costume I've made, and the first non-trivial sewing project I've done, so it's far from perfect, but I'm pretty happy with how it turned out overall.&lt;/p&gt;
&lt;p&gt;For reference, here's what Bloodseeker looks like... (yes, this is a photograph of my screen)&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/lol_s.png" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;
And for further reference, here's what my cosplay ended up looking like... (my face doesn't &lt;em&gt;usually&lt;/em&gt; look like that)&lt;/p&gt;
&lt;p&gt;I took a &lt;em&gt;lot&lt;/em&gt; of progress pictures of the crafting, so I've tried to find a smallish number for each of the components. Most of the crafting took place in my boyfriend's parents' house, saving me the need to fly with two glaives in my bag. Also he has a heat gun.&lt;/p&gt;
&lt;h1&gt;The fabric bits&lt;/h1&gt;
&lt;p&gt;This costume was my introduction to using a sewing machine (save a day-long class I took with the &lt;a href="http://thethriftystitcher.co.uk/"&gt;Thrifty Stitcher&lt;/a&gt; in London), so I kept it two two stitches and fumbling around making patterns.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/general_01s.jpg" style="width: 48vw; display: block; margin: auto;"&gt;&lt;/p&gt;
&lt;p&gt;Never have I measured myself so much.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/loin_01s.jpg" style="width: 48vw; display: block; margin: auto;"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/loin_02s.jpg" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;I heard you're supposed to use muslin? Also doubles for making cold-brew coffee. This was me trying to deal with the fact that my waist is sort of, but not exactly, cone shaped. It turns out those little tucks I did are called 'darts', and I didn't invent them.&lt;/p&gt;
&lt;p&gt;Originally had this elaborate thing where the flappy bit on the loin cloth would wrap up and around from the back of the waist band. I wanted it to fall right, but I do not understand fabric nearly well enough.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/loin_03s.jpg" class="floatl" style="width: 48vw; display: block; margin: auto;"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/cape_01s.jpg" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;My sewing machine is a Janome Sewist 525S, because that is exactly the one I used at the &lt;a href="http://thethriftystitcher.co.uk/"&gt;aforementioned sewing class&lt;/a&gt;, so I avoided doing any market research at all.&lt;/p&gt;
&lt;p&gt;Sewing interfacing onto the inside of the front of the cape, on the tiniest ironing board. I wanted it to be a bit stiff, so it would retain a mostly-round shape even if there were bones and stuff on it. I think this worked okay.&lt;/p&gt;
&lt;p&gt;Comparing different shades of black/red paint/pen on the fabric. Is painting acrylic onto fabric a thing you're supposed to do? I did a lot of that.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/cape_02s.jpg" class="floatl" style="width: 48vw; display: block; margin: auto;"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/cape_03s.jpg" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;This is a collar. I improvised how you're supposed to do collars. I am looking forward to learning how to actually sew things.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/general_02s.jpg" style="float: left; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;Plain red fabric bits! I kept them in this state for ages because I was afraid of destroying them with painting. One day the skates on my floor will kill me.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/cape_04s.jpg" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;
Drawing patterns onto the cape. I forgot to get like... anything you're supposed to use to draw onto fabric, and it was Sunday (which means everywhere in Zurich is closed), so I just #yolo'd it and directly drew with an 8B pencil. This particular section was also improvisation, because I couldn't find an angle of Bloodseeker where you can actually see the top of his shoulders/back of his neck. There is no evidence Bloodseeker even has a neck. Slit in the back of the head hole to enable me to put it over my massive head. (My head is massive, my roller derby helmet confirms this. Massive.)&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/cape_05s.jpg" style="float: left; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;Painting acrylic onto fabric. I accidentally bought so much red/black paint! I preferred how it looked when it was sort of vaguely-shaded, rather than solid colours in the end. I will make a costume with more exciting painting in it next time.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/wraps_01s.jpg" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;These wraps &lt;em&gt;did not&lt;/em&gt; work out on the day. I did not think about how to get wraps to lie flat and not just slowly fall down my calves and bunch around my shoes. There's also supposed to be a red trim on this, and I did cut it out/sew it, but I didn't have time to do the final assembly. Things to fix for next time.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/wraps_02s.jpg" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;Sewing myself into the hand wraps was interesting.&lt;/p&gt;
&lt;h1&gt;bones&lt;/h1&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/bones_01s.jpg" style="float: left; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;The basic construction of the bones is a papery core with a plaster wrap ... wrapping. Because I obsessively hoard things I suspect may be one day useful, I have an entire box of &lt;em&gt;generic recycled sheets of paper&lt;/em&gt;. I managed to use almost a tenth of the box making these bones. Next costume: mostly paper?&lt;/p&gt;
&lt;p&gt;Applying plaster wrap is extremely satisfying. I spent an afternoon making these while listening to podcasts. If you ever find you're too easily distracted by your phone/computer, you should try covering your hands with plaster.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/bones_02s.jpg" class="floatl" style="width: 48vw; display: block; margin: auto;"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/bones_03s.jpg" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;Painted a few coats of English breakfast tea on these bad boys. Check out that extremely Swiss shopping bag.&lt;/p&gt;
&lt;h1&gt;helmet&lt;/h1&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/helmet_01s.jpg" style="float: left; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;EVA foam! Drafting a helmet: challenging. Building a rugby ball: all too easy. We didn't have a dremel so you can see the jagged edges where I tried to hack at the edge of the foam to create a surface I could join.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/helmet_02s.jpg" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;
Not pictured: &lt;em&gt;sanding some extremely angular corners&lt;/em&gt;. Still too angular. These angles haunt my nightmares. Learned I love sanding (somehow??).&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/helmet_03s.jpg" style="float: left; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;
Helmet... wings? Flaps? That horrible join is what the angular corners looked liek before the extensive sanding. I think if I do this costume again I will just make the helmet out of fake leather.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/helmet_04s.jpg" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;
Painting on foam is easier than fabric. I actually ended up going over this paint, because it's better if you go EVA foam -&amp;gt; PVA glue -&amp;gt; spray paint -&amp;gt; normal paint, apparently.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/helmet_05s.jpg" style="float: left; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;
I'm pretty proud of the solution I came up with for attaching + fanning the feathers. That's a strip of softer craft foam, which I then painted black and glued to the inside of the helmet. Also, because the helmet is shaped like a rugby ball, there's space for feather shafts in the back.&lt;/p&gt;
&lt;h1&gt;glaives&lt;/h1&gt;
&lt;p&gt;I made these in the ~1.5 days before MCM, but they were a lot easier than trying to make any clothing. Trying to get things to fit is really hard. For the glaives, I just had to measure how long my forearms are.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/glaives_01s.jpg" style="width: 48vw; display: block; margin: auto;"&gt;&lt;/p&gt;
&lt;p&gt;The basic idea of the glaives is that they're a sandwich of EVA foam (the handles) around a thin-craft-foam-worbla sandwich (the blades). I decided to do it this way because it seemed physically plausible, and would result in something that wasn't too bulky-looking. This photo shows the thin and EVA foams, pre-sanding.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/glaives_02s.jpg" style="float: left; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;
&lt;a href="http://www.worbla.com/?page_id=6543"&gt;Worbla's Finest Art&lt;/a&gt; ready to be sandwiched around the thin craft foam. I did a sort of &lt;em&gt;partial&lt;/em&gt; sandwich, where the worbla didn't completely enclose the foam (see the black extending beyond the gold), to give a sharper blade edge.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/glaives_03s.jpg" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;Half-sandwiching in progress. Working with Worbla is quite nice! Once we realised you can turn the heat gun down from 600C it all got a lot easier.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/glaives_04s.jpg" class="floatl" style="width: 48vw; display: block; margin: auto;"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/glaives_05s.jpg" style="float: left; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;Love sanding.&lt;/p&gt;
&lt;p&gt;Sanded EVA handles + partial worbla sandwich + sharp foam edge. Part of me thinks they looked better like this than post-painting. I think it's to do with the texture - acrylic paint introduces a very... plastic texture.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/glaives_06s.jpg" class="floatl" style="width: 48vw; display: block; margin: auto;"&gt;&lt;/p&gt;
&lt;p&gt;Post painting! Ignore the horrible worbla seams! I had to make up some designs to put on the handles. My boyfriend did most of the painting of the glaives, because I was mysteriously busy with something else at the time (possibly painting acrylic on fabric). Also, he knows how to paint things to look like metal.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/general_03s.jpg" style="float: left; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;
Here I am with my boyfriend in his costume!&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017/bloodseeker/glaives_07s.jpg" style="float: right; width: 24vw; margin: 1.5vw 1.5vw 1.5vw 1.5vw;"&gt;&lt;/p&gt;
&lt;p&gt;The last two things I did with the glaives were: painting shading to try to make them look a bit more three-dimensional, and painting blood on. I bought some fake blood, but it just sort of sat on the surface of the blades, jamlike.&lt;/p&gt;
&lt;h1&gt;conclusion&lt;/h1&gt;
&lt;p&gt;I think some people who make costumes also make an effort to get good photographs of themself wearing it. I did not do this. (Luckily, the (only) other Dota 2 cosplayer I met at MCM got some photos of us &lt;a href="https://twitter.com/anjaelster/status/924674811829342209"&gt;here&lt;/a&gt;). Next time, I will get some better photos.&lt;/p&gt;</content><category term="dota"></category><category term="cosplay"></category><category term="costume"></category><category term="foam"></category><category term="sewing"></category></entry><entry><title>NIPS 2016</title><link href="/ml/2016-12-16-nips2016_b.html" rel="alternate"></link><published>2016-12-16T00:00:00+00:00</published><updated>2016-12-16T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2016-12-16:/ml/2016-12-16-nips2016_b.html</id><summary type="html">&lt;p&gt;We return for another installment of Stephanie Summarises a Conference. My previous work in this area is &lt;a href="/ml/2015-12-14-nips2015.html"&gt;NIPS 2015&lt;/a&gt;, &lt;a href="/ml/2016-02-17-aaai2016.html"&gt;AAAI 2016&lt;/a&gt;, and &lt;a href="/ml/2016-07-05-icml2016.html"&gt;ICML 2016&lt;/a&gt;. I was pleasantly surprised at NIPS to be asked if I was going to write one of these again. Apparently someone somehow found my blog. Ignorance …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We return for another installment of Stephanie Summarises a Conference. My previous work in this area is &lt;a href="/ml/2015-12-14-nips2015.html"&gt;NIPS 2015&lt;/a&gt;, &lt;a href="/ml/2016-02-17-aaai2016.html"&gt;AAAI 2016&lt;/a&gt;, and &lt;a href="/ml/2016-07-05-icml2016.html"&gt;ICML 2016&lt;/a&gt;. I was pleasantly surprised at NIPS to be asked if I was going to write one of these again. Apparently someone somehow found my blog. Ignorance of this is one of the downsides (??) of not having creepy tracking analytics.&lt;/p&gt;
&lt;p&gt;This time we get a &lt;em&gt;table of contents&lt;/em&gt; so I can be guiltlessly verbose (I fear how long my PhD thesis is going to be):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#wiml"&gt;Women in Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Main Conference&lt;ul&gt;
&lt;li&gt;&lt;a href="#invited"&gt;Invited Talks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#posters"&gt;Posters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#law"&gt;Machine Learning and the Law&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mlhc"&gt;Machine Learning for Healthcare&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#misc"&gt;Miscellaneous Comments/Observations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a name="wiml"&gt;&lt;/a&gt;Women in Machine Learning Workshop&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;"What are women and how can machine learning stop them?"&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I didn't register for WiML in time last year, so this was my first time attending. I also managed to miss all the Sunday events by arriving to Barcelona at midnight that night. There was a workshop on Effective Communication where I could perhaps have learned how to write shorter blog posts.&lt;/p&gt;
&lt;p&gt;My feelings about having 'woman-only/woman-centric' events are complex, poorly-understood and otherwise beyond the scope of this particular post, but the reality is that women are wildly underrepresented in computer science and machine learning is no exception (about 15% of the 6000-odd NIPS attendees were women, and I don't know what fraction of those were recruiters). I'm so used to being surrounded by men that I barely notice it (except for the occasional realisation that I'm the only woman in a room), so having a large conference hall full of women for this workshop was a bit surreal.&lt;/p&gt;
&lt;h3&gt;Interesting talks/posters:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;(talk) &lt;a href="http://maithraraghu.com/"&gt;Maithra Raghu&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1606.05336"&gt;On the expressive power of deep neural networks&lt;/a&gt;. They study the expressive power (ability to accurately represent different functions) of neural networks and show that this depends on a quantity they call 'trajectory length'. There's also a companion paper, &lt;a href="https://arxiv.org/abs/1606.05340"&gt;Exponential expressivity in deep neural networks through transient chaos&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;(poster) Niranjani Prasad, Barbara Engelhardt, Li-Fang Cheng, Corey Chivers, Michael Draugelis and Kai Li. &lt;em&gt;A Reinforcement Learning Approach to Weaning of Mechanical Ventilation in ICU&lt;/em&gt;: relevant to my ICU-interests, but this poster was unfortunately on the &lt;em&gt;other side of the board&lt;/em&gt; to mine, so I only got to look at it briefly. They're using MIMIC-III, looking at pneuomnia patients and the question of intubation. A challenge was engineering the reward function, which required consultation with clinicians.&lt;/li&gt;
&lt;li&gt;(poster) Luisa M Zintgraf, Taco S Cohen, Tameem Adel and Max Welling. &lt;em&gt;Visualizing Deep Neural Network Decisions&lt;/em&gt;. They propose a 'prediction difference analysis' method to visualise regions of an image which either support or oppose a particular prediction. This is based on assigning 'relevance' to parts of the input, based on the 'weight of evidence' a particular input gives to a certain class. This is a pre-existing idea, and a cursory glance at the paper doesn't highlight what's novel about their approach - possibly applying it to deep networks? Extending it to analysing the influence of multiple features at a time, possibly?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I accidentally presented my poster for &lt;em&gt;most&lt;/em&gt; of the poster session and therefore missed out on going around to others. This is a compelling argument for having co-authors who can share the load. For the record, the work I was presenting was &lt;a href="https://arxiv.org/abs/1607.04903"&gt;Learning Unitary Operators with Help from u(n)&lt;/a&gt;, which I did with my advisor Gunnar Rätsch, and which will be appearing in AAAI-17. I also presented it at the Geometry in ML workshop at ICML, see my post &lt;a href="/ml/2016-07-05-icml2016.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Roundtables&lt;/h3&gt;
&lt;p&gt;What I found especially valuable and unique about WiML were the &lt;strong&gt;roundtables&lt;/strong&gt; - one for research advice, one for career guidance. In each one there were subtables for specific topics, with 'experts' to extract wisdom from.&lt;/p&gt;
&lt;p&gt;I shamelessly hogged space at the healthcare research roundtable in the first session to listen to Jennifer Healey. She's a researcher at Intel Labs working on using sensor data for human health. That is, if you have continuous audio recording (as one can get from a phone microphone), you can identify a person coughing, measure qualities of it, its frequency, onset and so on. This information is incredibly valuable for making diagnoses and treatment decisions, and it's the kind of data that one could reasonably imagine everyone collecting in the future. One thing I really enjoyed about the discussion was that she was quite aware of the &lt;em&gt;HORRIFYING PRIVACY IMPLICATIONS&lt;/em&gt; of this kind of data, and the need to avoid storing (and calculating on) this data on The Cloud. I'm really excited about this avenue of healthcare (as I say every time it comes up) and I'm really glad to hear a senior researcher from a big company talking about the importance of the privacy considerations. As was mentioned in the ML and Law symposium, all personal data you collect is a privacy vulnerability. But collecting this data could have such massive positive healthcare implications that 'solving' the privacy problem is really important. Especially if the data is going to end up getting collected anyway...&lt;/p&gt;
&lt;p&gt;The second roundtable I went to (about careers/advice), I spoke to some people at Deepmind about working there (me and everyone else at NIPS, it feels like...), and some other people about how to decide between industry (that is, &lt;em&gt;industrial research&lt;/em&gt;) and academia. Both experts at the industry/academia table were in industry, so I'm not sure I got an unbiased perspective on it. The context for all of this is that I'm a 'late-stage' PhD student (the idea of that is rather scary to me - there's still so much to learn!), so I'm looking for internships (got any spare internships? &lt;a href="https://apeiroto.pe/pages/about.html"&gt;contact me&lt;/a&gt;) and thinking about post-PhD land. The most concrete difference I learned about was that in companies, you may need to send your paper to the legal team before submitting it to a conference, in case they want to patent something first. I'd imagine this also applies to preprints and code and so on. Otherwise, the level of intellectual freedom one enjoys seems to vary, but everyone I spoke to (from a biased sample) seemed largely unconstrained by their industrial ties.&lt;/p&gt;
&lt;p&gt;I'd imagine there's a gulf of misery between brand-new startups that have yet to become overly concerned with Product, and established tech companies with the luxury of blue-skies research labs, where you don't get to do cool things and instead must live in a box desperately trying to demonstrate the commercial viability of your research. I'd also imagine that said box-dwellers don't attend roundtables (how do you fit a round table in a square box?).&lt;/p&gt;
&lt;p&gt;The final notable thing that happened at WiML was me apparently winning a raffle, but being shamefully absent. I was upstairs charging my laptop and catching up with a friend from MLSS, blissfully ignorant of the prize I would never receive.&lt;/p&gt;
&lt;h1&gt;The Main Conference&lt;/h1&gt;
&lt;h2&gt;&lt;a name="invited"&gt;&lt;/a&gt;Invited Talks&lt;/h2&gt;
&lt;p&gt;The main conference opened with a talk (the Posner Lecture) from &lt;strong&gt;Yann LeCun&lt;/strong&gt;. LeCun is famous enough in machine learning that people were excitedly acquiring and then sharing selfies taken with him (a practice I find puzzling), so the things he said will likely echo around the community and I need not repeat them in detail here. In gist he was talking about unsupervised learning (although focusing on a subtle variant he called 'predictive learning'). He used a cake analogy which spawned parodies and further cake references throughout the conference/social media. The analogy is that reward signals (as in reinforcement learning) are the cherry, labels for supervised learning is the icing, and the rest of the cake is essentially unlabelled data which requires unsupervised learning. The growing importance of unsupervised learning is not new, I can say from my intimidating &lt;em&gt;one year&lt;/em&gt; of previous NIPS conferences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Marc Raibert&lt;/strong&gt; from Boston Dynamics gave an entertaining talk about dynamic legged robots. This featured many YouTube videos I'd already seen, but was happy to gormlessly rewatch. One amusing thing is the fact that they can't use hydraulics in domestic robots, because they leak. That's a great example of a &lt;em&gt;real-world&lt;/em&gt; problem. It might be common knowledge amongst roboticists, but 'you can't use hydraulics because nobody wants oil and stuff on their carpet' would not have occurred to me if I for some reason needed to design a robot. Now, maybe I would not need to design a robot directly, but it's not entirely unlikely that I could design an algorithm making assumptions about the kinds of movements, or the cost of those movements, that a robot could make. And this is why 'domain experts' will always be needed. Probably.&lt;/p&gt;
&lt;p&gt;At the end of the talk, someone asked if Boston Dynamcis uses machine learning. They do not. Maybe they should?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Saket Navlakha&lt;/strong&gt; spoke about 'Engineering Principles from Stable and Developing Brains'. Part of this talk was based on &lt;a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004347"&gt;this PLoS CB paper&lt;/a&gt; where they compare neural network development in the brain to that of engineered networks. In brains, connections are created rapidly and excessively, and then pruned back over time dependent on use (they demonstrate this in mouse models). This is to be contrasted with engineered networks, where adding and removing edges in this way would be seen as wasteful. They demonstrate however that the hyper-creation and then aggressive pruning results in improved network function. They're particularly interested in routing networks, so the applicability to artificial neural networks is not immediately apparent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Susan Holmes&lt;/strong&gt; gave the Brieman Lecture, which exists to bridge the gap between the statistics and machine learning communities. This was the &lt;em&gt;single&lt;/em&gt; talk of the conference where I took &lt;a href="https://www.evernote.com/shard/s404/sh/0f22b0fa-65c7-44ac-bd55-551349cc6cbf/2f19eb24c584f88e"&gt;notes&lt;/a&gt;, because the relevance of the topic to me and others in my lab overwhelmed the need to preserve precious limited laptop battery. The title of the talk was "Reproducible Research: the case of the Human Microbiome", and so was mostly a story about how to do reproducible research, in the context of microbiome analysis. One really cool thing she mentioned was a web application called &lt;a href="http://joey711.github.io/shiny-phyloseq/"&gt;shiny-phyloseq&lt;/a&gt;, which seems to be an interactive web interface to their phyloseq package. &lt;em&gt;However&lt;/em&gt;, it also (I think) records what you do with the data as you explore, which you can then export as a markdown file to include with your paper. I try to emulate this by pipelining my analysis in bash scripts (or within python), but having something to passively record as you interactively explore data seems additionally very beneficial. The &lt;a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf"&gt;garden of forking paths&lt;/a&gt; is a risk during any data exploration. Also, the garden of forgetting exactly what preprocessing steps you did.&lt;/p&gt;
&lt;p&gt;There was a touching memorial to &lt;a href="http://www.inference.phy.cam.ac.uk/mackay/"&gt;Sir David MacKay&lt;/a&gt; during one of the sessions. It's easy, as an early-stage scientist, to get swept up in the negative aspects of academic culture (looking at you, Publish or Perish) and lose sight of the reasons for doing any of this. Hearing about scientists like MacKay, who both think and care deeply, is genuinely inspirational. The only book on my Christmas wishlist this year is "Information Theory, Inference, and Learning Algorithms".&lt;/p&gt;
&lt;h2&gt;&lt;a name="posters"&gt;&lt;/a&gt;Interesting Papers/Posters&lt;/h2&gt;
&lt;p&gt;Necessarily, a subset of the interesting work.&lt;/p&gt;
&lt;h3&gt;Misc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1602.03534"&gt;Learning Transferrable Representations for Unsupervised Domain Adaptation&lt;/a&gt; - &lt;em&gt;Ozan Sener · Hyun Oh Song · Ashutosh Saxena · Silvio Savarese&lt;/em&gt; - jointly learn representation, cross-domain transformation as well as labels to do better domain adaptation.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://people.csail.mit.edu/beenkim/papers/KIM2016NIPS_MMD.pdf"&gt;Examples are not enough, learn to criticize! Criticism for Interpretability&lt;/a&gt; - &lt;em&gt;Been Kim · Oluwasanmi Koyejo · Rajiv Khanna&lt;/em&gt; - this was a great poster and spotlight talk. The idea is this: to help make sense of massive datasets, we ideally identify some 'representative samples' ('prototypes') which we can manually assess and use to generalise about the rest of the data. The danger is that there will be non-stereotypical data points, which are nonetheless represented in the data and should be considered. They call these examples 'criticisms', and describe an approach to generate both prototypes and criticisms from large datasets.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1606.09184"&gt;Disease Trajectory Maps&lt;/a&gt; - &lt;em&gt;Peter Schulam, Raman Arora&lt;/em&gt; - the objective here is to find latent representations of patient trajectories, and then characterise them (i.e. through clustering). They use a fairly complicated probabilistic model to do this, so the more interesting details are in the paper. They also associate the representations with clinical outcomes to prove that they're 'clinically meaningful', comparing with some other methods of representing time series.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Reinforcement Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1606.03137"&gt;Cooperative Inverse Reinforcement Learning&lt;/a&gt; - &lt;em&gt;Dylan Hadfield-Menell · Stuart J Russell · Pieter Abbeel · Anca Dragan&lt;/em&gt; - in traditional inverse reinforcement learning (IRL), the agent tries to learn the expert's reward function. However, to have benevolent robots, we would like them to maximise rewards &lt;em&gt;for humans&lt;/em&gt;, not themselves. Additionally, in IRL the agent observes assumed-optimal expert trajectories, which may nonetheless be &lt;em&gt;sub&lt;/em&gt;-optimal for learning - one would rather generate teaching, or demonstration trajectories. They formulate a solution to these concerns as a two-player game with learning and acting (deployment) phases.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cushmanlab.fas.harvard.edu/docs/NIPS_2016_Teaching_by_demonstration_w_supplementary.pdf"&gt;Showing versus doing: Teaching by demonstration&lt;/a&gt; - &lt;em&gt;Mark K Ho · Michael Littman · James MacGlashan · Fiery Cushman · Joe Austerweil · Joseph L Austerweil&lt;/em&gt; - this work focuses on the second issue raised in the previous one - how does a &lt;em&gt;teaching&lt;/em&gt; trajectory differ from a &lt;em&gt;doing&lt;/em&gt; trajectory? They formulate it as 'Pedagogical Inverse Reinforcement Learning'd. What's really neat about this work is that they actually did experiments with humans to validate their model's predictions about how people would behave while trying to teach versus simply doing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1606.02647"&gt;Safe and Efficient Off-Policy Reinforcement Learning&lt;/a&gt; - &lt;em&gt;Remi Munos · Tom Stepleton · Anna Harutyunyan · Marc Bellemare&lt;/em&gt; - 'safety' in this work refers to the capacity of the algorithm to deal with arbitrary 'off-policyness' (that is, the policy to evaluate and the behaviour policy observed need not be close), and 'efficiency' refers to using data ... efficiently. The work seems to combine previous approaches which are either safe or efficient into an algorithm enjoying the benefits of both, with various theoretical results.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1606.04753"&gt;Safe Exploration in Finite Markov Decision Processes with Gaussian Processes&lt;/a&gt; - &lt;em&gt;Matteo Turchetta · Felix Berkenkamp · Andreas Krause&lt;/em&gt; - 'safe' here roughly has its common meaning. They address the issue where an agent, looking to maximise long-term (discounted, perhaps) reward, is willing to tolerate temporary very negative rewards. This is unacceptable for safety-critical agents - they used the example of a Mars rover getting stuck in a crater - so they develop an algorithm (SafeMDP) to &lt;em&gt;safely&lt;/em&gt; explore, avoiding unsafe states/actions using noisy observations from nearby states. They also ensure the agent can't get stuck in states without safe escape routes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Recurrent Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1605.07571"&gt;Sequential Neural Models with Stochastic Layers&lt;/a&gt; - &lt;em&gt;Marco Fraccaro · Søren Kaae Sønderby · Ulrich Paquet · Ole Winther&lt;/em&gt; - they combine state-space models (uncertainty about states) with recurrent neural networks (sequential, long time dependencies), and describe a variational inference procedure for the model.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1610.09513"&gt;Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences&lt;/a&gt; - &lt;em&gt;Daniel Neil · Michael Pfeiffer · Shih-Chii Liu&lt;/em&gt; - they add a time gate to the LSTM unit, which has a parametrized oscillation frequency, controlling when individual parts of the memory cell can be updated. This allows for irregularly sampled sensor data to be integrated and they demonstrate improved performance on long memory tasks. They also have really nice figures.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1611.00035"&gt;Full-Capacity Unitary Recurrent Neural Networks&lt;/a&gt; - &lt;em&gt;Scott Wisdom · Thomas Powers · John Hershey · Jonathan Le Roux · Les Atlas&lt;/em&gt; - this is pretty relevant for/similar to my &lt;a href="https://arxiv.org/abs/1607.04903"&gt;recent work&lt;/a&gt;, so I'm going to read this paper in detail later. My initial thought upon seeing the poster is that they have some &lt;em&gt;really unnecessary&lt;/em&gt; mathematics in there, which also appears in the manuscript - the entirety of section three in their paper is self-evident. I'm a bit concerned that reviewers might think well-known mathematical facts restated as 'theorems' may constitute novel results. &lt;em&gt;Anyway&lt;/em&gt; cattiness aside, their model is interestingly different to my approach - they optimise on the Stiefel manifold of unitary matrices directly (I optimise in the Lie algebra), although if you define the Riemannian gradient using inner products on the tangent space, this &lt;em&gt;probably&lt;/em&gt; becomes equivalent in some sense. It requires further analysis. Their results seem quite impressive, although they don't do a comprehensive comparison on the same experiments as &lt;a href="https://arxiv.org/abs/1511.06464"&gt;Arjovsky &amp;amp; Shah&lt;/a&gt;, which are the ones I'm familiar with. I had a nice conversation with one of the authors at the poster, which is really what conferences are about.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1608.05745"&gt;RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism&lt;/a&gt; - &lt;em&gt;Edward Choi · Mohammad Taha Bahadori · Joshua Kulas · Jimeng Sun · Andy Schuetz · Walter Stewart&lt;/em&gt; - their focus here is to have an &lt;em&gt;interpretable&lt;/em&gt; model, so the evidence used to make a decision is easily identified. They achieve this using an attention mechanism where the recurrence is on the &lt;em&gt;attention mechanism&lt;/em&gt;, not on the hidden state. I'm not sure why RNNs should be seen as intrinsically uninterpretable (you can get gradients of cost with respect to any input, for example), so I'm going to think about this more. Interpretability is crucial for any medical applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a name="law"&gt;&lt;/a&gt;Machine Learning and the Law Symposium&lt;/h2&gt;
&lt;p&gt;I was and remain to be confused by the choice of symposia. The options were: Deep Learning, Recurrent Neural Networks, and ML and the Law. RNNs aren't deep? What was the DL symposium covering? Deep but Not Recurrent Learning? Weight-Sharing Is OK but Not Over Time, Never Over Time? As evidenced by the title of this section, I didn't attend either of them, and I also didn't attend enough of the Counterfactual Reasoning workshop on Saturday to say what would have happened if I had gone to them, but there seems to be a naming/scope issue here. Whatever it was, the RNN Symposium was Hot Shit and had to switch rooms with us ML+Law people during the lunch break. As soon as the room change was announced, people started appearing at the fringes of the Law symposium and may have been inadvertently exposed to some meta-ethics. I'm not sure how this planning error occurred - it is natural to assume that most of the growth in NIPS attendance is coming from &lt;em&gt;DEEP LEARNING&lt;/em&gt;, which should (??) include RNNs, so that symposium was likely to be popular. Maybe they thought enough people would go to the &lt;em&gt;other&lt;/em&gt; DL symposium.&lt;/p&gt;
&lt;p&gt;The real question is - did non-DL non-justice machine learners feel cheated of a symposium? Am I wrong to try to place the RNN symposium inside the DL one?&lt;/p&gt;
&lt;p&gt;Having just published a &lt;a href="https://arxiv.org/abs/1607.04903"&gt;paper (arguably) about RNNs&lt;/a&gt;, I &lt;em&gt;should&lt;/em&gt; have gone to the RNN symposium, but I can't resist thinking about the broader social impact of machine learning. I've also found myself thinking about morality and justice (and therefore law) more than usual lately, so I had to attend this. Discussions of normative ethics at a machine learning conference? Yes.&lt;/p&gt;
&lt;p&gt;I'd consider this symposium a law-oriented follow-on to the 'Algorithms Among Us: the Societal Impacst of Machine Learning' symposium at NIPS 2015 (see my summary &lt;a href="/ml/2015-12-14-nips2015.html"&gt;here&lt;/a&gt;. Having a focus is good. The impacts of machine learning on society are widespread, so trying to cover too many all forces a shallower treatment. High level talk is well and good, but &lt;em&gt;getting stuff done&lt;/em&gt; requires being specific. This is actually a point that was raised during one of the panel discussions: how do we balance the need in computational science to formulate very specific, quantified definitions of things (like discrimination) with the requirement of margin of interpretation in law? I was surprised, as a non-lawyer, to hear that such ambiguity could be tolerated, much less &lt;em&gt;desired&lt;/em&gt;. The example given for this was in discussions where compromise may only be attained through baking some ambiguity into an agreement, which would then (I suppose) later be argued over as necessary. This leads to another point which was made - law is not a monolith, laws are not absolute immutable statements - law is a &lt;em&gt;process&lt;/em&gt;, an argumentative tradition (at least in the US), evolving and iterating and requiring justification at all times (get it - &lt;em&gt;justice pun&lt;/em&gt;!). How to integrate algorithms into this process is not as simple as treating them as Truth Functions (shout out to my main man Wittgenstein) on Evidence ... &lt;em&gt;or is it?&lt;/em&gt; I get ahead of myself.&lt;/p&gt;
&lt;h3&gt;Legal Perspectives&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.iankerr.ca/"&gt;Ian Kerr&lt;/a&gt;, &lt;em&gt;Learned justice: prediction machines and big picture privacy&lt;/em&gt;. The 'learned' in the title is partially a reference to the US judge &lt;a href="https://en.wikipedia.org/wiki/Learned_Hand"&gt;Learned Hand&lt;/a&gt; (&lt;em&gt;what a name&lt;/em&gt;). A quote from him, "If we are to keep our democracy, there must be one commandment: Thou shalt not ration justice". As an example of a 'learned AI' he mentioned &lt;a href="www.donotpay.co.uk/"&gt;'The World's First Robot Lawyer'&lt;/a&gt;, which helps people generate appeal letters. It's actually a pretty standard chat bot, but it's helped to overturn over 160,000 parking tickets in London and New York, which is a &lt;em&gt;massive&lt;/em&gt; impact ('helping to protecte vulnerable people from state coercion'). What could we do with more powerful algorithms? He then spoke about &lt;em&gt;prediction&lt;/em&gt;, highlighting the links between prediction, preemption, and presumption. This brought us to the &lt;a href="https://en.wikipedia.org/wiki/Prediction_theory_of_law"&gt;prediction theory of law&lt;/a&gt;, an idea coming from the legal scholar &lt;a href="https://en.wikipedia.org/wiki/Oliver_Wendell_Holmes_Jr."&gt;Oliver Wendell Holmes&lt;/a&gt;. This is the idea that 'the law' is simply about predicting what the courts will do, and nothing else. So the study of law is the study of prediction, not morality or anything else. He went on to talk about the 'reasonable expectation of privacy' which is required to understand the scope of the 4th Amendment of the US Constitution. The difficult part is &lt;em&gt;not&lt;/em&gt; defining 'reasonable', but rather 'expectation'. What does this word mean? There are two interpretations: it could be &lt;em&gt;normative&lt;/em&gt;, or &lt;em&gt;predictive&lt;/em&gt;. The US courts have taken the latter stance, and one's 'expectation' of privacy therefore depends on what is &lt;em&gt;possible&lt;/em&gt; with generally-available technology. This is terrifying - if I know my phone microphone is always on, and my phone is at risk of being hacked, do I lose the expectation of privacy whenever my phone is on me?&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.vub.ac.be/LSTS/members/hildebrandt/"&gt;Mireille Hildebrandt&lt;/a&gt;, &lt;em&gt;No Free Lunch&lt;/em&gt;. One particularly pertinent thing she spoke about was 'Data &amp;amp; Pattern Obesitas'. That is, there is a general desire to collect as much data as possible, to look for as many patterns as possible, simply &lt;em&gt;because&lt;/em&gt;. This is dangerous for several reasons, the most obvious of which being that any personal data that is stored is a security risk (looking at you, Big Healthcare Databases). And so she highlighted the importance of &lt;em&gt;salience of purpose&lt;/em&gt;, citing the security adage of 'select before you collect'. I think this idea likely goes against the inclinations of many researchers in machine learning/data science, who would rather grab everything, and do some sort of automated relevance detection later. This may be fine in certain domains, but when the data you're operating on is sensitive in some way, it can be fatal.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Prediction_theory_of_law"&gt;Deirdre Mulligan&lt;/a&gt; - &lt;em&gt;Governance and Machine Learning&lt;/em&gt;: there was not &lt;em&gt;so much&lt;/em&gt; machine learning in this talk, but she spoke about various ways technology and governance interact. Voting machines are one obvious place (and topical!). She spoke about how electronic voting systems failed to reproduce the traditional voting system. In pen-and-paper voting, the ballot is a physical artefact of the vote, but in these systems, apparently it was rendered on the fly and not saved. There was no storage of the ballot image, it simply incremented a counter somewhere in the backend. This is obviously a terrible system, but these machines were closed-source (!?!?!), so I guess nobody realised they were working like this until they reverse-engineered them? The mind boggles. Other examples are automobiles - you can hack them (like everything on the IoT), they were avoiding regulation (Volkswagen), product safety was compromised by software updates. The last case highlights the need for certification and verification of post-purchase software updates. If you want to run Windows XP on your computer that's your own business, but unsafe cars (from either software or hardware) are public safety risks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Technical Perspectives&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.cis.upenn.edu/~aaroth/"&gt;Aaron Roth&lt;/a&gt;: &lt;em&gt;Quantitative tradeoffs between fairness and accuracy in machine learning&lt;/em&gt; - Rawls provides a definition of fairness, which is "fair equality of opportunity", which he formalised using a 'discrimination index' - the probability of victimisation (not being selected despite being the most qualified, I &lt;em&gt;think&lt;/em&gt;) conditional on being present at a bad round (a round in which a sub-optimal applicant is selected). This was all formulated in a contextual bandit setting, and he described an algorithm called 'fairUCB' (from UCB - upper confidence bound, a standard bandit algorithm) and gave its regret bound.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://people.mpi-sws.org/~gummadi/"&gt;Krishna P. Gummadi&lt;/a&gt;: &lt;em&gt;Measures of fairness, and mechanisms to mitigate unfairness&lt;/em&gt; - the focus here was on &lt;em&gt;discrimination&lt;/em&gt;, which is a specific kind of unfairness. So what is discrimination? A definition is "wrongfully imposing relative disadvantage based on membership in socially salient groups". One could ask what most of these terms mean &lt;em&gt;exactly&lt;/em&gt; (and indeed, we must, if we want to computationally model anything), but he focused on the phrase "based on". Some attributes are sensitive, and some are not. Can you simply ignore them? The problem is that, people in different sensitive attribute groups may have &lt;em&gt;different&lt;/em&gt; non-sensitive feature distributions, which risks disparate mistreatment and disparate impact. One can test disparity of impact through, for example, proportionality tests, e.g. "an 80% rule" - if 50% of men are accepted, then 40% of women should too. And a shout-out to &lt;a href="http://www.fatml.org/"&gt;Fairness, Accountability and Transparency in ML&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There were more talks, but I was drifting into the semi-delirious pre-fever stages of the Conference Flu at this point.&lt;/p&gt;
&lt;h3&gt;Panel Discussions&lt;/h3&gt;
&lt;p&gt;The discussion spotlight was 'Regulation by Machine' from &lt;a href="http://www.law.utoronto.ca/faculty-staff/full-time-faculty/benjamin-alarie"&gt;Benjamin Alarie&lt;/a&gt;. A question - how to use AI to make better laws? My notes are sparse but a recurring theme (also in MLHC) is that we should use machine learning to &lt;em&gt;help and augment&lt;/em&gt; humans, not to replace them. So he was speaking about using ML to - for example - help to predict if it's 'worth' taking a case to court. Apparently many cases go to court which are 'overdetermined given the facts', and it's somewhat easy (citation needed) for an algorithm to identify which these are.&lt;/p&gt;
&lt;p&gt;My notes on the actual panel are &lt;em&gt;sketchy at best&lt;/em&gt;. It may have been the time or how sick I was but, it felt like people were saying a lot of interesting things without obvious argumentative structure or direction, so it's hard to summarise any &lt;em&gt;salient points&lt;/em&gt;. Here are some decontextualised, paraphrased snippets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deirde Mulligan: the judicial system is not always about applying the same law the same way. You must know the facts, the context... The law wants you to come in and argue about what it means. You can go to court to change the law (she asked how many people had been to court - a couple raised their hands - I've only been to court as a juror). Any algorithm for the law must be both performative and output-focused.&lt;/li&gt;
&lt;li&gt;Neil Lawrence: how do judges come to opinions? Also, "I don't want to talk too much as I'm not on the panel."&lt;/li&gt;
&lt;li&gt;??? (unknown panel member) - we're assuming the law will furnish us with specific definitions, but actually, policies breed on, thrive on, require a lack of specificity and precision - ambiguity is &lt;em&gt;not&lt;/em&gt; an accident!&lt;/li&gt;
&lt;li&gt;Ian Kerr: &lt;a href="https://en.wikipedia.org/wiki/Paul_the_Octopus"&gt;Paul the Octopus&lt;/a&gt; was highly accurate, but does that mean we should trust it?&lt;/li&gt;
&lt;li&gt;Deirde: shout-out to &lt;a href="https://www.nolo.com/"&gt;Nolo press&lt;/a&gt;, making the law easier to understand. Especially important in areas where the cost of fighting something isn't worth it...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And a final shoutout to &lt;a href="http://robots.law.miami.edu/2014/wp-content/uploads/2013/06/Chief-Justice-John-Roberts-is-a-Robot-March-13-.pdf"&gt;Chief Justice John Roberts is a Robot&lt;/a&gt; - &lt;em&gt;Ian Kerr and Carissima Mathen&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a name="mlhc"&gt;&lt;/a&gt;Machine Learning for Healthcare Workshop&lt;/h2&gt;
&lt;p&gt;With the caveat that these are &lt;em&gt;workshop contributions&lt;/em&gt;, here are some interesting papers/posters (with accompanying arXiv papers, so I have a chance to remember anything about them):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1612.02460"&gt;Demographical Priors for Health Conditions Diagnosis Using Medicare Data&lt;/a&gt; - &lt;em&gt;Fahad Alhasoun, May Alhazzani, Marta C. González&lt;/em&gt; - they look at insurance claims data from Brazil over a 15 month period - about 6.6 million visits. They represent ICD-10 codes by their distribution over ages (a 100-dimensional normalised vector) and do clustering on this representation.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1610.08735"&gt;Stratification of patient trajectories using covariate latent variable models&lt;/a&gt; - &lt;em&gt;Kieran R. Campbell, Christopher Yau&lt;/em&gt; - they describe a kind of linear latent variable model taking patient covariates into account, and use it on a TCGA RNAseq dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1611.07663"&gt;Learning Cost-Effective and Interpretable Regimes for Treatment Recommendation&lt;/a&gt; - &lt;em&gt;Himabindu Lakkaraju, Cynthia Rudin&lt;/em&gt; - related (possibly extended version) paper here: &lt;a href="https://arxiv.org/abs/1610.06972"&gt;Learning Cost-Effective Treatment Regimes using Markov Decision Processes&lt;/a&gt;. The 'interpretability' comes in here because their &lt;em&gt;state space&lt;/em&gt; (of the MDP) consists of the &lt;em&gt;effects&lt;/em&gt; on their patient population of decision lists - ordered lists of rules, each consisting of tuples of predicates (like, properties a patient must fulfill) and actions.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1612.01055"&gt;Modeling trajectories of mental health: challenges and opportunities&lt;/a&gt; - &lt;em&gt;Lauren Erdman, Ekansh Sharma, Eva Unternahrer, Shantala Hari Dass, Kieran ODonnell, Sara Mostafavi, Rachel Edgar, Michael Kobor, Helene Gaudreau, Michael Meaney, Anna Goldenberg&lt;/em&gt; - they're interested identifying subtypes of mental illness using time series, and predicting future phenotypic values. They use a Dirichlet Process-Gaussian Process and compare with latent class mixed models, finding that the LCMMs are actually as good as the DP-GP, although neither model is yet good enough for clinical use.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1612.00475"&gt;Transfer Learning Across Patient Variations with Hidden Parameter Markov Decision Processes&lt;/a&gt; - &lt;em&gt;Taylor Killian, George Konidaris, Finale Doshi-Velez&lt;/em&gt; - they're concerned with patient heterogeneity, and cast this as a multitask learning problem, where different tasks are different patients. They share information between tasks using a GP-LVM, removing the requirement to visit every state to learn the dynamics (which is, of course, infeasible in medicine). -&lt;a href="https://arxiv.org/abs/1612.00611"&gt;Predictive Clinical Decision Support System with RNN Encoding and Tensor Decoding&lt;/a&gt; - &lt;em&gt;Yinchong Yang, Peter A. Fasching, Markus Wallwiener, Tanja N. Fehm, Sara Y. Brucker Volker Tresp&lt;/em&gt; - they represent the patient's time series with a LSTM encoder and concatenate the static information into a representation.As a decoder, they use tensor factorisation. I'm not entirely clear on what is actually contained in this tensor, so the paper will need to be read more carefully.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://affect.media.mit.edu/pdfs/16.Jaques-Taylor-et-al-PredictingHealthStressHappiness.pdf"&gt;Multi-task Learning for Predicting Health, Stress, and Happiness&lt;/a&gt; - &lt;em&gt;Natasha Jaques, Sara Taylor, Ehimwenma Nosakhare, Akane Sano, Rosalind Picard&lt;/em&gt; - they have wearable sensors and smartphone logs from 30 days of monitoring. They looked at three multi-task approaches: multi-task multi-kernel learning, hierarchical bayes with Dirichlet process priors, neural networks (sharing hidden layers), and single-task versions of all of these.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mandatory shout-out to my contribution to the workshop: - &lt;a href="https://arxiv.org/abs/1612.00467"&gt;Neural Document Embeddings for Intensive Care Patient Mortality Prediction&lt;/a&gt; - &lt;em&gt;Paulina Grnarova, Florian Schmidt, Stephanie L. Hyland, Carsten Eickhoff&lt;/em&gt; - we used document embeddings to predict patient mortality in MIMIC-III, purely using text notes. The embedding procedure uses two layers of CNNs - word vectors are combined into sentence vectors (with a CNN), and sentence vectors are combined into patient vectors (with a CNN), and we use target replication to improve predictive accuracy. This was fairly preliminary (there are many other factors to consider, as ever), but we beat previous work using topic modelling on the task, which is encouraging, and perhaps unsurprising given LDA's inability to deal with multi-word phrases.&lt;/p&gt;
&lt;p&gt;This is only a snippet of the interesting work presented at the workshop. I unfortunately came down with Conference Flu about half way through NIPS, and was at my sickest during the MLHC workshop (ironically), so I didn't get to speak to as many poster presenters as I would have liked.&lt;/p&gt;
&lt;h2&gt;&lt;a name="misc"&gt;&lt;/a&gt;Miscellaneous Comments/Observations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Generative Adversarial Networks are super hot right now, and by saying this I am contributing to the hype.&lt;/li&gt;
&lt;li&gt;Despite having around 6000 attendees, NIPS didn't feel overcrowded (contrast with ICML this year). I'm guessing this was a combination of having an appropriately-sized venue and good crowd-control from the venue staff (they were closing off the top floor when it got too full), or maybe everyone was just busy enjoying Barcelona.&lt;/li&gt;
&lt;li&gt;Being a vegetarian in Spain sucks. Given my diet was largely eggs, potatoes and bread for the week, I feel sorry for the vegans in the NIPS community. I for one devolved into a patatas-bravas guzzling monster and don't want to even think about tapas for the foreseeable future.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a name="conclusion"&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I feel less obviously exuberant about NIPS than I did last year, which I attribute to a combination of having been (and continuing to be somewhat) ill, and being in the development stage of several new projects where I just want to be &lt;em&gt;getting stuff done&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As I've mentioned before, I think about approaching research in an exploration-exploitation framework. At this NIPS I realised that even within the exploration mode, one can explore exploitatively. That is, you can distinguish between diversity-increasing exploration (seeing areas of the state space/field you've never been in before) and depth-increasing exploration (refining your knowledge of partially-explored states/topics). The latter is arguably a kind of exploitation, because it's exploration with the aim to increase knowledge of things you are &lt;em&gt;intending&lt;/em&gt; to use later. You hope.&lt;/p&gt;
&lt;p&gt;Bringing this strained analogy back to conferences, this makes the difference between going to talks on things you already sort of know and going to &lt;em&gt;totally new&lt;/em&gt; topics. I tried a bit of the latter, because chances are I'm going to read papers relevant to me &lt;em&gt;regardless&lt;/em&gt;, but I found spotlight talks suboptimal for learning new ideas without sufficient background knowledge. An alternative approach would be to be incredibly exploitative, pre-emptively read the relevant papers and then talk to the authors at the poster sessions. Perhaps next year I'll be organised enough to do that, because unless you go to the tutorials, 15-minute talks of questionable presentation quality on cutting edge research are not good ways to learn new topics.&lt;/p&gt;
&lt;p&gt;What &lt;em&gt;is&lt;/em&gt; a good way to learn a new topic (personally), is to write about it. I've been working on a pedagogical post about sparse Gaussian process classification, which will be up next, after a brief diversion into roller derby.&lt;/p&gt;</content><category term="nips"></category><category term="conference"></category><category term="barcelona"></category><category term="spain"></category><category term="AI"></category><category term="MLHC"></category><category term="law"></category></entry><entry><title>roller derby week 4</title><link href="/life/2016-09-29-roller-derby-4.html" rel="alternate"></link><published>2016-09-29T00:00:00+01:00</published><updated>2016-09-29T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2016-09-29:/life/2016-09-29-roller-derby-4.html</id><summary type="html">&lt;p&gt;&lt;em&gt;See week 3 &lt;a href="/life/2016-09-24-roller-derby-3.html"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It had occurred to me that I might injure myself while practising, and I had thought about avoiding practising on the weekends to give myself time to heal before Monday. I think about a lot of things that don't happen. So having somehow intensified the the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;See week 3 &lt;a href="/life/2016-09-24-roller-derby-3.html"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It had occurred to me that I might injure myself while practising, and I had thought about avoiding practising on the weekends to give myself time to heal before Monday. I think about a lot of things that don't happen. So having somehow intensified the the bruise on my thigh on Sunday afternoon, I spent Monday afternoon trying to talk myself out of goinng to training. On one hand, my body was &lt;em&gt;fine&lt;/em&gt;... everywhere except the bruise. On the other hand, even light touches against the bruise were bafflingly painful. On another part of the second hand, apparently that place is where I fall, and apparently falling is a thing I do. I consulted the internet on the consequences of bruising already-bruised skin and it told me I have cancer, so I went back pensively writing SQL queries while waiting for the subconscious to figure it out.&lt;/p&gt;
&lt;p&gt;In the end, I printed off the &lt;a href="https://wftda.com/rules"&gt;rules of flat track roller derby&lt;/a&gt; and went to training, where I sat solemnly in the centre of the track, huddled in my antiquated Pirate Party hoodie, gazing jealously at the rest of the newbies. I figured, even if I'm not skating, I can learn something by watching and listening. &lt;em&gt;Or&lt;/em&gt;, instead of watching and listening, I could continue to deliberate about joining in, while slowly succumbing to the clammy sense of inadequacy inspired by watching other people get better while I sit on my partially-broken ass shivering like an abandoned dog. I stand by my decisions.&lt;/p&gt;
&lt;p&gt;They covered backwards skating, transitions (switching between forward and backwards skating), and derby stops. A derby stop is roughly when you turn around to use your toe stops to stop (in quad skates, the rubber stop thing is on the &lt;em&gt;front&lt;/em&gt; of the shoe, as opposed to the back on inline skates). I just went looking for an illustrative gif and fell into a hole of roller derby gifs followed by kentucky derby puppy gifs, so I offer no further explanation of derby stops.&lt;/p&gt;
&lt;p&gt;Transitions are worrying for me, because one approach (the simpler/easier one, possibly) is to do a mohawk turn. This amounts to briefly going into first or second position in ballet, where your feet form a line with toes pointing outwards. I have &lt;em&gt;really tight&lt;/em&gt; hips, which has not served me well through many hours of ballet and yoga (I can't sit cross-legged, to the confusion of many), so achieving that position with my feet takes time and hurts. I have yet to properly learn transitions (I have yet to properly learn skating backwards), but so far I have been using a weird combination of multi-step-hops and spinning on my toes to turn around. Time will tell if these are acceptable methods, cause I don't see mohawk turns happening any time soon. Maybe I'll skip that part and go straight to jumping 180 degrees.&lt;/p&gt;
&lt;p&gt;Later that week, something bizarre happened. Deflated after sitting out of training, and struggling to pull myself out of silver league in Overwatch, I tried some very casual, very careful derby practice at home. My room is small and the rest of the house contains far too many breakable objects to practice in, so home-practice consists of putting my skates on and then rolling carefully back and forth between my computer desk and my wardrobe. One day I will stream myself skating around my room while waiting to respawn in Overwatch, and it will be beautiful and terrible. The bizarre thing was that when I put my skates on, instead of my body seizing up in terror, I continued to feel like a normal person capable of controlling my legs. That is when I discovered I could spin on my front wheels. Had I always felt like this? Was the fear a strange dream? Had I actually found it difficult to do side lunges before? Was I suffering from some temporary bruise-induced delusion? I honestly don't know what happened. It is unsurprising that I would become more comfortable with practice, but I didn't expect it to be a step function. That's not to say that I feel entirely comfortable on skates (&lt;em&gt;lord&lt;/em&gt; no), but it seems my fear is now focused on new things (like transitions and crossovers), instead of &lt;em&gt;everything&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Gear talk interlude: The 'newbie' skates I bought are Riedell R3s, which come with PowerDyne (round, adjustable) toe stops. It turns out that these are on the smaller end for toe stops, so balancing on them is a &lt;em&gt;little&lt;/em&gt; like balancing on high heels (except, you know, on the toe). Natalie has voiced concerns about twisting her ankle while trying to do a derby stop with these, so we are going to order bigger toe stops, most likely Gumballs. Hobbies: never not incurring costs.&lt;/p&gt;
&lt;p&gt;On Saturday we found a carpark by Zurich airport for practice. It was a &lt;em&gt;little&lt;/em&gt; too inclined for me to feel happy doing much more than intensely failing at slalom, but I also took some time to explain crossover mechanics to Natalie. I can &lt;em&gt;understand&lt;/em&gt; things without being able to &lt;em&gt;do&lt;/em&gt; them, a fact which is persistently frustrating. Part of this explanation involved me standing mostly-still and crossing one foot over the other, which is not a thing I thought I could do. In fact, that is a thing which I explicitly said I couldn't do &lt;a href="/life/2016-09-15-roller-derby-2.html"&gt;two weeks earlier&lt;/a&gt;, so I was astonished and smug in equal measure for the rest of the day. Also a wizard. I have a quiet confidence that I could do crossovers if I tried now, but at the time of writing, I've had The Bruise for almost three weeks and it's &lt;em&gt;still&lt;/em&gt; there and &lt;em&gt;still&lt;/em&gt; (somewhat) painful. During the earlier gif tangent I found a catalogue of derby bruises, which begs the question: how is everyone's first bruise so goddamn &lt;em&gt;small&lt;/em&gt;?&lt;/p&gt;</content><category term="roller derby"></category></entry><entry><title>roller derby week 3</title><link href="/life/2016-09-24-roller-derby-3.html" rel="alternate"></link><published>2016-09-24T00:00:00+01:00</published><updated>2016-09-24T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2016-09-24:/life/2016-09-24-roller-derby-3.html</id><summary type="html">&lt;p&gt;&lt;em&gt;See week 2 &lt;a href="/life/2016-09-15-roller-derby-2.html"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Having discovered and then fixed an embarassingly serious bug in my code on Friday, I spent the weekend before the paper deadline at the office rerunning experiments. This is a slow process - for whatever reason, tensorflow takes several minutes to fully initialise the computation graph for …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;See week 2 &lt;a href="/life/2016-09-15-roller-derby-2.html"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Having discovered and then fixed an embarassingly serious bug in my code on Friday, I spent the weekend before the paper deadline at the office rerunning experiments. This is a slow process - for whatever reason, tensorflow takes several minutes to fully initialise the computation graph for my experiments, so there's a decent lag before even first results start coming out. Delays like this are frustrating because they're too long to spend staring at the screen or otherwise doing nothing, yet too short to properly do anything else. Sure, I could practice mindfulness meditation or skim abstracts or read emails or temporarily intensify the attention I am paying to the Hamilton soundtrack constantly playing in the background, but I am a human and I have limits. That weekend, I capitalised on the solitude of a Swiss office on a Sunday to practice some skating. That means putting on all my gear, sitting quietly at my desk typing, and then doing laps up and down the corridor while tensorflow backpropagates through time.&lt;/p&gt;
&lt;p&gt;The corridor is long and smooth and mostly empty, but it's not especially &lt;em&gt;wide&lt;/em&gt;, so corners and crossovers and such were out of the question. I skated up and down and bumped gracelessly into walls on either end and then I somehow, just, sort of &lt;em&gt;got&lt;/em&gt; stickyfeet. What had been demanding and somehow counterproductive became obvious and natural. What did I figure out? Physical actions are hard to explain, but here goes. It has to do with the distribution of weight/balance on the different wheels. So the situation with basic stickyfeet is that you're keeping both skates on the ground, but propelling yourself forward by moving your legs 'out', while your toes point out a bit (if your toes point in, you go backwards and then die). But it's not just 'move your legs out'. &lt;em&gt;For me&lt;/em&gt;, it seems that I need to release some weight from my &lt;em&gt;front&lt;/em&gt; wheels to facilitate the forward moving. I asked Natalie and she thought it might be outer/inner wheels, so maybe mileage varies here. Whatever it is, I'm pretty sure the DerbyNoob Stance of attempting to cling to the ground through one's skates is directly in opposition to the kind of subtle balance shifts required to actually do anything beyond scrabble desperately.&lt;/p&gt;
&lt;p&gt;I also did a bit of backwards stickyfeet-skating, because the movement is the same, just reversed somehow. If I thought I would have been skating backwards (for some definition of skating - I am bad at backwards) within a month of putting skates on, I would not have believed.&lt;/p&gt;
&lt;p&gt;In class week three, we did:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;slalom: This is something from skiing, I think? I did cross-country skiing once. When I was growing up, skiing was the domain of private school kids and my family in Chile. In derby it's weaving between cones. Current status: nah, not really, nope. I just can't turn that tightly. I tried weaving between every &lt;em&gt;second&lt;/em&gt; cone and that was almost possible. However, bizarrely, later in the class we had to weave between skaters and I could do that. Maybe staring at cones on the ground makes skating harder. Maybe I'm just afraid of cones. &lt;/li&gt;
&lt;li&gt;hopping: I mentioned in &lt;a href="/life/2016-08-28-roller-derby-0.html"&gt;week 0&lt;/a&gt; that I'm able to jump on the surface at the gym. This is true. You know what happens when I jump on other surfaces? I'd upload a picture of The Bruise but it's on that part of the leg where ass becomes thigh, the part of the leg you can't just display unless you're at a beach or in a sexually liberated society. The forbidden leg zone. Let me give myself some credit here. We had to jump over a small pile of cones (&lt;em&gt;cones&lt;/em&gt;, you mysterious bastards). My initial reaction was "no, hell no, jesus no" but then I remembered that all is meaningless and came to peace with the jumble of bones and gristle that would soon constitute my body, and I gave it a go. The first few times I basically just landed too hard on the front wheels and kept going, either hitting my toe stops or falling on my knees. A goldilocks siuation was afoot - that was too much front, so I overcompensated the next time and went too much back. Instead of getting eaten by bears or whatever it is that happened to goldilocks, I landed on my back wheels and fell arms-flailingly backwards onto my thigh. Apparently my left upper thigh is just where I fall. I think I crawled off the track and climbed onto a chair, because the initial impact caused my leg to go various kinds of numb. The resulting bruise was mysteriously round and violently purple, probably about eight cm in diameter. More on this later.&lt;/li&gt;
&lt;li&gt;focus: this was really fun. We assembled a pack (roughly, 'be within arms distance of two people at all times') and then had to skate around while identifying colours and numbers from various teachers/refs who were possibly behind us. Apparently I skate better when I am slightly distracted. I am reminded of a cognitive test I had done some years earlier at the &lt;a href="https://dublin.sciencegallery.com/"&gt;Science Gallery&lt;/a&gt; where I had to memorise numbers said aloud while also solving a maze puzzle. They tested both tasks independently and then together, and I did better doing both at once. My explanation for this is that while free to either look or listen (i.e. no maze or no numbers) I got distracted by things in the environment (this was happening in a fairly busy room) which broke my concentration, whereas doing both at once required focus but not in a way that excluded my capacity at the other task. Another example: I find it very helpful to draw/doodle/fidget while listening to things, but unfortunately that comes across badly in meetings. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another amazing thing during the class was that I did a crossover. How? Well, I just... sort of... did it. I had been trying to first practice crossing my feet over while standing still, or skating on one foot, or whatever. But then one of the teachers was like 'just try it' and I was probably already delirious from all the blood pooling in my thigh so I went for it, and it happened, and everything was beautiful and nothing hurt. And then I fell on my knees, but I think that's because I had no end-game for the move. I assumed I would try and fall, so once I wasn't falling I didn't know where to go, and I fell. So a holy grail (there are many) of skating seemed within reach. I had done it once, and I could - in theory - do it again. So that Sunday, after The Bruise had recovered enough that I could walk mostly normally again, I hit the gym.&lt;/p&gt;
&lt;p&gt;Insert a wheel-switching montage here. Did I mention I ordered bearings for the new wheels, to make switching easier? And I forgot how to do numbers, so I got half as many as I needed? Masters degree in mathematics right here.&lt;/p&gt;
&lt;p&gt;I had been at the gym for about twenty minutes when it happened. I had skated in circles, I had skidded weirdly on stripes on the floor (it's one of those multipurpose courts covered in every sports marking), and then I went for the crossover. And I fell. straight. onto. the. bruise. I wanted to yell at everyone and no one in particular that I already had a bruise there, that I already had a massive, deep bruise, so they would not judge me for crawling back to the bench, trying not to cry. It hurt, it hurt so bad and I felt repulsed and yet obsessed by the idea of pressing on that mess of blood and broken veins. &lt;/p&gt;
&lt;p&gt;I sat on the bench and breathed deliberately, waiting for the waves of pain and dizzying shock to subside, psyching myself up to try &lt;em&gt;again&lt;/em&gt; because I was so determined to get this, and then a man appeared. He was not in sports attire. He did not have a smiling face. He communicated in limited English that rollerblading is not permitted in the hall. I asked if there was somewhere else I could go. He said no. I asked again and he said he would get someone who spoke better English. He retrieved one of the basketball players who had been in the hall beside me. The basketball player told me that skating was not allowed. The skates would damage the surface. I asked if there was somewhere else I could practice. He said no. I nodded. I thanked the men. I ripped off my knee pads and elbow pads and wrist pads. I took off my skates. I looked at the pile of gear sitting beside me. I looked at the changing room door on the far side of the hall. I tried to shove my gear into my helmet and it didn't fit, so I put my helmet on and grabbed the shoes by the laces and looped my keys around a finger as I held my bottle under my arm, knee pads cupping the shoulder pads, the skates getting heavier as I left sweaty footprints on the floor.&lt;/p&gt;</content><category term="roller derby"></category><category term="tensorflow"></category></entry><entry><title>roller derby week 2ish</title><link href="/life/2016-09-15-roller-derby-2.html" rel="alternate"></link><published>2016-09-15T00:00:00+01:00</published><updated>2016-09-15T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2016-09-15:/life/2016-09-15-roller-derby-2.html</id><summary type="html">&lt;p&gt;&lt;em&gt;See week 1 &lt;a href="/life/2016-08-29-roller-derby-1.html"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I endeavoured to revise and presumably master all the material we covered in the first class on a Sunday afternoon (nerd, remember. Historically 'Good At School'). It seemed quite straight forward. I would just repeat the difficult thing until it became easy. I've been there. I …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;See week 1 &lt;a href="/life/2016-08-29-roller-derby-1.html"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I endeavoured to revise and presumably master all the material we covered in the first class on a Sunday afternoon (nerd, remember. Historically 'Good At School'). It seemed quite straight forward. I would just repeat the difficult thing until it became easy. I've been there. I used to spend an hour a day &lt;em&gt;just&lt;/em&gt; playing scales on my violin. Repetition is the key to mastery. &lt;/p&gt;
&lt;p&gt;I failed to account for the fact that in physical activities, mistakes aren't free. They hurt. They can damage you. I'm writing this three and a half weeks after the failed T-stop at the first training, and my ankle still hurts. So when I put my gear on and stood up in the sports hall, instead of taking off with the casually intense focus of one committed to a task, I became very aware of how precarious my situation was. It seemed likely if not utterly predetermined that I would fall, badly, onto my back - maybe twisting my legs or ankles as I went down, or falling onto an arm and breaking it, implausibly - my wrists were too well protected but - presumably the forearm can break somehow - can you break elbows? This in mind, my plans of mastery withered to a single hope - to survive unscathed, and maybe be a bit less terrified in an hour.&lt;/p&gt;
&lt;p&gt;Remembering the almost-comically predictable 'falling at the corners' I'd done at training, I managed to expand my rapidly-contracting ambition for the day to include learning how to turn. Because the gym is only available for a few hours on Sundays, I first went home and watched a lot of youtube videos (some relevant) and thought about acceleration, broadly defined.&lt;/p&gt;
&lt;p&gt;Here is what I learned: you can turn by pushing the outer foot out. You lean a bit into the turn, putting weight on the inner leg, and push the outer foot roughly 'outwards' (actual trajectory is more like a curve since you're moving forwards at the same time). &lt;/p&gt;
&lt;p&gt;When I realised this - in the sense of actually &lt;em&gt;achieving&lt;/em&gt; it, rather than understanding it conceptually (which was easy), I felt so accomplished I almost forgot how wildly I had moved the goalposts on my afternoon to get there. I got excited for the prospect of trying the '27 in 5' again, because the secret to turning corners without losing &lt;em&gt;all&lt;/em&gt; my speed had been unlocked and maybe I'd get a score I could say in public.&lt;/p&gt;
&lt;p&gt;Unfortunately, I almost missed training session two, because I spent most of the day in a haze of pain and sickness, semi-conscious and clawing pitifully in the direction of Netflix. Scientist was baffled. I made a deal with myself where I would go via tram (as opposed to bike - not a fan of cycling when I'm dizzy) and take breaks as often as necessary. Probably not a good health choice, probably don't take health advice from me... or definitely &lt;em&gt;do&lt;/em&gt;, because I miraculously got better, and it was certainly due to roller derby and not the extended nap I took in the afternoon.&lt;/p&gt;
&lt;p&gt;Highlights of the class included me seeing side planks with leg lifts, thinking 'I can probably do this, I have done this before', and being &lt;em&gt;totally incapable&lt;/em&gt; of lifting my legs, because that's what happens when you add a 2kg (?) weight to the end of your leg and also stop going to yoga six months earlier. We also did some 'agility', which involved things like balancing on one skate (I was pretty good at this - I have good balance), jumping to the side (somehow easier than it sounds), and stepping to the side while crossing one foot over the other (&lt;em&gt;LITERALLY IMPOSSIBLE&lt;/em&gt;, anyone who can do this is a &lt;em&gt;WIZARD&lt;/em&gt;). We also did a pattern of zig-zagging around the track (cutting from side to side) which exceeded my limited ability to do sticky feet (skating without lifting your feet) and involved a lot of baleful middle-distance stares as I rolled unceremoniously to a stop. But I barely fell, and someone said it looked like I had been practicing, so I left the place wearing the invisible sunglasses of a person inwardly giddy with pride.&lt;/p&gt;
&lt;p&gt;I was then travelling for rougly a week while trying to finish and submit a paper, so I missed the next training - but it was actually an intro to the extensive rules of roller derby, which I have already largely ready (nerd, remember), so the exhilirating story of learning-to-skate continues uninterrupted whenever I write the next one of these. Right now I have to find a sleeping position that doesn't involve the bruised side of my body.&lt;/p&gt;</content><category term="roller derby"></category></entry><entry><title>roller derby week 1</title><link href="/life/2016-08-29-roller-derby-1.html" rel="alternate"></link><published>2016-08-29T00:00:00+01:00</published><updated>2016-08-29T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2016-08-29:/life/2016-08-29-roller-derby-1.html</id><summary type="html">&lt;p&gt;&lt;em&gt;See week 0 &lt;a href="/life/2016-08-28-roller-derby-0.html"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I am a nerd. I didn't want to go to class &lt;em&gt;unprepared&lt;/em&gt;, so a few weeks before the Rookie Course started, I went to a neighbouring canton to buy derby gear so I could practice. Practice and hope to attain a minimal competency such that I …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;See week 0 &lt;a href="/life/2016-08-28-roller-derby-0.html"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I am a nerd. I didn't want to go to class &lt;em&gt;unprepared&lt;/em&gt;, so a few weeks before the Rookie Course started, I went to a neighbouring canton to buy derby gear so I could practice. Practice and hope to attain a minimal competency such that I would not be so horribly bruised again, so quickly. Roller derby gear purchased in Switzerland is pretty expensive. My knee pads &lt;em&gt;alone&lt;/em&gt; cost 100 francs. But I have a job and a single yoga lesson here costs at least 20 CHF so I can deal with it. And those pads make falling forwards painless, which I rate quite highly. One must remember to fall &lt;em&gt;forward&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;An expedition was undertaken to find a location suitable for practice. Such a location must be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;flat, ideally completely flat, oh god a single stone will kill me dead&lt;/li&gt;
&lt;li&gt;safely enclosed from roads, traffic, hills, chasms&lt;/li&gt;
&lt;li&gt;devoid of other people, with their beady, judging eyes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And another thing I did not even think to think about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;of a surface appropriate for the wheels on my skates&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It turns out that the wheels which came with the 'starter' skates I got (Riedell R3s) are of a hardness appropriate for concrete and other hard-ish surfaces. (They're Sonar Flat Out wheels, with a hardness of 88A, for reference.) The indoor multi-purpose court we found at our university's sports centre fulfills the first three conditions, to an extent. There are people on exercise machines overlooking the court, but I can deal with judgement better than I can deal with skating into traffic and dying. It fails badly on the last condition. It has some kind of rubberized surface (you can leave small impressions in it with your nails) and when I first put on my skates and tried it out, something felt... wrong. I could stand upright with no effort to stay in one place. I could roll to a graceful stop by ceasing movement. When I tried to move forwards, my feet lagged strangely, and made me stumble. I could jump without filling with abject terror. Something was &lt;em&gt;wrong&lt;/em&gt;. I went home and did some research ('why roller skate sticky') and deduced that my wheels were responsible. For a surface that soft you need harder wheels to compensate, and my 88As wouldn't cut it. So I went off and bought some 95A hardness wheels and they seemed &lt;em&gt;better&lt;/em&gt;, not &lt;em&gt;perfect&lt;/em&gt; but good enough because I'm not made of money or willingness to spend a morning going to Aarau, as pretty as their &lt;a href="https://upload.wikimedia.org/wikipedia/commons/thumb/6/62/Flag_of_Canton_of_Aargau.svg/220px-Flag_of_Canton_of_Aargau.svg.png"&gt;cantonal flag&lt;/a&gt; may be.&lt;/p&gt;
&lt;p&gt;Minor logistical issues aside, I managed a tiny bit of practice before the first training session. It's hard to know how much worse I would have been without it. I was bad, real bad. I fell (backwards or otherwise messily) five times. While trying to do a T-stop I messed up somehow and twisted my ankle in a way that seems like something should have broken. My issue was mostly corners. I was trying to stay on the inside of the track to avoid being in the way of the others, who are much better than me (they are mostly more experienced, so this is natural). I can deal with hurting myself but I don't want to cause someone else to fall. The inside of the track has a tigher corner however, so I kept getting off balance. Obvious solution is to stop hugging the inside of the track. Or learn how to turn (coming soon!).&lt;/p&gt;
&lt;p&gt;At the end of the session we did a practice '27 in 5' (part of the minimal skills test is to skate 27 laps in 5 minutes) and I did &lt;em&gt;twelve&lt;/em&gt; laps in five minutes. That's awful. So bad. Much slow ... &lt;em&gt;but&lt;/em&gt; I was intentionally avoiding gaining speed on the straights because I had no idea how to deal with it on the corners, and I &lt;em&gt;definitely&lt;/em&gt; didn't want to make someone else fall while they were trying to go fast. Especially not if it involves them falling on me.&lt;/p&gt;
&lt;p&gt;We also covered plow stops, which I find a &lt;em&gt;lot&lt;/em&gt; easier and less ankle-destroying than T-stops, probably because I can keep both feet on the ground. Overall though, my favourite stopping method is 'falling and curling up in a ball'. Fall small, they say. Be grand, they say.&lt;/p&gt;
&lt;p&gt;I was alarmed at first by the horrible disparity in skill level at the Rookie course. Some people were doing &lt;em&gt;crossovers&lt;/em&gt; in week 1. I mistakenly tried to keep up and got several days of hobbling around the office as my reward. I resolved to practice (once my leg regained full function) and &lt;em&gt;definitely catch up&lt;/em&gt;, because that is &lt;em&gt;definitely possible&lt;/em&gt; in a week (spoiler: no).&lt;/p&gt;
&lt;p&gt;And now for some thoughts on skill-acquisition. At this point, being on skates feels terrifying. The ground moves perilously beneath me at every moment. Even staying still requires intense and tiresome muscle activity. My whole body is inexplicably involved in the task of not falling to my doom. I imagine however that this is how cycling felt, once. On cycling: I am a good cyclist. I am not an &lt;em&gt;advanced&lt;/em&gt; cyclist: there are many things I can't do. I can't swing my leg over my bike and jump off while I'm slowing down, and I can't cycle stably with no hands. I can't turn really sharply. I don't try to do these things. I am good at the things I choose to do (causal direction left as exercise for the reader). I feel utterly comfortable on my bike - it is an extension of me, and I can go where I desire without consciously acting. It would be hard for me to explain exactly how I cycle, because so much of it is automatic. My hope and belief is that this is how experienced people feel on rollerskates. Until then, I can't &lt;em&gt;play&lt;/em&gt; while on skates, because I am far too occupied with questions of basic mechanics. I want to get to the stage where I can think more about what I'm doing and less about &lt;em&gt;how&lt;/em&gt; I'm doing it. &lt;/p&gt;</content><category term="roller derby"></category><category term="cycling"></category></entry><entry><title>roller derby week 0</title><link href="/life/2016-08-28-roller-derby-0.html" rel="alternate"></link><published>2016-08-28T00:00:00+01:00</published><updated>2016-08-28T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2016-08-28:/life/2016-08-28-roller-derby-0.html</id><summary type="html">&lt;p&gt;In a turn of events baffling to those who know me, I decided to sign up to (try to) learn to play roller derby. Roller derby is a full-contact sport played on rollerskates. The game basically consists of getting in someone's way, or getting through people who are getting in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In a turn of events baffling to those who know me, I decided to sign up to (try to) learn to play roller derby. Roller derby is a full-contact sport played on rollerskates. The game basically consists of getting in someone's way, or getting through people who are getting in your way. It's like 'walking through Times Square' but on rollerskates and the pushing is consensual.&lt;/p&gt;
&lt;p&gt;My interest in the sport is incredibly out of character. I'm not a fan of falling over. I don't like getting hurt. I don't really like doing things which might hurt others, and while I &lt;em&gt;do&lt;/em&gt; enjoy the sensation of moving quickly, it also terrifies me. I appear to lack any thrill-seeking bones and I've always been fine with that. Why would I do something &lt;em&gt;dangerous&lt;/em&gt;? Also, and this is not unrelated, I'm &lt;em&gt;sort&lt;/em&gt; of small.  I'm 161cm (5'3") tall and weigh about 55kg (120lb). In a game of momentum transfer, I am going to lose. I fare a lot better at academic pursuits, like writing code while slowly horizontalising myself, or drawing amorphous manifolds on whiteboards. Also computer games. I got an accidental headshot in Overwatch last night, so you could say I'm pretty good.&lt;/p&gt;
&lt;p&gt;However, I also moved to a new country (Switzerland) recently, and having one's non-work life unceremoniously deleted is a good opportunity to find new hobbies. (Or rediscover old ones, like competitive online video games) I had been aware of roller derby for some time because a former flatmate was involved with the Dublin Roller Girls back when we lived together in 2010. At the time I wasn't interested in the idea of sports, much less &lt;em&gt;violent&lt;/em&gt; ones, so I never seriously considered the idea of doing it. Also, I'm so small! I was in the 'too light to donate blood' category back then. However, now that I am older and wiser and excluded from blood-donation for different reasons, I have decided to get out of my comfort zone. Wildly. So far that traditional notions of distance can no longer be meaningfully used to describe the relative locations of roller derby and my comfort zone.&lt;/p&gt;
&lt;p&gt;So, while scouting around for Things To Take Up in Zurich I found the &lt;a href="http://www.rollerderby.ch/"&gt;Zürich City RollerGirlz&lt;/a&gt;, noticed they had a try-out day and convinced my good friend and coworker Natalie to go with me. That was my first time ever on rollerskates. I fell twice on the same spot and had a bruise which lasted, visibly, for three weeks.  But it was fun and I learned that the game is more nuanced and less openly brutal than 'shove people, also you're on rollerskates'. There are rules. I knew in a sense that there would be, but I also feared that 'full contact' means anything goes. Thankfully not. &lt;/p&gt;
&lt;p&gt;Natalie and I signed up to the rookie course, and I intend to document my progress. Natalie has a video-camera too, so if we remember we might compile clips to make a sweet training montage video at the end. It lasts 12 weeks and culminates in the Minimal Skills Test required to be eligible to actually play roller derby. It seems implausible to me that I can go from &lt;em&gt;never having skated&lt;/em&gt; to passing that test in a 12 week period (while also doing a PhD and becoming a professional Overwatch player), but time shall tell. I'm quite motivated to try, because everyone loves underdog stories, and I really like having things to care about that &lt;em&gt;aren't&lt;/em&gt; related to my PhD. File this under coping mechanisms.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The course actually started two weeks ago, I just didn't think to blog about it until now. I was searching for things like 'how to roller skate' and came across some really useful and reassuring blog posts from other beginners (or ex-beginners), and realised how much I value reading about other people sucking at things. So here I am, sucking at a thing.&lt;/em&gt;&lt;/p&gt;</content><category term="roller derby"></category><category term="moving"></category><category term="overwatch"></category></entry><entry><title>transcribing my accent</title><link href="/lang/2016-08-21-transcribing-my-accent.html" rel="alternate"></link><published>2016-08-21T00:00:00+01:00</published><updated>2016-08-21T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2016-08-21:/lang/2016-08-21-transcribing-my-accent.html</id><summary type="html">&lt;p&gt;An exercise in the international phonetic alphabet (IPA).&lt;/p&gt;
&lt;p&gt;Here's a quote from the magnificent Margaret Atwood book, "The Handmaid's Tale":&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Now we walk along the same street, in red pairs, and no man shouts obscenities at us, speaks to us, touches us. No one whistles.&lt;/p&gt;
&lt;p&gt;There is more than one …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;An exercise in the international phonetic alphabet (IPA).&lt;/p&gt;
&lt;p&gt;Here's a quote from the magnificent Margaret Atwood book, "The Handmaid's Tale":&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Now we walk along the same street, in red pairs, and no man shouts obscenities at us, speaks to us, touches us. No one whistles.&lt;/p&gt;
&lt;p&gt;There is more than one kind of freedom, said Aunt Lydia. Freedom to and freedom from. In the days of anarchy, it was freedom to. Now you are being given freedom from. Don't underrate it."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I recorded myself reading it at a normal pace, not trying to enunciate correctly, trying &lt;em&gt;not&lt;/em&gt; to think about my accent, &lt;a href="/audio/handmaid.mp3"&gt;here&lt;/a&gt; is the recording.&lt;/p&gt;
&lt;p&gt;And here is my attempt to transcribe the recording into IPA:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;[nɐu wi wɒk əlɑŋ ðə seɪm st͡ʃɹitʰ, ɪn ɹɜd pɛɹz, ənd nɵʊ mæn ʃɐət͡s əbsɛnɪtiz ætʰ əs, spiks tu əs, tət͡ʃɘz əs.  nɵʊ wʌn wɪsœlz.]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[dʰɛɹ ɪz mɒɹ ðɜn wʌn kɜnd əv fɹidəm, sɛd æntʰ lɪdiæ. fɹidəm tu ɶnd fɹidəm fɹɐm. ɪn ðə dez əv anœɹki, ɪtʰ wəs fɹidəm tu. nɐu jəɹ bin ɡɪvɪn fɹidəm fɹɐm. dɵʌnt əndəɹetʰ ɪtʰ]&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My accent is... a bit weird. I spent my first twenty-three years in south Dublin, so my accent &lt;em&gt;should&lt;/em&gt; be unquestionably 'Irish', but people often think I sound American. I don't know why. ¯\_(ツ)_/¯&lt;/p&gt;
&lt;p&gt;Things I noticed during transcription:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I don't break between words, apparently (does anyone?). While transcribing 'freedom' I had to check that it wasn't actually 'freedoms', because there's no gap in the audio between the [m] and the [s].&lt;/li&gt;
&lt;li&gt;vowels are hard and mostly schwas.&lt;/li&gt;
&lt;li&gt;I simply cannot make out the difference between some letters, like [æ] and [a]. My ear just isn't that good (yet). I think I need to spend more time learning how the sounds are &lt;em&gt;created&lt;/em&gt;, because I found that immensely helpful in distinguishing between, for example, [ʃ] and [ʂ] while studying the consonants.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you &lt;a href="/pages/about.html"&gt;send me&lt;/a&gt; a recording of yourself reading the quote I will try to transcribe it to IPA for comparison purposes. Comparison and judgement.&lt;/p&gt;
&lt;p&gt;I imagine transcribing someone else speaking is much more difficult because one can't rely on slow careful repetition with internal observation of the shape of the mouth. That might be a good thing.&lt;/p&gt;</content><category term="IPA"></category><category term="books"></category><category term="atwood"></category><category term="accent"></category></entry><entry><title>important site updates</title><link href="/meta/2016-07-06-important-site-update.html" rel="alternate"></link><published>2016-07-06T00:00:00+01:00</published><updated>2016-07-06T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2016-07-06:/meta/2016-07-06-important-site-update.html</id><summary type="html">&lt;p&gt;Two matters, one vastly more important than the other:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SSL is active! It was incredibly easy to set up with &lt;a href="https://letsencrypt.org/"&gt;Let's Encrypt&lt;/a&gt;. Losing SSL was one sad thing about moving away from GitHub pages, but it is clearly remedied. Now you can access my content securely.&lt;/li&gt;
&lt;li&gt;This site has a …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;Two matters, one vastly more important than the other:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SSL is active! It was incredibly easy to set up with &lt;a href="https://letsencrypt.org/"&gt;Let's Encrypt&lt;/a&gt;. Losing SSL was one sad thing about moving away from GitHub pages, but it is clearly remedied. Now you can access my content securely.&lt;/li&gt;
&lt;li&gt;This site has a new subdomain: &lt;a href="http://dog.apeiroto.pe/"&gt;http://dog.apeiroto.pe/&lt;/a&gt;. It shows a new dog gif/image on reload. SSL is... not working on the subdomain, I think I need to poke at nginx for that. You can now emulate friendship with me by hitting refresh on that page!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The way I did the new image on reload thing is hacky so here goes: I considered trying to actually learn JavaScript and then remembered I had to go wash my horse. So I found a 'load random image on reload' script, which puls images from a list defined in the script. Given I want to just drop files in a folder and have them enter the pupper-rotation this was no good, so I wrote a script (in bash) to compile the contents of the dog-folder and stick that in the HTML. Easy. What I need to do now is combine the 'count puppers and update HTML' script with the 'sync puppers to web server' script and I'll be sorted. Or put both on cronjobs and forget about it.&lt;/p&gt;
&lt;p&gt;At the time of writing there are 210 such images. That's 2.1G of doggos. I regularly collect these pictures from twitter/imgur/giphy so it should grow slowly.&lt;/p&gt;
&lt;p&gt;Coming soon: cat.apeiroto.pe.&lt;/p&gt;</content><category term="dogs"></category><category term="gifs"></category><category term="ssl"></category><category term="encryption"></category></entry><entry><title>ICML 2016 not by the day</title><link href="/ml/2016-07-05-icml2016.html" rel="alternate"></link><published>2016-07-05T00:00:00+01:00</published><updated>2016-07-05T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2016-07-05:/ml/2016-07-05-icml2016.html</id><summary type="html">&lt;p&gt;The &lt;a href="icml.cc"&gt;International Conference on Machine Learning (ICML)&lt;/a&gt; was in NYC this year! Unfortunately(?) for me, I moved from NYC to Zürich two months ago. Fortunately for me, I was able to return to attend the conference. Instead of doing a day-by-day breakdown (as I did for &lt;a href="/ml/2015-12-14-nips2015.html"&gt;NIPS&lt;/a&gt; and &lt;a href="/ml/2016-02-17-aaai2016.html"&gt;AAAI&lt;/a&gt;), this …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="icml.cc"&gt;International Conference on Machine Learning (ICML)&lt;/a&gt; was in NYC this year! Unfortunately(?) for me, I moved from NYC to Zürich two months ago. Fortunately for me, I was able to return to attend the conference. Instead of doing a day-by-day breakdown (as I did for &lt;a href="/ml/2015-12-14-nips2015.html"&gt;NIPS&lt;/a&gt; and &lt;a href="/ml/2016-02-17-aaai2016.html"&gt;AAAI&lt;/a&gt;), this post will be arranged thematically. Let's see how I deal with the hard group assignment problem... Skip to the bit you care about.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Caveats&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I missed some non-trivial fraction of ICML due to finishing my poster, helping collaborators with a grant application, and coming down with illness&lt;ul&gt;
&lt;li&gt;Future conference goal: finish my poster &lt;em&gt;before&lt;/em&gt; I travel. &lt;/li&gt;
&lt;li&gt;Also don't try to print A0 posters in the USA. It ain't pretty.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I took very patchy notes, haven't read all the papers deeply.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Volunteering at ICML&lt;/h2&gt;
&lt;p&gt;I was a student volunteer for ICML, which consisted of working two ~five-hour shifts at the conference. For me these were both Registration Desk. I had 07.30-12.30 on the first and last days, which was possible purely by my being in European time for much of the trip. I woke up at 4am on the first day. Here are some observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;people actually register on the last day,  but more people just want to get their badge reprinted&lt;ul&gt;
&lt;li&gt;protip: don't forget your name badge!&lt;/li&gt;
&lt;li&gt;you paid hundreds of dollars to get that piece of paper&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;some people turn up really early to register&lt;/li&gt;
&lt;li&gt;90% of ICML attendees were DeepMind employees&lt;/li&gt;
&lt;li&gt;registration desk workers could easily be replaced by name-badge-printing kiosks&lt;/li&gt;
&lt;li&gt;conference attendees expect a pile of swag upon registration: pens and bags and mugs and programs booklets. Not receiving these items is cause for thinly-veiled indignation&lt;/li&gt;
&lt;li&gt;queues for registration are worst in the gap between sessions, naturally&lt;/li&gt;
&lt;li&gt;people manage to make it to the top of a line without attempting to find the documents they need&lt;ul&gt;
&lt;li&gt;I have also observed this phenomenon in airports and banks&lt;/li&gt;
&lt;li&gt;why&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I registered a bunch of people whose papers I have read, and I maintained composure&lt;/li&gt;
&lt;li&gt;if I were running the registration desk with excessive time to spare, we would have had a graph of cumulative registrations over time, maybe with a breakdown for geographic origin/broad affiliation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall it was surprisingly fun. Apparently I rather enjoy that kind of work, so if this whole research thing doesn't work out I have a bright future as a vending machine.&lt;/p&gt;
&lt;h2&gt;Tutorial on Deep Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;I was only able to attend one tutorial due to volunteering, and it was Deep RL. It was so popular there were &lt;em&gt;two&lt;/em&gt; overflow rooms. Intense community interest in deep RL continues. Here's an abbreviated version:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://twitter.com/__hylandSL/status/744602517275803648"&gt;&lt;img src="/images/icml2016_deeprl.png" style="width: 40vw;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The deep part comes into play when you use a deep neural network to approximate your value function, policy, environment etc.&lt;/p&gt;
&lt;h2&gt;Interesting Papers/Talks&lt;/h2&gt;
&lt;p&gt;These are the papers I flagged in the conference app. Did I attend all of these talks? No. Did I attend all of the posters? Also no. In hopefully-meaningful categories:&lt;/p&gt;
&lt;h3&gt;Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/lie16.pdf"&gt;Learning to Generate with Memory&lt;/a&gt;: &lt;em&gt;Chongxuan Li, Jun Zhu, Bo Zhang&lt;/em&gt;: a deep generative model with external memory and attention mechanism. The deepness comes in through some nonlinear functions on latent variables which are defined by (deterministic) deep neural networks. Each layer in the network has access to its own external memory, which is seemingly novel in this model. In each layer lower-layer information is combined with the memory to produce the output, using some attention function taking as input the information from the lower layer. I'm not entirely convinced by the experiments that the memory mechanism actually helps that much, although they say it gives better 'qualitative' results.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/arjovsky16.pdf"&gt;Unitary Evolution Recurrent Neural Networks&lt;/a&gt;: &lt;em&gt;Martin Arjovsky, Amar Shah, Yoshua Bengio&lt;/em&gt;: The idea here is to use a unitary matrix as the evolution operator in an RNN, with a hope to avoid exploding gradients. It seems to result in an RNN which can retain information for longer than a LSTM, and while gradients do vanish slowly, they do so more slowly than other models, and don't explode. I'm working on something of an extension to this work right now, and I had the pleasure of speaking with the authors at length. More details in forthcoming paper, I guess? Or blog post, we'll see.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/balduzzi16.pdf"&gt;Strongly-Typed Recurrent Neural Networks&lt;/a&gt;: &lt;em&gt;David Balduzzi, Muhammad Ghifary&lt;/em&gt;: I really like the spirit of this work. Let's try to understand RNNs! And take inspiration from functional programming and physics, because why not? The physics part is roughly to preserve 'dimensions' (think units) by preserving the &lt;em&gt;basis&lt;/em&gt; of the space. I took issue with this because I think any map from a space to itself is already preserving something (preserving being in the space, that is), but what that means for the model is less clear. The part from functional programming is about separating state and computation, a separation into &lt;em&gt;learnware&lt;/em&gt; (with parameters) and &lt;em&gt;firmware&lt;/em&gt; (having no parameters, but having state).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/cohenc16.pdf"&gt;Group Equivariant Convolutional Networks&lt;/a&gt;: &lt;em&gt;Taco Cohen, Max Welling&lt;/em&gt;: Wild simplification/mild understatement: they extend convolutional layers to other kinds of symmetries, not just translational.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/taylor16.pdf"&gt;Training Neural Networks Without Gradients: A Scalable ADMM Approach&lt;/a&gt;: &lt;em&gt;Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, Tom Goldstein&lt;/em&gt;:  ADMM stands for Alternating Direction Method of Multipliers. They use this with Bregman iteration to train networks &lt;em&gt;without SGD&lt;/em&gt;! This method scales linearly over cores, and they compare this to an asynchronous SGD model called Downpour, which scales very strangely. SGD, having &lt;em&gt;many&lt;/em&gt; small computations is good for GPUs, whereas CPUs are better for a smaller number of &lt;em&gt;expensive&lt;/em&gt; calculations, preferably involving a lot of data. This approach also combats the vanishing gradient problem (unsurprising given there are no gradients to vanish: gradients come pre-vanished), and SGD's tendency towards lingering near saddle-points.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Reinforcement Learning / Bandits&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/he16.pdf"&gt;Opponent Modeling in Deep Reinforcement Learning&lt;/a&gt;: &lt;em&gt;He He, Jordan Boyd-Graber, Kevin Kwok, Hal Daume III&lt;/em&gt;: They develop a model called DRON: Deep Reinforcement Opponent Network, which is close enough to TRON to make me happy. It's based on Mnih's deep Q-networks. DRON has both policy-learning module and opponent-learning module. It's essentially two networks, and they look at ways of combining them: concatenation and using mixtures-of-experts.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/simsek16.pdf"&gt;Why Most Decisions Are Easy in Tetris—And Perhaps in Other Sequential Decision Problems, As Well&lt;/a&gt;: &lt;em&gt;Ozgur Simsek, Simon Algorta, Amit Kothiyal&lt;/em&gt;: by 'easy' they mean: "one can choose well among the available actions without knowing an evaluation function that scores well in the game". The idea is that comparison becomes easy when some criteria are met, and the relationship between features and criterion (of the comparison) is linear. This linearity requirement seems restrictive, but holds true for the best known tetris player (BCTS).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/le16.pdf"&gt;Smooth Imitation Learning for Online Sequence Prediction&lt;/a&gt;: &lt;em&gt;Hoang Le, Andrew Kang, Yisong Yue, Peter Carr&lt;/em&gt;: They're looking at imitation learning where actions and the environment are continuous, but the environment is exogenous (not affected by actions). They consider the state space to be both environment and actions (so the policy considers the previous action taken), and enforce smoothness of &lt;em&gt;actions&lt;/em&gt;. The application is smooth camera control (the paper is from Disney research), hence smooth actions. Their approach learns a fully deterministic stationary policy, and they have some other contributions whose gravity are somewhat lost on me, but are presumably important.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/mniha16.pdf"&gt;Asynchronous Methods for Deep Reinforcement Learning&lt;/a&gt;: &lt;em&gt;Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu&lt;/em&gt;: As an alternative to experience replay, they asychronously run multiple agents in different instances of the environment, in parallel. This can then be run on a multi-core CPU rather than a GPU, and is more resource efficient. Some nice ggplots, too.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/wu16.pdf"&gt;Conservative Bandits&lt;/a&gt;: &lt;em&gt;Yifan Wu, Roshan Shariff, Tor Lattimore, Csaba Szepesvári&lt;/em&gt;: a multi-armed bandit problem where a company wants to maximise revenue while keeping revenue above a constant baseline. In this setting there exists a 'conservative default action', and they propose an extension to UCB (upper confidence bound) where a budget is accumulated using the conservative arm, and when large enough allows for 'safe' exploration.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Representation Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/steeg16.pdf"&gt;The Information Sieve&lt;/a&gt;: &lt;em&gt;Greg Ver Steeg, Aram Galstyan&lt;/em&gt;: What an intriguing title. This is about representation-learning. The idea seems to be to iteratively 'sieve' the data, extracting a latent feature at a time, then passing on a version of the data with the contribution from that feature somehow removed, and so on. Sieving. It relies on the total correlation, or multivariate mutual information, and they describe a way for finding the factors which cause this total correlation to decompose into non-negative contributions.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/trouillon16.pdf"&gt;Complex Embeddings for Simple Link Prediction&lt;/a&gt;: &lt;em&gt;Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, Guillaume Bouchard&lt;/em&gt;: a scoring function for link prediction (subject, predicate, object type triples) which uses &lt;em&gt;complex&lt;/em&gt;-valued embeddings for entities. Using the inner product in complex space amounts to taking dot products with complex conjugates, which handles asymmetry of the triples. The relationships appear to be parametrised with complex-valued vectors. At a glance it looks like a complex version of DistMult.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other / ???&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/yoon16.pdf"&gt;ForecastICU: A Prognostic Decision Support System for Timely Prediction of Intensive Care Unit Admission&lt;/a&gt;: &lt;em&gt;Jinsung Yoon, Ahmed Alaa, Scott Hu, Mihaela van der Schaar&lt;/em&gt;: the application here is predicting when/if a patient needs to be admitted to the ICU. They cast it as an optimal stopping problem, and try to learn the unknown stopping rule of the stochastic process: how the physician decides (on the basis of the stream of data) to admit the patient to ICU. They assume patients belong to 'stable' or 'deteriorating' classes, which describe different distributions over physiological streams.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/gal16.pdf`"&gt;Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning&lt;/a&gt;: &lt;em&gt;Yarin Gal, Zoubin Ghahramani&lt;/em&gt;: I'm not going to give this paper justice by skim-summarising it, so I'll just quote a sentence: _"In this paper we give a complete theoretical treatment of the link between Gaussian processes and dropout, and develop the tools necessary to represent uncertainty in deep learning". Cool cool cool.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/gilad-bachrach16.pdf"&gt;CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy&lt;/a&gt;: &lt;em&gt;Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, John Wernsing&lt;/em&gt;: Homomorphic encryption! Homomorphic encryption only allows for addition and multiplication, and ideally with low-degree polynomials, so they have to approximate the usual max pool, sigmoid etc. transformations. One also has to be careful as all operations in the cryptosystem are applied modulo some number. A key thing to note here is that they're not &lt;em&gt;training&lt;/em&gt; on encrypted data, just predicting.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v48/bauer16.pdf"&gt;The Arrow of Time in Multivariate Time Series&lt;/a&gt;: &lt;em&gt;Stefan Bauer, Bernhard Schölkopf, Jonas Peters&lt;/em&gt;: Non-Gaussian noise breaks time symmetry in multivariate autoregressive moving average (VARMA) models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a href="https://sites.google.com/site/gimliworkshop/"&gt;Geometry in Machine Learning Workshop&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Is the title of this workshop an &lt;em&gt;intentional&lt;/em&gt; Lord of the Rings reference? I sure hope so.&lt;/p&gt;
&lt;p&gt;I spent the whole day at this workshop, since I was presenting a poster and also &lt;em&gt;yay differential geometry&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So why care about geometry for machine learning? Firstly, by geometry we're talking about &lt;em&gt;differential&lt;/em&gt; geometry, which is focused on differentiable manifolds (manifolds which &lt;em&gt;locally&lt;/em&gt; look flat). Data &lt;em&gt;usually&lt;/em&gt; lies on a manifold. We often assume this manifold is Euclidean space (nice and flat), but it often isn't. A simple example is data which lies on a &lt;em&gt;circle&lt;/em&gt;, which if you've encountered if you've ever dealt with angular measurements. &lt;a href="http://me.jhu.edu/faculty/gregory-s-chirikjian/"&gt;Gregory S. Chirikjian&lt;/a&gt; gave a really nice illustrating example in his talk "Learning and Lie Groups": if you consider the range of motions available to a simple noisy robot, after a certain number of steps its possible location will be given by some probability distribution (this is called the 'banana distribution'). This distribution is &lt;em&gt;not&lt;/em&gt; Gaussian in &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; (the coordinates of the Euclidean manifold a.k.a. the plane the robot was moving on), but if you recall that its motions were constrained to come from a Lie group (specifically the planar special Euclidean group, SE(2), consisting of translations and rotations in the plane), you can define a Gaussian distribution relative to coordinates in &lt;em&gt;that&lt;/em&gt; group space (since Lie groups are manifolds), and &lt;em&gt;this&lt;/em&gt; distribution describes its location. For more details, see the paper: &lt;a href="http://www.roboticsproceedings.org/rss08/p34.pdf"&gt;The Banana Distribution is Gaussian: A Localization Study in Exponential Coordinates&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Reasons to be careful when your data lies on a manifold seem to be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;doing statistics requires a notion of distance, so you must use the distance &lt;em&gt;on the manifold&lt;/em&gt;  &lt;/li&gt;
&lt;li&gt;gradient-based optimisation requires, well, gradients, so you must use the gradient &lt;em&gt;on the manifold&lt;/em&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This second point is actually highly relevant to the work I was presenting at the workshop, which will become entirely clear once I put the paper on the arXiv.&lt;/p&gt;
&lt;p&gt;I think machine learning as a field already cares about manifolds a lot, particularly when it comes to finding low-dimensional subspaces within a dataset. This workshop was however primarily concerned with cases where the (sub-)manifold is already known.&lt;/p&gt;
&lt;p&gt;And now, the content: (also, you can get the slides for these talks on the &lt;a href="https://sites.google.com/site/gimliworkshop/schedule"&gt;workshop page&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;a href="https://web.math.princeton.edu/~nboumal/"&gt;Nicolas Boumal&lt;/a&gt; spoke about &lt;strong&gt;Optimisation on Manifolds&lt;/strong&gt;. &lt;a href="https://web.math.princeton.edu/~nboumal/papers/boumal_optimization_and_estimation_on_manifolds_phd_thesis.pdf"&gt;Here&lt;/a&gt; is his PhD thesis on the topic. The take-homes were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we have some convergence guarantees for non-convex optimisation on manifolds, see the paper: &lt;a href="https://arxiv.org/abs/1605.08101"&gt;Global rates of convergence for nonconvex optimisation on manifolds&lt;/a&gt;, &lt;em&gt;Boumal, Absil and Cartis&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;he has developed a Matlab toolbox for optimisation on manifolds: &lt;a href="http://www.manopt.org"&gt;Manopt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;free book, &lt;a href="http://press.princeton.edu/chapters/absil/"&gt;Optimization Algorithms on Matrix Manifolds&lt;/a&gt;,  &lt;em&gt;Absil, Mahony, Sepulchre&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://web.eecs.umich.edu/~girasole/"&gt;Laura Balzano&lt;/a&gt; spoke about &lt;strong&gt;Subspace Learning by Incremental Gradient Descent on the Grassmannian&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the Grassmannian is a manifold comprised of all low-dimensional subspaces of a particular ambient space, I believe with a pre-specified dimension (so it could be the space of all lines, for example)&lt;/li&gt;
&lt;li&gt;her focus area is streaming data, where you want to use first-order methods (not enough data to estimate hessians, for example)&lt;/li&gt;
&lt;li&gt;doing SVD where the learned matrices are elements of the Grassmannian (that is, living in a lower-dimensional space), so gradients are on the Grassmannian&lt;/li&gt;
&lt;li&gt;more details probably in this paper: &lt;a href="http://arxiv.org/abs/1506.07405"&gt;Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation&lt;/a&gt;, &lt;em&gt;Zhang &amp;amp; Balzano&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;also featuring a live demonstration of separating foreground from background in video! using a laptop and a webcam! More here: &lt;a href="http://arxiv.org/abs/1309.6964"&gt;Online Algorithms for Factorization-Based Structure from Motion&lt;/a&gt; - &lt;em&gt;Kennedy, Balzano, Wright, Taylor&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://me.jhu.edu/faculty/gregory-s-chirikjian/"&gt;Gregory S. Chirikjian&lt;/a&gt; spoke about &lt;strong&gt;Learning and Lie Groups&lt;/strong&gt; as I mentioned above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper again; &lt;a href="http://www.roboticsproceedings.org/rss08/p34.pdf"&gt;The Banana Distribution is Gaussian: A Localization Study in Exponential Coordinates&lt;/a&gt;, &lt;em&gt;Long, Wolfe, Mashner, Chirikjian&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;also a book (although not free ;_;): &lt;a href="http://www.springer.com/us/book/9780817648022"&gt;Stochastic Models, Information Theory, and Lie Groups&lt;/a&gt;, &lt;em&gt;Chirikjian&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.sci.utah.edu/~fletcher/"&gt;Tom Fletcher&lt;/a&gt; spoke about &lt;strong&gt;Probabilistic Geodesic Models&lt;/strong&gt;. The motivation is shape analysis (with a medical application in brains), particularly for dimensionality reduction and regression.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;he gave a nice introduction to the idea of shape: basically, geometry of object invariant of position, orientation, size: when you remove these things you are on the &lt;em&gt;SHAPE MANIFOLD&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Kendall's Shape Space: defined in a complex space. The idea here is that multiplication by a complex value is a rotation and scaling in complex space, so if you 'quotient' that out, you get Kendall's Shape Space, a complex projective space. (and amusingly for me, Projective Geometry is a class I used to sneak into)&lt;/li&gt;
&lt;li&gt;back to the idea that statistics requires a notion of distance, he defined for us the &lt;a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_mean"&gt;fréchet mean&lt;/a&gt;, allowing points to be 'averaged' on a manifold, and allowing you to define something that looks like a Gaussian-on-a-manifold...&lt;/li&gt;
&lt;li&gt;but a different one than that proposed by Chirikjian, because: there are many ways to arrive at a Gaussian distribution (as a solutions to heat and diffusion equations, as maximum-entropy distributions, the central limit theorem, maximum likelihood solutions to least squares, etc.) and while these seemingly converge on the much-loved Normal distribution in Euclidean space, this doesn't happen on other manifolds... so we end up having 'normal distributions' that look different depending on which definition we started with... oh dear.&lt;/li&gt;
&lt;li&gt;I think it was at this point that someone voiced the concern that in an arbitrary manifold, the distance metric is &lt;em&gt;locally defined&lt;/em&gt; (because it is defined on the tangent space at a point), so the normalisation constant in your Gaussian-on-a-manifold actually depends on the centre of the distribution. The solution to this is to only look at &lt;em&gt;homogeneous manifolds&lt;/em&gt;, manifolds whose isometry group acts transitively, so the manifold 'looks the same' everywhere.&lt;/li&gt;
&lt;li&gt;some homgeneous manifolds: spaces with constant curvature, Lie groups, Stiefel manifolds, Grassmiannians, dot dot dot&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.oasis-brains.org/"&gt;Open Access Series of Imaging Studies (OASIS)&lt;/a&gt;: open access brain (MRI) images!&lt;/li&gt;
&lt;li&gt;then it got into geodesic regression and the manifold of diffeomorphisms, with a shout-out to the &lt;a href="https://en.wikipedia.org/wiki/Sobolev_space"&gt;Sobolev metric&lt;/a&gt;, and a mention of Gaussian processes, thus ensuring my interest was piqued&lt;/li&gt;
&lt;li&gt;generalisation of probabilistic PCA on a Riemannian manifold: &lt;a href="https://www.sci.utah.edu/publications/zhang13/Zhang_NIPS2013.pdf"&gt;Probabilistic Principle Geodesic Analysis&lt;/a&gt;, &lt;em&gt;Zhang and Fletcher&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;another relevant paper: &lt;a href="http://link.springer.com/article/10.1007/s11263-012-0591-y"&gt;Geodesic Regression and the Theory of Least Squares on Riemannian Manifolds&lt;/a&gt;, &lt;em&gt;Fletcher&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://comet.lehman.cuny.edu/stjohn/"&gt;Katherine St. John&lt;/a&gt; spoke about &lt;strong&gt;Dimensionality Reduction on Treespaces&lt;/strong&gt;, specifically evolutionary trees. Hey, biology! Phylogenetics! The core issue is: you see a set of organisms (their genomes, rather) and want to find the optimal evolutionary tree, out of a very very large set of trees. What to do? Metrics on trees usually look at things like rearrangements ("remember balancing red-black trees?"), distances which are NP-hard to compute. I apparently didn't take many notes during this talk, so have some likely-relevant references:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://comet.lehman.cuny.edu/stjohn/research/treespaceReview.pdf"&gt;The Shape of Phylogenetic Trees (Review Paper)&lt;/a&gt;, &lt;em&gt;St John&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://comet.lehman.cuny.edu/stjohn/research/hillClimbing.pdf"&gt;Characterizing Local Optima for Maximum Parsimony&lt;/a&gt;, &lt;em&gt;Urheim, Ford, St. John&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://web.cse.ohio-state.edu/~mbelkin/"&gt;Mikhail Belkin&lt;/a&gt; spoke about &lt;strong&gt;Eigenvectors of Orthogonally Decomposable Functions: Theory and Applications&lt;/strong&gt;. This was partially lost on me, but what I got was:
- we have a well-defined notion of eigenvectors and eigenvalues for matrices, but what of &lt;em&gt;tensors&lt;/em&gt; (multilinear forms)? There's no spectral theorem here, the idea of rank is different, 'things are just sort of unpleasant'
- focusing on orthogonally-decomposable tensors makes things easier (sort of an analogue of eigen-decomposition)
- then the trick is to recover the 'basis' the tensor is orthogonally-decomposable on
- he said this was primarily about work with Rademacher and Voss, so this paper is likely the reference: &lt;a href="http://arxiv.org/abs/1411.1420"&gt;Basis Learning as an Algorithimic Primitive&lt;/a&gt;, &lt;em&gt;Belkin, Rademacher, Voss&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, &lt;a href="http://seat.massey.ac.nz/personal/s.r.marsland/"&gt;Stephen Marsland&lt;/a&gt; spoke about &lt;strong&gt;Principal Autoparallel Analysis: Data Analysis in Weitzenbock Space&lt;/strong&gt;. This talk got into discussion of connections (maps between elements of tangent spaces), and their curvature, and torsion. It had the same effect that looking at my copy of Spivak's 'A Comprehensive Introduction to Differential Geometry' has: excitement to (re)learn these things but the vague guilt of indulgence in intellectually stimulating but &lt;em&gt;maybe&lt;/em&gt; not so directly applicable mathematics. But so cool. Also the sense of having come so close to &lt;em&gt;getting&lt;/em&gt; fibre bundles. One of these days.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;this talk included an entertaining story about the history of Weitzenbocks spaces, Cartan not receiving recognition, and racist messages hidden in books. Forgetting the umlaut in Weitzenböck's name is OK, because he was a racist.&lt;/li&gt;
&lt;li&gt;we usually look at the Levi-Civita connection, which is unique and torsion-free. This one weird non-zero torsion tensor. Mathematicians &lt;em&gt;hate&lt;/em&gt; it!&lt;/li&gt;
&lt;li&gt;intuitive explanation of curvature: the amount you've rotated upon returning to your original position&lt;/li&gt;
&lt;li&gt;intuitive explanation of torsion: the amount you've failed to return to your original position, sort of, or, 'how hard it is to stay on the manifold'&lt;/li&gt;
&lt;li&gt;Riemann-Cartan space reduces to: Riemannian if torsion is 0, and Weitzenbock if curvature is 0&lt;/li&gt;
&lt;li&gt;cryptic statement in my notes: 'prior over tangent spaces?'&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And that's where my notes end.&lt;/p&gt;
&lt;p&gt;The poster session was really good in that I got to speak about my work a lot, but really bad in that it ended before I got to see anyone else's work, or talk much about my work at all. I had so many more things to say! Good thing I have a blog. I'm also working on a manuscript which is very &lt;em&gt;almost&lt;/em&gt; ready to go on the arXiv, honestly.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://sites.google.com/site/icml2016ersonalization/"&gt;Computational Frameworks for Personalisation Workshop&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Mistakes were made. I spent the first quarter of this workshop working the registration desk, and the second quarter standing &lt;em&gt;outside&lt;/em&gt; the workshop. The afternoon I spent at &lt;a href="https://sites.google.com/site/icml2016data4goodworkshop/"&gt;Machine Learning in Social Good Applications&lt;/a&gt;, which was not a mistake (although I arrived too late to get a t-shirt in my size), as I &lt;em&gt;think&lt;/em&gt; I had already seen the work from &lt;a href="http://www.cs.columbia.edu/~blei/"&gt;David Blei's&lt;/a&gt; talk present at the New York Academy of Sciences Machine Learning Symposium.&lt;/p&gt;
&lt;p&gt;The name of the workshop got truncated to 'Computational Frameworks' on the sign outside, so I got to feel vaguely useful providing disambiguation services while trying to glimpse content. &lt;/p&gt;
&lt;p&gt;The content I was most interested in (and managed to catch part of) was &lt;a href="http://cs.mcgill.ca/~jpineau/"&gt;Joelle Pineau&lt;/a&gt; speaking about &lt;strong&gt;Contextual Bandits for Effective Discovery of Personalized Adaptive Treatment Strategies&lt;/strong&gt;. The focus here is on &lt;em&gt;adaptive&lt;/em&gt; protocols, such as adaptive clinical trials or adaptive treatment strategies. In each case, earlier outcomes influence subsequent decisions: it's, you know, adaptive. The computational framework they use is the multi-armed bandit: you have a set of &lt;em&gt;K&lt;/em&gt; actions with probabilistic outcomes. You don't know the outcomes or the probabilities, but you have to select actions to maximise some expected utility. This poses the classic exploration-exploitation trade-off so integral to sequential decision making. Once you discover an 'ok' action, do you choose it repeatedly (exploiting it), or do you attempt to find yet better actions, risking stumbling upon inferior outcomes (exploration)? This also raises questions about whether it's possible to explore 'safely', which was the subject of &lt;a href="https://las.inf.ethz.ch/krausea"&gt;Andreas Krause's&lt;/a&gt; keynote at AAAI this year. &lt;/p&gt;
&lt;p&gt;Back to exploration-exploitation: In adaptive Bayesian trials, they use &lt;a href="https://en.wikipedia.org/wiki/Thompson_sampling"&gt;Thompson Sampling&lt;/a&gt;. This requires having a posterior over models, sampling one and selecting the action with highest expected utility relative to &lt;em&gt;that&lt;/em&gt; model. So you act greedily given your belief (exploiting), but your belief is random (exploring). Another approach is to define an upper confidence bound &lt;a href="http://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf"&gt;(Auer 2002)&lt;/a&gt;, where you estimate the confidence of the estimate of the expected utility of an action using how many times the action has been tried, and select arms maximising the estimate + the confidence bound. In this way, you select actions which are either &lt;em&gt;very&lt;/em&gt; good, decent and uncertain, or &lt;em&gt;very&lt;/em&gt; uncertain. The third example in her slides is BESA: Best Empirical Sampled Average &lt;a href="http://link.springer.com/chapter/10.1007/978-3-662-44848-9_8"&gt;(Baranski, Maillard, Mannor, 2014)&lt;/a&gt;, which seems to involve subsampling the arm which has more data, then selecting the one with highest expected reward.&lt;/p&gt;
&lt;p&gt;The specific application was &lt;strong&gt;cancer&lt;/strong&gt;, specifically trying to minimise tumour volume in mice. They did a pure exploration phase, where mice with induced tumours had random treatments of combinations of two drugs (fluorouracil and imiquimod). They then considered the adaptive problem of selecting treatments given the current tumour size. This makes it a &lt;em&gt;contextual&lt;/em&gt; bandit problem. They used Gaussian Processes to model the reward function over the space of continuous contexts (tumour sizes) and arms (discrete treatments). Then, given a specific context, you can select the arm maximising the expected reward, using these earlier-described methods. At this point there's a reference to Durand &amp;amp; Pineau 2015 for the GP extension of BESA but I somehow cannot find it. The idea seems to be to re-estimate the GP using a sub-sample of the data, then using that GP to estimate the maximum expected reward. Preliminary results using the adaptive approach look promising, and they're interested in doing sequential reinforcement learning (rather than bandits) in the future.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://sites.google.com/site/icml2016data4goodworkshop/"&gt;Machine Learning In Social Good Applications&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I approximately made it to the &lt;strong&gt;Disease&lt;/strong&gt; section of this workshop, which is unfortunate because I would have liked to see &lt;a href="https://arxiv.org/abs/1606.06121"&gt;Quantifying and Reducing Stereotypes in Word Embeddings&lt;/a&gt;, &lt;em&gt;Bolukbasi et al.&lt;/em&gt; I'd consider this under the umbrella task of removing &lt;em&gt;unwanted&lt;/em&gt; patterns from data, or perhaps more accurately, training a model such that it doesn't pick up on these patterns. See also 'racist algorithms' and this &lt;a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing"&gt;ProPublica piece on Machine Bias&lt;/a&gt;. Will there be a conference summary where I don't mention Fairness, Accountability and Transparency in Machine Learning? Probably not.&lt;/p&gt;
&lt;p&gt;Anyway, I have an especially strong memory of &lt;a href="http://www.hanlab.science/"&gt;Barbara Han's&lt;/a&gt; talk on &lt;strong&gt;Predicting Novel Tick Vectors of Zoonotic Diseases&lt;/strong&gt;, possibly because it contained many horrifying images of ticks. This work is part a project to use &lt;a href="http://www.caryinstitute.org/science-program/research-projects/machine-learning-predict-zoonotic-disease"&gt;machine learning to predict zoonotic diseases&lt;/a&gt;, and also featured a (iirc) undergraduate researcher! The problem is basically: ticks act as disease vectors, but not all of them carry zoonoses. They mined entomological literature (and maybe other sources) to come up with feature sets for ticks, trained a supervised classifier (if I recall they used boosted regression trees), and predicted novel vectors. They also did some feature analysis to understand what differentiates these classes of tick. It turns out that a strong predictor is the number of hosts the tick feeds on. It seems like this could be confounded with the need to feed on a &lt;em&gt;specific&lt;/em&gt; host (since that host has to be reservoir of the zoonosis), I asked and they hadn't done a breakdown looking at the specific species. Anyway, a straight-forward machine learning task but an important problem in ecology and epidemiology.&lt;/p&gt;
&lt;h2&gt;A Rant about the Venue&lt;/h2&gt;
&lt;p&gt;Times Square is the worst. Times Square is why people hate NYC. Tunnels should be built under Times Square so we never have to look at it. I acknowledge its utility to tourists and I reserve through gritted teeth some respect for their bloody-minded dedication to milling at junctions, drifting absent-mindedly across sidewalks, and stopping suddenly. I just don't enjoy being the person trying to weave between them on my way to lunch, especially when it's summer in NYC and I'm an inappropriately-attired Irishwoman. (We don't do 'direct sunlight' very well.)&lt;/p&gt;
&lt;p&gt;I thought of some reasons to locate a conference on Times Square:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the rest of the world has been destroyed&lt;ul&gt;
&lt;li&gt;Times Square stands alone in the void, a final stand for humanity against the encroaching oblivion&lt;/li&gt;
&lt;li&gt;there is nothing left to do but hold conferences&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;there are no other appropriate venues in New York City&lt;/li&gt;
&lt;li&gt;conferences require a density of hotels only offered by Times Square&lt;/li&gt;
&lt;li&gt;holding a conference in what is probably a &lt;em&gt;very&lt;/em&gt; expensive hotel is a demonstration of power and status&lt;ul&gt;
&lt;li&gt;for... someone. ML researchers maybe? 😎&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The venue itself was interesting because the conference was distributed across multiple floors. This meant lots of using the futuristic elevator system. I was involved in more than one 'what algorithm does this elevator system use' conversation. And hey, here's the &lt;a href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node111.html"&gt;chapter of the Sutton Reinforcement Learning book about Elevator Dispatching&lt;/a&gt;.  I wonder how many interesting methods have been developed to solve simple problems arising in the work environment of engineer/scientist types. I certainly used to think about the optimal road-crossing strategy when I lived in NYC (the problem is slightly interesting because east/west and north/south aren't symmetric due to differing block lengths and crossing times, so always going with the go sign isn't an optimal policy[citation required]).&lt;/p&gt;
&lt;p&gt;The negative side-effect of this layout was (to me) a lack of general 'focal point' for the conference, especially since there were various other things going on in the hotel. (Excitingly, on the final day there was an Edward Tufte seminar on the &lt;em&gt;same floor&lt;/em&gt; as us.)&lt;/p&gt;
&lt;p&gt;TL;DR limit registrations to a number your venue can comfortably accommodate. Turning people away is sad (especially if they are, like me, students who only knew they were going once their workshop submission was accepted), but overcrowding is detrimental to good conferencing.&lt;/p&gt;
&lt;h2&gt;In Conclusion&lt;/h2&gt;
&lt;p&gt;Despite missing about half the conference between volunteering, working and being sick, I saw a lot of good work and had some great discussions with people. I'm a bit disappointed there was no proper closing ceremony with summary statistics like at NIPS (unless it was at the party on the Wednesday, which I spent coughing in my hotel room). The multi-track format makes it a little hard to get an overview of the broader field, ad there was a strange lack of closure on the last day. I'd say I'm looking forward to next year, but I &lt;em&gt;think&lt;/em&gt;* it's going to be in Sydney, so we'll see about that.&lt;/p&gt;
&lt;p&gt;*I don't know why I think this and I can't find any evidence supporting it. I did however learn that ICML also stands for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;international conference on minority languages&lt;/li&gt;
&lt;li&gt;international congress of medical librarians&lt;/li&gt;
&lt;li&gt;international conference on chronic myeloid leukaemia&lt;/li&gt;
&lt;li&gt;international conference on malignant lymphoma&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The more you know.&lt;/p&gt;</content><category term="icml"></category><category term="conference"></category><category term="nyc"></category><category term="geometry"></category><category term="reinforcement learning"></category><category term="personalistion"></category></entry><entry><title>characterising treatment pathways at scale using the OHDSI network</title><link href="/biomed/2016-06-15-characterising-treatment-pathways-at-scale-using-the-ohdsi-network.html" rel="alternate"></link><published>2016-06-15T00:00:00+01:00</published><updated>2016-06-15T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2016-06-15:/biomed/2016-06-15-characterising-treatment-pathways-at-scale-using-the-ohdsi-network.html</id><summary type="html">&lt;p&gt;This post is about the paper &lt;a href="http://www.pnas.org/content/early/2016/06/01/1510502113.full"&gt;Characterizing treatment pathways at scale using the OHDSI network&lt;/a&gt; from the hefty author list: &lt;em&gt;George Hripcsak, Patrick B. Ryan, Jon D. Duke, Nigam H. Shah, Rae Woong Park, Vojtech Huser, Marc A. Suchard, Martijn J. Schuemie, Frank J. DeFalco, Adler Perotte, Juan M. Banda …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is about the paper &lt;a href="http://www.pnas.org/content/early/2016/06/01/1510502113.full"&gt;Characterizing treatment pathways at scale using the OHDSI network&lt;/a&gt; from the hefty author list: &lt;em&gt;George Hripcsak, Patrick B. Ryan, Jon D. Duke, Nigam H. Shah, Rae Woong Park, Vojtech Huser, Marc A. Suchard, Martijn J. Schuemie, Frank J. DeFalco, Adler Perotte, Juan M. Banda, Christian G. Reich, Lisa M. Schilling, Michael E. Matheny, Daniella Meeker, Nicole Pratt, and David Madigan&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let's have at it. &lt;em&gt;Note: including figures is needlessly time-consuming for me, so I'm going to refer to the paper assuming you have it to hand.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;tl;dr&lt;/h3&gt;
&lt;p&gt;They looked at which medications patients received, for one of three diseases (type 2 diabetes, hypertension, depression), considering sequences of medications. Diabetes treatment is mostly dominated by metformin, and there is more variation for the other diseases. Many patients only ever receive metformin. They break it down by medical centre and find hetereogeneity between centres (and thus countries). Heterogeneity suggests we attempt to generalise with care.&lt;/p&gt;
&lt;h3&gt;What is OHDSI?&lt;/h3&gt;
&lt;p&gt;Pronounced 'Odyssey', OHDSI is the Observational Health Data Sciences and Informatics collaboration. From &lt;a href="http://www.ohdsi.org/"&gt;the website&lt;/a&gt;, 'OHDSI has established an international network of researchers and observational health databases with a central coordinating center housed at Columbia University.' I was shamefully unaware of its existence, despite it being very relevant to my interests. Evidence-based medicine through data analaysis! International collaboration! Open source! Reproducibility! All great. Fawning section over, on to the contents of the paper.&lt;/p&gt;
&lt;h3&gt;What did they do?&lt;/h3&gt;
&lt;p&gt;They analysed data from the OHDSI collection of databases to look at &lt;em&gt;treatment pathways&lt;/em&gt; (ordered sequences of &lt;em&gt;medications&lt;/em&gt; given to a patient) for three diseases: hypertension, diabetes mellitus type 2, and depression. Details in subsequent sections.&lt;/p&gt;
&lt;h3&gt;Why did they do it?&lt;/h3&gt;
&lt;p&gt;This feels like a proof-of-concept paper to me. The concept being that large-scale collaborations involving multiple health centres are possible, and that insights can be gained from analysis of the data. Essentially, the mission of OHDSI. More specifically, supporting the use of &lt;em&gt;observational data&lt;/em&gt; to supplement medical research, which classically relies heavily on clinical trials. Observational data is 'free' in a sense (data-collection and storage, privacy-violating concerns &lt;em&gt;temporarily&lt;/em&gt; aside), can cover wider populations and goes on indefinitely. Exploiting that has clear benefits. They highlight three key areas of benefit:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identifying which current therapies should be compared with a new therapy (for experimental design)&lt;/li&gt;
&lt;li&gt;Testing clinical hypotheses on observational data (acknowledging the need to do the appropriate statistical modelling)&lt;/li&gt;
&lt;li&gt;Understanding population characteristics to aid in extrapolation of results (both observational and experimental)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This study focuses mainly on the first point, as the look at medication trends.&lt;/p&gt;
&lt;h3&gt;Data resources&lt;/h3&gt;
&lt;p&gt;OHDSI, at the time of writing, has 52 databases containing 682 million patient records. For this study they used 11 databases with 250 million records. I don't know why they didn't use all the data. These databases were: (this is Table 2)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AUSOM (Ajou University School of Medicine, Korea)&lt;/li&gt;
&lt;li&gt;CCAE (MarketScan Commerical Claims and Encounters, I guess USA)&lt;/li&gt;
&lt;li&gt;CPRD (UK Clinical Practice Research Datalink)&lt;/li&gt;
&lt;li&gt;CUMC (Columbia University Medical Centre, USA)&lt;/li&gt;
&lt;li&gt;GE (General Electric Centricity, I guess USA)&lt;/li&gt;
&lt;li&gt;INPC (Regenstrief Institute, Indiana Network for Patient Care, USA)&lt;/li&gt;
&lt;li&gt;JMDC (Japan Medical Data Center)&lt;/li&gt;
&lt;li&gt;MDCD (MarketScan Medicaid Mult-state, USA)&lt;/li&gt;
&lt;li&gt;MDCR (MarketScan Medicare Supplement and Coordination of Benefits)&lt;/li&gt;
&lt;li&gt;OPTUM (Optum ClinFormatics, I guess USA)&lt;/li&gt;
&lt;li&gt;STRIDE (Stanford Translational Research Integrated Database Environment, USA)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So that's one from the UK, one from Japan, one from Korea and eight from the USA. The biggest population by far was CCAE, which contributed 119 million patients. Japan and Korea only comprised 5 million patients together, and the UK 11 million, so &lt;em&gt;most&lt;/em&gt; of these patients are in the USA.&lt;/p&gt;
&lt;p&gt;The databases have various types of data in them, which is of great interest to me, but in this study they just extracted medications.&lt;/p&gt;
&lt;h3&gt;Data processing&lt;/h3&gt;
&lt;h4&gt;Filtering for patients&lt;/h4&gt;
&lt;p&gt;So: which patients did they include in the analysis? &lt;/p&gt;
&lt;p&gt;Patients had to satisfy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;≥ 4 continuous years in the database&lt;ul&gt;
&lt;li&gt;≥ 1 year &lt;em&gt;before&lt;/em&gt; any treatment for that disease&lt;/li&gt;
&lt;li&gt;≥ 3 years of continuous treatment after that (this means patients who died during the period were excluded)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;≥ 1 diagnosis code for corresponding disease&lt;/li&gt;
&lt;li&gt;0 diagnosis codes for &lt;em&gt;excluded&lt;/em&gt; diagnoses (these were: pregnancy for all, diabetes type 1 for diabetes type 2, and bipolar 1 disorder or schizophrenia for depression)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This resulted in 1,182,792 hypertension patients, 327,110 diabetes patients, 264,841 depression patients. I'm not sure what the breakdown by centre was.&lt;/p&gt;
&lt;p&gt;Excluding patients who died during that period seems problematic to me, because that's &lt;em&gt;probably&lt;/em&gt; not a random event. I worry about excluding subpopulations with more aggressive forms of the disease, or excluding &lt;em&gt;badly-treated&lt;/em&gt; patients (although that's slightly outside the scope of this paper I think, but is a question of particular interest to me). The phenotype here is already incredibly broadly defined - what if the observed heterogeneity in treatment pathways is due to such subpopulations? I'm not sure what a better approach here would have been, though - exclude patients who died of reasons &lt;em&gt;unrelated&lt;/em&gt; to the disease, perhaps?&lt;/p&gt;
&lt;h4&gt;Data standardisation&lt;/h4&gt;
&lt;p&gt;Diagnoses were defined by mapping SNOMED (Systematized Nomenclature of Medicine) and Medical Dictionary for Regulatory Activities to ICD-9-CM (International Classification of Diseases, ninth revision, clinical modification).
Medications were defined by their ingredients using RxNorm, and grouped according to classification hierarchies (such as, they state, Anatomical Therapeutic Chemical classification and First Data Bank's terminology).
I'm not especially familiar with these ontologies, except for SNOMED. Most of what I've done to date involved UMLS (which contains SNOMED and possibly everything else that has existed).&lt;/p&gt;
&lt;h4&gt;Constructing medication sequences&lt;/h4&gt;
&lt;p&gt;Having filtered to these patients they queried the OHDSI databases for the sequences of medications for these patients. Some notes on this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sequences were limited to a maximum of 20 medications&lt;/li&gt;
&lt;li&gt;if a patient switched from one medication and then later back to it, only the first exposure was recorded&lt;/li&gt;
&lt;li&gt;combination medications (with multiple active ingredients) were treated as prescriptions of multiple single-ingredient medicines&lt;/li&gt;
&lt;li&gt;I don't think the time between medications is considered - they're just ordered sequences of drugs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Having defined these sequences, they then counted the numbers of patients with each sequence and did other analyses. For example, they looked at medication &lt;em&gt;classes&lt;/em&gt;, which are listed in table 1.&lt;/p&gt;
&lt;h3&gt;What did they find?&lt;/h3&gt;
&lt;p&gt;Also known as: let's look at the figures!&lt;/p&gt;
&lt;h4&gt;Figure 2&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Which drugs do patients get &lt;em&gt;first&lt;/em&gt;? Is there a standard entry into treatment-for-disease?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For diabetes, it seems yes. 76% of patients start with metformin. For hypertension, hydrochlorothiazide is &lt;em&gt;sort of&lt;/em&gt; most popular (I am squinting at the figure), and in depression citalopram is also &lt;em&gt;sort of&lt;/em&gt; most popular, but there's no clear winner. This is where I wonder about subpopulations. The immediate questions are: what's different about these patients? Why did they receive a different first medication? Does it vary by centre (yes - see figure 3)? By other diagnoses? Age? So many variables to consider! (I realise that this paper cannot answer all of these questions and I'm not criticising it - the results just inspire further research.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Do patients stay on a single drug?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For diabetes, 29% of patients took &lt;em&gt;only&lt;/em&gt; metformin. For hypertension, 6.44% took only lisinopril. For depression, 5.18% took only citalopram. Once again I wonder what this means. Was this medication especially effective for them, and if yes why? We see the potential for this large-scale observational data to shed light on differences in response to therapy that might be missed on the smaller-scale of a clinical trial. Maybe.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unique treatment pathways?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Some patients are unique in the entire dataset: 10% of diabetes patients, 24% of hypertension patients, 11% of depression patients have unique treatment pathways. Clearly doing a nearest-neighbour treatment recommendation approach would fail for these patients, although I wonder if these patients may simply have rather &lt;em&gt;long&lt;/em&gt; sequences of medications? It might be in the supplemental data, but I wonder what the distribution of sequence length is.&lt;/p&gt;
&lt;h4&gt;Figure 3&lt;/h4&gt;
&lt;p&gt;This is figure 2 but broken down by data centre, for some samples. We see immediately that metformin is less popular in the Japanese database than in the UK or US examples shown. I think the overall gist of this figure is that there is between-centre heterogeneity, and also (as in Figure 2) heterogeneity in the choice of second-line drugs. You could definitely look deeper into this data (hence my feeling that this paper is a proof of concept), but there is a risk (as always) of wading around without a clear hypothesis.&lt;/p&gt;
&lt;h4&gt;Figure 4&lt;/h4&gt;
&lt;p&gt;The y-axis here is a fraction of patients in the population. The fraction of interest is given by the lettering. x-axis is time, so we're looking at medicating trends.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;A&lt;/em&gt;: patients on monotherapy: this became somewhat more popular&lt;/li&gt;
&lt;li&gt;&lt;em&gt;B&lt;/em&gt;: patients on monotherapy which is the &lt;em&gt;most popular monotherapy&lt;/em&gt; for that diesase: the medication is listed with the disease now (so this is a subset of the patients in &lt;em&gt;A&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;C&lt;/em&gt;: patients whose first medication started with the &lt;em&gt;most popular starting medication&lt;/em&gt; for that disease (not necessarily most popular monotherapy)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The conclusion from B is that monotherapy in diabetes is somewhat dominated by metformin, whereas in hypertension and depression there is more variation.&lt;/p&gt;
&lt;p&gt;I don't know how they decided which drug was most popular - is this over all patient trajectories over all time (I suspect yes)? It seems unlikely but the apparent absence of a dominant monotherapy in hypertension and depression &lt;em&gt;could&lt;/em&gt; be explained by a strong bias towards some drugs being popular at some times: so at any moment in time there &lt;em&gt;is&lt;/em&gt; a dominant monotherapy, but because its identity is always changing, it goes undetected by this analysis. Or more similarly, there &lt;em&gt;is&lt;/em&gt; a dominant monotherapy, but it's not lisinopril/sertraline. Would this be an interesting finding? Perhaps. Discovering that medication practices are highly influenced by trends could be a cause for concern. Equally, finding that medication practices lag (between centres or behind research) could also be concerning. Or heartening. Who knows.&lt;/p&gt;
&lt;h4&gt;Figure 5&lt;/h4&gt;
&lt;p&gt;This is figure 4 but now the data series corespond to data &lt;em&gt;centre&lt;/em&gt;, and the different diseases get their own graphs. They bind the y axes together across rows, so there are inset graphs to give the zoomed-in views. Mmm, data visualisation.&lt;/p&gt;
&lt;p&gt;There's so much going on here that looking at this figure fills me with vague dread. We have the potential to learn how data centres vary in their medicating trends.&lt;/p&gt;
&lt;p&gt;Gravitating towards the most extreme-looking data series, something is going on in STRIDE (US) for monotherapy. 100% of diabetes patients in 2004 were on metformin? This is also when this database appears to begin, so I guess something strange was going on (like only data from diabetes patients on metformin was being recorded, or something)...&lt;/p&gt;
&lt;p&gt;The authors draw attention to the lack of consistent bias between use of EHR data and claims data in what they report. This is potentially very interesting, because claims data is somewhat more 'available' from what I can tell (people seem to be publishing more with claims data[citation required]), but is biased towards billing (obviously) and less 'rich' than a full EHR. Being able to use claims data as a proxy for EHR would be good and useful. &lt;em&gt;However&lt;/em&gt;, the analyses here draw on medication information, which is &lt;em&gt;probably&lt;/em&gt; well covered by claims data, so the finding is &lt;em&gt;probably&lt;/em&gt; less striking.&lt;/p&gt;
&lt;h4&gt;Figure 6&lt;/h4&gt;
&lt;p&gt;Once again, we see a fraction of something on the y-axis, with time on the x-axis. In this case, it's the fraction of medication &lt;em&gt;changes&lt;/em&gt; in that year which were &lt;em&gt;within the same structural class&lt;/em&gt; (these classes are &lt;em&gt;not&lt;/em&gt; fully listed in table 1, and are definitely in the supplemental information). &lt;/p&gt;
&lt;p&gt;I am not sure what to conclude from this figure. Do different strutural classes correspond to very different mechanism of action for the drug? Would changing structural class mean the doctor believes the patient's disease to be characterised differently? I am not a doctor (as might be obvious) and I'm cancer-focused so I'm speculating wildly here. There isn't much discussion of this figure in the main paper. Not much of a trend is observed, anyway.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I reiterate my feeling that this is a proof of concept paper, or possibly a paper to advertise the &lt;em&gt;seemingly incredibly amazing&lt;/em&gt; data resource OHDSI is creating. There aren't really any hypotheses tested in this work, and I don't come away from it with a strong conclusion beyond 'heterogeneity exists'. Then again, I came into this paper with little by way of prior expectation for the findings. &lt;/p&gt;
&lt;p&gt;There are some further avenues of research (some of which I mentioned in this blog post) prompted by this study, but whether they're truly worth pursuing requires further thought, as ever. And I'm definitely going to check out what else OHDSI is up to.&lt;/p&gt;</content><category term="papers"></category><category term="EHR"></category><category term="open access"></category></entry><entry><title>play me a match of doto for your heart</title><link href="/life/2016-06-11-play-me-a-match-of-doto-for-your-heart.html" rel="alternate"></link><published>2016-06-11T00:00:00+01:00</published><updated>2016-06-11T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2016-06-11:/life/2016-06-11-play-me-a-match-of-doto-for-your-heart.html</id><summary type="html">&lt;p&gt;Here are some strange messages I have received on OKCupid.&lt;/p&gt;
&lt;h3&gt;???&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;you are evil // Sorry you do not you're Sorry&lt;/li&gt;
&lt;li&gt;hi // how are you? // what did you mean with that you are dog person&lt;/li&gt;
&lt;li&gt;Hi. How are you? Please I need a favor&lt;/li&gt;
&lt;li&gt;Hey bitch, are you studying computer science? &lt;em&gt;(this …&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Here are some strange messages I have received on OKCupid.&lt;/p&gt;
&lt;h3&gt;???&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;you are evil // Sorry you do not you're Sorry&lt;/li&gt;
&lt;li&gt;hi // how are you? // what did you mean with that you are dog person&lt;/li&gt;
&lt;li&gt;Hi. How are you? Please I need a favor&lt;/li&gt;
&lt;li&gt;Hey bitch, are you studying computer science? &lt;em&gt;(this is probably the
    closest thing to 'abuse' I have received on the site)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;I love dogs tho&lt;/li&gt;
&lt;li&gt;All of which are American dreams comrade&lt;/li&gt;
&lt;li&gt;MY GOD YOU HAVE A FACE // Sorry, was that rude? Is it rude to tell a
    woman she has a face?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;people respond to my profile&lt;/h3&gt;
&lt;p&gt;The context here is that I write very small pieces of speculative
fiction in the 'explanation' box for my questions. Someone asked if they
were Cormac McCarthy quotes once, which may be my greatest achievement.
I've also gotten a few messages written in the same style as my
responses. Dating site as platform for collaborative fiction-writing,
anyone?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I could not and still can not tell if your page is a real dating
    profile or just a place to either copy and paste or write original
    satire&lt;/li&gt;
&lt;li&gt;I have an incredible amount of respect for your commitment to
    this bit.&lt;/li&gt;
&lt;li&gt;five stars for FUCKING TERRIFYING OH MY GODDDDDDDDDDDD&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;are you a Turing test?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;play me a match of doto for your heart &lt;em&gt;(amazing)&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Ayyy gurl, no need to get a BKB cuz my love is pure.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Never change, internet.&lt;/p&gt;</content><category term="lol"></category><category term="dating"></category><category term="dota"></category></entry><entry><title>moved to pelican</title><link href="/meta/2016-05-17-moved-to-pelican.html" rel="alternate"></link><published>2016-05-17T00:00:00+01:00</published><updated>2016-05-17T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2016-05-17:/meta/2016-05-17-moved-to-pelican.html</id><summary type="html">&lt;p&gt;Here's what happened:&lt;/p&gt;
&lt;p&gt;My phone started acting up. The lock button turned into a 'maybe turn the phone off' button. The unlock button turned into a 'maybe turn the phone off, &lt;em&gt;or&lt;/em&gt; open the camera?' button. I impulse-purchased a new phone, and the old one magically fixed itself. New phone …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here's what happened:&lt;/p&gt;
&lt;p&gt;My phone started acting up. The lock button turned into a 'maybe turn the phone off' button. The unlock button turned into a 'maybe turn the phone off, &lt;em&gt;or&lt;/em&gt; open the camera?' button. I impulse-purchased a new phone, and the old one magically fixed itself. New phone went into a box and my SIM remained micro-sized for a little longer.&lt;/p&gt;
&lt;p&gt;I moved to Switzerland, got a nano SIM, started dual-wielding phones. New phone for new SIM, old phone for everything else. I migrated my apps to new phone (this was surprisingly tricky and there's an entire story in there about me conveniently being in the USA to receive a single text message, but it's a tangent), so I got a new Signal fingerprint. (Then my old phone bricked itself, as they do.)&lt;/p&gt;
&lt;p&gt;I went to put my new Signal fingerprint on my lovely SSLy Github-pages website and noticed it was broken. For how long this was the case I don't know. I think it was a Jekyll update? Who knows. The time had come. I'd been thinking of leaving gh-pages for a while, so this was a good excuse.&lt;/p&gt;
&lt;p&gt;Reasons for wanting to leave:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;using plugins with Jekyll and gh-pages is mildly painful&lt;/li&gt;
&lt;li&gt;the solution, and/or Jekyll itself was &lt;em&gt;increasingly frustratingly slow&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;gh-pages was too mysterious&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and the new reason:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;website was mysteriously broken&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are probably fixable, I am sure. Maybe my solution isn't the best, but it's mine. I decided to: &lt;strong&gt;move from Jekyll to &lt;a href="link to pelican"&gt;Pelican&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;host my site myself&lt;/strong&gt;. By the time you're reading this, I'll have achieved that second part. At the time I'm writing it, I haven't even started.&lt;/p&gt;
&lt;p&gt;Why did I do this?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jekyll uses Ruby, Pelican uses Python, I know Python a &lt;em&gt;lot&lt;/em&gt; better than Ruby&lt;/li&gt;
&lt;li&gt;I never knew Jekyll very well, so there were no temptingly sunk costs to care about&lt;/li&gt;
&lt;li&gt;I saw a site using Pelican during an impressionable moment&lt;/li&gt;
&lt;li&gt;self-hosting (in a VPS, let's not be unreasonable) affords a level of control that I apparently want&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, this site now looks &lt;em&gt;rather different&lt;/em&gt;. That's because another thing happened: I realised that I'm not a front-end web developer, or designer, or basically a person who has touched HTML since she was making Pokémon fansites as a ten-year-old. But I'd also like my site to look good. The old version, [which I should screenshot for posterity] was my first adventure into CSS and was unsurprisingly minimalist. I got some compliments on the design of it (yay!) but it was very hand-crafted and it looked it. &lt;/p&gt;
&lt;p&gt;Now I am balancing competing desires: a site which &lt;strong&gt;looks good&lt;/strong&gt; (and works well), and a site which is &lt;strong&gt;my creation&lt;/strong&gt;. The solution for now is to use an existing Pelican theme, made by someone who presumably knows a lot more about websiting than I do, and modify it to my own purposes. Apparently the one I picked was intended to look like &lt;a href="..."&gt;Medium&lt;/a&gt; so now after several hours of mucking around I have a Medium blog with no features. Excellent. &lt;/p&gt;
&lt;p&gt;I'm modifying the theme (see &lt;a href="https://github.com/corcra/medius"&gt;my fork&lt;/a&gt;) and I will continue to do so until I get distracted by &lt;a href="http://www.internationalphoneticalphabet.org/ipa-sounds/ipa-chart-with-sounds/"&gt;something else&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This post brought to you by an internet outage at the AirBnB I currently live in.&lt;/em&gt;&lt;/p&gt;</content><category term="webdev"></category><category term="python"></category><category term="pelican"></category><category term="css"></category><category term="phone"></category><category term="so many tags why"></category><category term="IPA"></category></entry><entry><title>installing tmux locally</title><link href="/tips/2016-03-13-installing-tmux-locally.html" rel="alternate"></link><published>2016-03-13T00:00:00+00:00</published><updated>2016-03-13T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2016-03-13:/tips/2016-03-13-installing-tmux-locally.html</id><summary type="html">&lt;p&gt;I have been setting myself up on a new computing cluster (CentOS 6.7), so I'm in the lovely land of installing things without root. &lt;code&gt;tmux&lt;/code&gt; proved a bit frustrating, so here's what I ended up doing:&lt;/p&gt;
&lt;h2&gt;install libevent&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;tmux&lt;/code&gt; needs this, I didn't have it (you &lt;em&gt;might&lt;/em&gt;, so try …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have been setting myself up on a new computing cluster (CentOS 6.7), so I'm in the lovely land of installing things without root. &lt;code&gt;tmux&lt;/code&gt; proved a bit frustrating, so here's what I ended up doing:&lt;/p&gt;
&lt;h2&gt;install libevent&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;tmux&lt;/code&gt; needs this, I didn't have it (you &lt;em&gt;might&lt;/em&gt;, so try installing &lt;code&gt;tmux&lt;/code&gt; first). Grabbed it from the repository, then:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone https://github.com/libevent/libevent
$ cd libevent
$ ./configure --prefix=$HOME
$ make
$ make verify       # this failed for me, oh well
$ make install
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Choose your &lt;code&gt;prefix&lt;/code&gt; as you desire.&lt;/p&gt;
&lt;p&gt;Now &lt;code&gt;libevent&lt;/code&gt; should be installed in &lt;code&gt;$HOME&lt;/code&gt;. Easy y0.&lt;/p&gt;
&lt;h2&gt;install tmux&lt;/h2&gt;
&lt;p&gt;Git all the things.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone https://github.com/tmux/tmux
$ cd tmux
$ zsh autogen.sh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now for the thing which was &lt;em&gt;required to make everything work&lt;/em&gt;, a slightly augmented version of the &lt;a href="http://unix.stackexchange.com/a/17918"&gt;winning answer&lt;/a&gt; from &lt;a href="https://unix.stackexchange.com/questions/17907/why-cant-gcc-find-libevent-when-building-tmux-from-source"&gt;this Stack Exchange post&lt;/a&gt;...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/lib         # possibly optional, see below
$ DIR=$HOME
$ ./configure CFLAGS=&amp;quot;-I$DIR/include&amp;quot; LDFLAGS=&amp;quot;-L$DIR/lib&amp;quot; --prefix=$DIR
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Obviously I could have used &lt;code&gt;$HOME&lt;/code&gt; there instead of &lt;code&gt;$DIR&lt;/code&gt;, but I am staying consistent.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ make
$ make install
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And everything should Just Work now. This probably shouldn't have posed the difficulty that it did, but I'm a scientist, not a sysadmin.&lt;/p&gt;
&lt;h2&gt;final thing&lt;/h2&gt;
&lt;p&gt;Does the &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; line seem redundant to you? It seems redundant to me, but I had to add it to my &lt;code&gt;.zshenv&lt;/code&gt; (or, try &lt;code&gt;.bashrc&lt;/code&gt;) after this to get &lt;code&gt;tmux&lt;/code&gt; to continue working. May or may not have been necessary during installation (what with all that redundancy) but I'm not going to uninstall just to check, because tmux is &lt;em&gt;actually working&lt;/em&gt; and if I look at it too closely it will &lt;em&gt;definitely break&lt;/em&gt;.&lt;/p&gt;</content><category term="unix"></category><category term="tmux"></category></entry><entry><title>AAAI 2016 by the day</title><link href="/ml/2016-02-17-aaai2016.html" rel="alternate"></link><published>2016-02-17T00:00:00+00:00</published><updated>2016-02-17T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2016-02-17:/ml/2016-02-17-aaai2016.html</id><summary type="html">&lt;p&gt;I started writing this in Phoenix airport, so if the &lt;a href="/ml/2015-12-14-nips2015.html"&gt;current trend&lt;/a&gt; (n=2) continues, I'll start recounting my next conference half-way through, with interesting implications for the latter half of the post. This was my first time attending the Association for the Advancement of Artificial Intelligence Conference &lt;a href="https://www.aaai.org/Conferences/AAAI/aaai16.php"&gt;(AAAI)&lt;/a&gt;, so …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I started writing this in Phoenix airport, so if the &lt;a href="/ml/2015-12-14-nips2015.html"&gt;current trend&lt;/a&gt; (n=2) continues, I'll start recounting my next conference half-way through, with interesting implications for the latter half of the post. This was my first time attending the Association for the Advancement of Artificial Intelligence Conference &lt;a href="https://www.aaai.org/Conferences/AAAI/aaai16.php"&gt;(AAAI)&lt;/a&gt;, so I made sure to spend most of it comparing it to NIPS. I stopped taking notes towards the end, so this coverage is a bit skewed.&lt;/p&gt;
&lt;h2&gt;Thursday&lt;/h2&gt;
&lt;p&gt;Shameless plug for a great vegan restaurant in Phoenix: &lt;a href="http://greenvegetarian.com/"&gt;Green&lt;/a&gt;. I would have eaten there a lot more if it were a bit closer to the conference centre. I ended up going to &lt;a href="http://www.veganhouseaz.com/"&gt;Vegan House&lt;/a&gt; a few times. A fair runner up in the list of Best Vegan(-ish) Restaurants in Downtown Phoenix (there are two).&lt;/p&gt;
&lt;h2&gt;Friday&lt;/h2&gt;
&lt;p&gt;Shortly before dawn, it became cold enough to sleep. I appreciated the vastness of the Arizona sky and the eerie absence of fellow pedestrians as I relocated to my downtown hotel. Coming from Dublin and then New York City, I find empty paths unsettling, especially coupled with wide roads and low buildings. I passed by a man selling paintings of owls from a rucksack, and order was restored.&lt;/p&gt;
&lt;p&gt;Friday and Saturday of the conference were tutorial/workshop days (the distinction between these categories is not clear). On Friday morning I went to...&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://www.cs.cmu.edu/~sandholm/organExchangeTutorials/organExchangeTutorial.aaai16.html"&gt;Organ Exchanges: A Success Story of AI in Healthcare: John Dickerson and Tuomas Sandholm&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I'd seen John Dickerson speaking at the &lt;a href="https://sites.google.com/site/nipsmlhc15/"&gt;NIPS 2015 Workshop on Machine Learning in Healthcare&lt;/a&gt; (some thoughts on MLHC in my &lt;a href="https://corcra.github.io/ml/2015/12/14/NIPS2015.html"&gt;NIPS 2015 post&lt;/a&gt;), so I was already somewhat familiar with this work. I think he's a good speaker, so even though this topic is not &lt;em&gt;entirely&lt;/em&gt; relevant to me, I figured I'd get something out of the tutorial. This was true to some extent - my attention started to flag at some point into what was essentially a 3.5 hour lecture.&lt;/p&gt;
&lt;p&gt;The link to the slides is above and &lt;a href="https://www.cs.cmu.edu/~sandholm/organExchangeTutorials/organExchangeTutorial.aaai16.html"&gt;here&lt;/a&gt;, so I will just outline the main idea and skip the algorithmic details.&lt;/p&gt;
&lt;p&gt;Kidney exchanges: you need a kidney, and a family member/friend/loved one is willing to donate one. Unfortunately, they may not be compatible. The solution is to 'trade' donors with someone else: "I'll give you my mother's kidney for your girlfriend's kidney", or, "I'll give you my mother's kidney so your girlfriend can give her kidney to that other person, and their friend can give &lt;em&gt;me&lt;/em&gt; their kidney", and so on. This amounts to finding cycles in a graph (the second example being a 3-cycle), which brings us into the wonderful world of combinatorial optimisation. The exchange actually requires everyone to go under the knife at the same time (something about trading organs I don't quite recall), so there are physical and logistical limits on the length of the cycle.&lt;/p&gt;
&lt;p&gt;They mentioned some other barter-exchange markets, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;holiday homes (&lt;a href="www.intervac-homeexchange.com/"&gt;intervac&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;books (&lt;a href="http://www.paperbackswap.com/index.php"&gt;paperback swap&lt;/a&gt;, &lt;a href="http://www.bookcrossing.com/"&gt;book crossing&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;odd shoes (&lt;a href="http://www.oddshoe.org/"&gt;national (US) odd shoe exchange&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are neat. People exchanging used items instead of buying new/throwing away is obviously great, and I approve of anyone supporting such efforts. It's what the 'sharing economy' should have been... and now back to organs.&lt;/p&gt;
&lt;p&gt;An interesting (and amazing!) thing can happen in these kidney exchanges: sometimes an altruistic donor will show up; someone who just has too many kidneys and wants to help out. These produce 'never-ending altruistic donor' chains ("a gift that gives forever"), and have apparently become more important than cycles for the kidney-matching problem.&lt;/p&gt;
&lt;p&gt;I zoned out of the tutorial for a bit to discuss the feasibility of simultaneous translation, prompted by this article: &lt;a href="http://www.wsj.com/articles/the-language-barrier-is-about-to-fall-1454077968"&gt;The Language Barrier is About to Fall&lt;/a&gt;. My gut reaction is to say 'it's too hard', but that's motivated by my enjoyment of learning languages - part of me (selfishly) doesn't &lt;em&gt;want&lt;/em&gt; this problem solved. I'm however learning to temper my skepticism when it comes to what machine learning can achieve, and we're actually getting pretty good at translation (for &lt;em&gt;some&lt;/em&gt; language pairs) so I'm pretty optimistic about this. And breaking language barriers, if it can be done cheaply, could be immense. I emphasize the relevance of cost because I see language most prohibitive not for holiday-goers but for migrants, who may not have the resources to buy a &lt;a href="https://en.wikipedia.org/wiki/List_of_races_and_species_in_The_Hitchhiker's_Guide_to_the_Galaxy#Babel_fish"&gt;babelfish&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are a lot of subtleties to consider in the kidney exchange problem, and much work has been done: see the slides.&lt;/p&gt;
&lt;p&gt;They concluded the tutorial with a discussion of other organ exchanges. Kidneys are sort of 'easy' because the cost to the donor is quite minimal, unlike in e.g. lung exchanges where the donor's quality of life (and life expectancy) are impacted. One can also do living donor liver exchanges, where some &lt;em&gt;fraction&lt;/em&gt; of the donor's liver is removed. There are essentially no altruistic donors here. Dickerson suggested combining multiple organs, so you thread a liver and kidney chain together. Perhaps a kidney patient's donor would be willing to donate liver to someone whose donor would give a kidney, and so on.&lt;/p&gt;
&lt;p&gt;My plan was to go to &lt;a href="http://www.inf.kcl.ac.uk/staff/danmag/aaai16tutorial_aips.html"&gt;&lt;strong&gt;AI Planning and Scheduling for Real-World Applications&lt;/strong&gt; (Steve Chien and Daniele Magazzeni)&lt;/a&gt; in the afternoon, but I made the mistake of being outside for slightly too long during lunch, and I spent the rest of the afternoon recovering in a dark and cool hotel room. Irish people: handle with care, keep out of direct sunlight.&lt;/p&gt;
&lt;h3&gt;Student Welcome Reception&lt;/h3&gt;
&lt;p&gt;One really nice thing about AAAI was the student activities. Being a student at a conference can be bewildering: there are so many people who seem to know each other, talking about things they seem to know about! I was also there by myself (my group does not typically attend AAAI), so the icebreakers they ran saved me from spending the rest of the conference lurking in corners and hissing at people.&lt;/p&gt;
&lt;p&gt;The actual ice-breaker activity was weird (although seemingly effective): we had to take photographs with a AI/AAAI/Phoenix theme (artificially intelligent &lt;em&gt;fire&lt;/em&gt;, maybe) featuring ourselves. A ploy to get pictures for a website? Possibly. We never did find out who won the fabled prize.&lt;/p&gt;
&lt;h2&gt;Saturday&lt;/h2&gt;
&lt;p&gt;&lt;img src="/images/aaai2016_01.png" class="floatl" style="width: 24vw;"&gt;&lt;/p&gt;
&lt;p&gt;Excluding a brief foray into the tutorial about 'Learning and Inference in Structured Prediction Models', and fruitless wandering in search of coffee shops open on a Sunday, I spent much of the day at...&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://www.cse.unsw.edu.au/~tw/aiethics/AI_Ethics/Introduction.html"&gt;Workshop on AI, Ethics, and Society&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This workshop had overlap in content/speakers/organisers with the 'Algorithms Among Us' symposium at NIPS 2015 (some thoughts &lt;a href="https://corcra.github.io/ml/2015/12/14/NIPS2015.html"&gt;here&lt;/a&gt;). My interests might be obvious by now.&lt;/p&gt;
&lt;p&gt;This was an interesting workshop. There was a mix of machine learners, AI researchers, (possibly) philosophers and miscellaneous other. There were fewer arguments than I would have expected. It's not that that I particularly &lt;em&gt;wanted&lt;/em&gt; to see (verbal) fighting, but people seem quite passionate about, e.g. whether or not The Singularity is something to worry about, so I expected more gloves on the floor.&lt;/p&gt;
&lt;p&gt;People are concerned about dangerous (powerful) AIs - how do we ensure they don't enslave us all in pursuit of paperclip-making? Do we have moral responsibility towards them? Should they feel pain? Should we be allowed to turn them off, once they're active/alive(?)? Are simulations of humans humans? These were some questions raised.&lt;/p&gt;
&lt;p&gt;Some more, uhh, &lt;em&gt;short-term&lt;/em&gt; concerns included the risks of adversarial machine learning, the effects of AI on labour markets (more on this later), the difficulty of measuring progress towards AGI, and enough other things that I didn't leave the workshop thinking &lt;em&gt;everyone&lt;/em&gt; is feeling Existentially Threatened. I certainly am not. &lt;/p&gt;
&lt;p&gt;I'm glad some people are thinking about long term threats (diversity of tactics!), but I am much more worried about the present and near future. AI (rather machine learning) already influences people, in potentially &lt;a href="http://arstechnica.co.uk/security/2016/02/the-nsas-skynet-program-may-be-killing-thousands-of-innocent-people/"&gt;irreversibly life-altering ways&lt;/a&gt; (to put it mildly), and I fear the technology is becoming integrated into society faster than anyone can measure its harm (see also: vaping). It's also quite easy for us as researchers to pretend our work is apolitical, that we simply explore and create things, blissfully ignorant of negative consequences should our creations be &lt;em&gt;misused&lt;/em&gt;. Positive applications presumably motivate much great work, and I don't wish that people &lt;em&gt;stop&lt;/em&gt; this work, necessarily. We just need to acknowledge that we cannot &lt;i&gt;un&lt;/i&gt;-discover things, and that people who don't understand the limitations of technology may still use it. &lt;/p&gt;
&lt;p&gt;I am meandering to a point: efforts such as the &lt;a href="https://www.stopkillerrobots.org/"&gt;Campaign to Stop Killer Robots&lt;/a&gt; are good and should be publicised and discussed. Perhaps the &lt;a href="http://www.ucsusa.org/"&gt;Union of Concerned Scientists&lt;/a&gt; should start thinking about 'algorithmic/autonomous threats' (to human lives, livelihoods and the environment). My ideas here are half-formed, which is all the more reason I'd like to see discussions about such issues at similar workshops. It's certainly important that AIs have ethics, but what about the ethics of AI &lt;em&gt;researchers&lt;/em&gt;?&lt;/p&gt;
&lt;h2&gt;Sunday&lt;/h2&gt;
&lt;p&gt;The conference begins in earnest!&lt;/p&gt;
&lt;h3&gt;Steps Toward Robust Artificial Intelligence - Thomas G. Dietterich&lt;/h3&gt;
&lt;p&gt;Quantifying our uncertainty (as probabilistic approaches to AI attempt to do) is about &lt;em&gt;known unknowns&lt;/em&gt;: rather, the thing we know we are uncertain about has to appear somewhere in the model. Dietterich drew attention to &lt;em&gt;unknown unknowns&lt;/em&gt;: things outside the model, perhaps outside our algorithm's model of the environment.&lt;/p&gt;
&lt;p&gt;One way to tackle this is to expand the model: keep adding terms to account for things &lt;a href="https://xkcd.com/793/"&gt;we just thought of&lt;/a&gt;. A risk of this is that these terms may introduce errors if we mismodel them. He suggested that we instead build &lt;em&gt;causal models&lt;/em&gt;, because causal relations are more likely to be robust, require less data and transfer to new situations more easily.&lt;/p&gt;
&lt;p&gt;Regarding new situations: what happens if at 'test' (deployment, perhaps) time, our algorithm encounters something wildly different to what it has seen before? Perhaps instead of allowing it to perform suboptimally (and worse still, to not know it is performing badly), it should recognise this &lt;em&gt;anomaly&lt;/em&gt; and seek assistance. This prompts an open question, "when an agent decides it has entered an anomalous state, what should it do? Is there a general theory of safety?"&lt;/p&gt;
&lt;h3&gt;Session: Learning Preferences and Behaviour&lt;/h3&gt;
&lt;p&gt;I'll not lie: I went to this session because it sounded creepy in a Skynet, Minority Report sort of way.&lt;/p&gt;
&lt;p&gt;My favourite talk of the session was &lt;a href="https://www.aaai.org/Conferences/AAAI/2016/Papers/03Evans12476.pdf"&gt;Learning the Preferences of Ignorant, Inconsistent Agents&lt;/a&gt; - &lt;em&gt;Owain Evans, Andreas Stuhlmueller and Noah D. Goodman&lt;/em&gt;. Roughly, they are concerned with inverse reinforcement learning (IRL) (so learning utility/reward functions) from &lt;em&gt;suboptimal&lt;/em&gt; agents, as humans often might be. A specific case they look at is time inconsistency, which is where agents make plans they later abandon. Seemingly any non-exponential discounting implies time-inconsistency, if my notes are correct. See paper for details. And a related project page: &lt;a href="http://agentmodels.org/"&gt;agentmodels.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I spent the early afternoon finishing up my 'plain English explanation' for the work I was presenting at AAAI, see the page &lt;a href="https://corcra.github.io/bf2/"&gt;here&lt;/a&gt;. I wanted to have something to point my family/friends at when they ask what I work on. Also, making science accessible is good, probably.&lt;/p&gt;
&lt;h3&gt;Session: Word/Phrase Embedding&lt;/h3&gt;
&lt;p&gt;I went to this because I was speaking (briefly) at it. Also, because it is relevant to my interests, so I'll list everything.&lt;/p&gt;
&lt;p&gt;The oral spotlights:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/15Sun11783.pdf"&gt;Inside Out: Two Jointly Predictive Models for Word Representations and Phrase Representations&lt;/a&gt; - &lt;em&gt;Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu and Xueqi Cheng&lt;/em&gt;: Modification of the word2vec-style skip-gram/continuous-bag-of-words model including morphology, project page: &lt;a href="http://ofey.me/projects/InsideOut/"&gt;InsideOut&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/15Wick12464.pdf"&gt;Minimally-Constrained Multilingual Embeddings via Artificial Code-Switching&lt;/a&gt; - &lt;em&gt;Michael Wick, Pallika Kanani and Adam Pocock&lt;/em&gt;: using artificial code-switching to help rapidly create multilingual tools, borrowing information across languages essentially.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/12Derczynski11779.pdf"&gt;Generalised Brown Clustering and Roll-Up Feature Generation&lt;/a&gt; - &lt;em&gt;Leon Derczynski and Sean Chester&lt;/em&gt;: I am shamefully ignorant about Brown clustering, so a lot of this was lost on me. &lt;a href="https://github.com/sean-chester/generalised-brown"&gt;Link to project repository, anyway.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/corcra/bf2/tree/master/media"&gt;&lt;img src="https://raw.githubusercontent.com/corcra/bf2/master/media/aaai_spotlight_p2.png" class="floatr" style="width: 25vw;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The poster spotlights:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/15Zhang12227.pdf"&gt;Building Earth Mover's Distance on Bilingual Word Embeddings for Machine Translation&lt;/a&gt; - &lt;em&gt;Meng Zhang, Yang Liu, Huanbo Luan, Maosong Sun, Tatsuya Izuha and Jie Hao&lt;/em&gt;: I may have spent this spotlight worrying about my spotlight.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/14Hyland12446.pdf"&gt;A Generative Model of Words and Relationships from Multiple Sources&lt;/a&gt; - &lt;em&gt;Stephanie L. Hyland&lt;/em&gt; (that's me) , &lt;em&gt;Theofanis Karaletsos and Gunnar Rätsch&lt;/em&gt;: People seemed to like the slides I made for this spotlight, so I put them in the project repository with some other 'media', see &lt;a href="https://github.com/corcra/bf2/tree/master/media"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/14Goikoetxea11777.pdf"&gt;Single or Multiple? Combining Word Representations Independently Learned from Text and WordNet&lt;/a&gt; - &lt;em&gt;Josu Goikoetxea, Eneko Agirre and Aitor Soroa&lt;/em&gt;: work in a similar vein to mine, in the sense of combining information from 'free text' and 'structured data' (in this case WordNet).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;From Proteins to Robots: Learning to Optimize with Confidence - Andreas Krause&lt;/h3&gt;
&lt;p&gt;Some interesting and important questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how can an AI system autonomously explore while guaranteeing &lt;em&gt;safety?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;how can we do optimised information gathering?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The former question is quite important for 'learning in the wild', and moving beyond the existing (rather successful) paradigm of test/train/validation that we have in machine learning - what happens when the data the algorithm sees depends on actions it takes?&lt;/p&gt;
&lt;p&gt;The latter is quite interesting for cases where we want to probe some nearly-black-box system, but probing is expensive. One can use the framework of &lt;em&gt;Bayesian Optimisation&lt;/em&gt; (Močkus, 1978), and score possible locations (to probe) by their utility in resolving the exploration/exploitation trade-off (via some kind of acquisition function, of which many have been proposed).&lt;/p&gt;
&lt;p&gt;He discussed how one can use Gaussian processes and confidence bounds to help with this, and I'll include a pointer to &lt;a href="http://arxiv.org/abs/0912.3995"&gt;Srinivas et al, 2010.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Some more paper pointers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://colt2008.cs.helsinki.fi/papers/80-Dani.pdf"&gt;Stochastic Linear Optimization under Bandit Feedback&lt;/a&gt; - Varsha Dani, Thomas P. Hayes, Sham M. Kakade&lt;/li&gt;
&lt;li&gt;&lt;a href="https://las.inf.ethz.ch/files/gotovos13active-long.pdf"&gt;Active Learning for Level Set Estimation&lt;/a&gt; - Alkis Gotovos, Nathalie Casati, Gregory Hitz, Andreas Krause&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmlr.org/proceedings/papers/v37/sui15.pdf"&gt;Safe Exploration for Optimization with Gaussian Processes&lt;/a&gt; - Yanan Sui, Alkis Gotovos, Joel Burdick, Andreas Krause &lt;/li&gt;
&lt;li&gt;&lt;a href="https://las.inf.ethz.ch/files/krause11contextual.pdf"&gt;Contextual Gaussian Process Bandit Optimization&lt;/a&gt; - Andreas Krause, Cheng Soon Ong&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(I am quite fond of Gaussian processes, in case that wasn't already obvious.)&lt;/p&gt;
&lt;p&gt;The conclusions were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;feedback loops abound in modern ML applications  &lt;/li&gt;
&lt;li&gt;exploration is central but also delicate, and safety is crucial  &lt;/li&gt;
&lt;li&gt;statistical confidence bounds allow navigating exploration-exploitation tradeoffs in a principled manner&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Poster Session 1&lt;/h3&gt;
&lt;p&gt;I was presenting at this session (see my poster &lt;a href="https://github.com/corcra/bf2/blob/master/media/aaai_poster.pdf"&gt;here&lt;/a&gt;), so I didn't get to look at anything else. I struggled to eat bean tacos one-handed, and I talked a &lt;em&gt;lot&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Monday&lt;/h2&gt;
&lt;h3&gt;Learning Treatment Policies in Mobile Health - Susan Murphy&lt;/h3&gt;
&lt;p&gt;I have Susan Murphy's paper &lt;a href="http://dept.stat.lsa.umich.edu/~samurphy/papers/optimal.pdf"&gt;Optimal dynamic treatment regimes&lt;/a&gt; on my desk &lt;em&gt;as I write this&lt;/em&gt;, so I was pretty excited to see her speaking. And on mHealth, too! Double excitement.&lt;/p&gt;
&lt;p&gt;It turns out that she is also involved in the &lt;a href="http://www.psc.isr.umich.edu/research/project-detail/36366"&gt;Heart Steps&lt;/a&gt; project with Ambuj Tewari, which I wrote about a little in my &lt;a href="https://corcra.github.io/ml/2015/12/14/NIPS2015.html"&gt;NIPS post&lt;/a&gt;, so I'm not going to repeat myself.&lt;/p&gt;
&lt;p&gt;The 'treatment optimisation' aspects of mHealth are interesting because it gets into the realm of HCI and psychology. You want to send the patient reminders to do a thing, but you don't want them to become habituated and ignore them, or irritated, or distracted. She mentioned the need to avoid pointlessly reminding the patient to go for a walk while they're &lt;em&gt;already&lt;/em&gt; walking, or dangerously alerting them while they're driving. I find it uncomfortable to be reminded that my phone knows when I'm walking/driving, but if the information is being recorded &lt;em&gt;anyway&lt;/em&gt;, you might as well use it, right? Insert something about dragnets here.&lt;/p&gt;
&lt;p&gt;But really, mHealth provides some very exciting opportunities to do reinforcement learning. She mentioned non-stationarity as a general challenge, and suggested one could perhaps do transfer learning within a user to tackle it.&lt;/p&gt;
&lt;h3&gt;Session: Active Learning&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/17Wray12000.pdf"&gt;A POMDP Formulation of Proactive Learning&lt;/a&gt; &lt;em&gt;(Kyle Hollins Wray and Shlomo Zilberstein)&lt;/em&gt; was interesting. The idea is that the agent must decide which &lt;em&gt;oracle&lt;/em&gt; to query to label a particular data point, where the underlying state is the &lt;em&gt;correctness&lt;/em&gt; of the current set of labels. I'm not familiar enough with the active learning field to say if this formulation is especially novel, but I liked it, possibly because I like POMDPs.&lt;/p&gt;
&lt;h3&gt;Session: Privacy&lt;/h3&gt;
&lt;p&gt;I experimented with taking no notes during this session to see how it would influence my recall of the material. The trade-off here is that taking notes is a little distracting for me (as well as providing many opportunities to notice Slack/email/etc.), but does provide a lasting record.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/10CuencaGrau12253.pdf"&gt;Logical Foundations of Privacy-Preserving Publishing of Linked Data&lt;/a&gt; &lt;em&gt;(Bernardo Cuenca Grau and Egor V. Kostylev)&lt;/em&gt; was strangely fascinating. They were talking about anonymisations of RDF graphs (a data type I'd been working with for my word embedding work). I'm also quite interested in information linkage (see e.g. &lt;a href="https://corcra.github.io/sec/2015/10/27/radical-networks.html"&gt;my talk at Radical Networks 2015&lt;/a&gt;), so this was up my alley.&lt;/p&gt;
&lt;p&gt;Not sure how the experiment worked out, further data required.&lt;/p&gt;
&lt;h3&gt;Session: Cognitive Systems&lt;/h3&gt;
&lt;p&gt;I was &lt;em&gt;heavily&lt;/em&gt; overbooked for this time-slot: I wanted to see Deep Learning 1, Discourse and Question Answering (NLP 6), the RSS talks (for my friend &lt;a href="https://cs.stanford.edu/people/asaxena/papers/sener_saxena_rCRF_rss15.pdf"&gt;Ozan's talk&lt;/a&gt;), Cognitive Systems (largely for Kazjon's talk - see below), and Machine Learning/Data Mining in Healthcare. Time turners have yet to be invented, unfortunately.&lt;/p&gt;
&lt;p&gt;One of the recurring themes of my AAAI v. NIPS pronouncements was that AAAI has, well... more &lt;strong&gt;AI&lt;/strong&gt; stuff. This session was probably the closest I got to that (unless you count the AI and Ethics workshop: I'd consider it meta-AI). People were doing reasoning &lt;em&gt;without&lt;/em&gt; probability distributions, using first order logic! One of the presentations included &lt;a href="https://www.youtube.com/watch?v=n9TWwG4SFWQ"&gt;this video&lt;/a&gt; which I found strangely distressing (to me it is - spoilers! - clearly about domestic abuse).&lt;/p&gt;
&lt;p&gt;The talk I had come to see, &lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/22Grace12456.pdf"&gt;Surprise-Triggered Reformulation of Design Goals&lt;/a&gt; &lt;em&gt;(Kazjon Grace and Mary Lou Maher)&lt;/em&gt;, along with numerous chats with Kazjon throughout the conference made me realise that computational creativity is a &lt;em&gt;thing&lt;/em&gt;. OK, full disclosure: I am loosely involved with some &lt;a href="https://twitter.com/hashtag/botally"&gt;generative art folks&lt;/a&gt; so I did sort of know this, but it hadn't occurred to me that one might use machine learning to represent or understand mental processes surrounding creativity. Neat! The idea here is that the way humans design things is iterative: you have some loosely-formed idea, and through the process of realising it, notice things you hadn't expected (experience &lt;em&gt;surprise&lt;/em&gt;, as it were), and modify your idea accordingly. So there is interplay between the internal representation (perhaps this is the design goal) and the external representation (the realisation). So they're interested in understanding &lt;em&gt;surprises&lt;/em&gt;: perhaps an element of a design is &lt;em&gt;unusual&lt;/em&gt; given other elements of the design, for example. I am going to have to actually read the paper before I elaborate any further on this, but the experiments involved generating weird (but edible) recipes so I'm looking forward to it.&lt;/p&gt;
&lt;p&gt;Very deep question raised by all of this: "can computers be creative?"&lt;br&gt;
Related: what is creativity? What is art? What are computers?&lt;/p&gt;
&lt;h3&gt;AI's Impact on Labor Markets - Nick Bostrom, Erik Brynjolfsoon, Oren Etzioni, Moshe Vardi&lt;/h3&gt;
&lt;p&gt;I managed to take no notes during this panel (my notes from AAAI actually dry up around here, I hit peak exasperation with keeping my devices charged).&lt;/p&gt;
&lt;p&gt;I have a lot of feelings about AI and labour, but I'm first going to direct attention to the Panel on Near-term Issues from the &lt;a href="https://corcra.github.io/ml/2015/12/14/NIPS2015.html"&gt;NIPS Algorithms Among Us Symposium&lt;/a&gt;, which had a similar lineup. &lt;/p&gt;
&lt;p&gt;Ultimately, it is hard to solve social and political issues using technology alone, especially if those issues arise as a result of the technology itself. I'd love to automate away all the mind-numbingly boring and unfulfilling jobs humans currently do, but I don't want to remove anyone's livelihood in the process. I don't think it's sufficient to say that society will 'figure it out somehow', especially in countries such as the USA where there is so little protection from poverty and homelessness. That said, I don't know what the solution is (except for some rather radical ideas with limited empirical support for their efficacy), and I don't know if it will, or should, come from the AI research community.&lt;/p&gt;
&lt;h3&gt;Poster Session 2&lt;/h3&gt;
&lt;p&gt;I got slightly side-tracked by ranting about how broken academic publishing is. Shoutout to the &lt;a href="https://www.mozillascience.org/contributorship-badges-a-new-project"&gt;Mozilla Contributorship Badges project&lt;/a&gt; for trying to deal with the credit-assignment problem, for one.&lt;/p&gt;
&lt;h2&gt;Tuesday&lt;/h2&gt;
&lt;h3&gt;Towards Artificial General Intelligence - Demis Hassabis&lt;/h3&gt;
&lt;p&gt;Google DeepMind are arguably the machine learning success story of the last year, given their &lt;a href="www.nature.com/nature/journal/v518/n7540/full/nature14236.html"&gt;Atari Nature paper&lt;/a&gt; and &lt;a href="http://deepmind.com/alpha-go.html"&gt;AlphaGo&lt;/a&gt; result (although the match against Lee Sedol in March will be more interesting). I'm very happy to see computer games featuring so prominently for evaluating and developing AGI: so much that I spent the session after this talk sketching out a project involving &lt;a href="https://en.wikipedia.org/wiki/Dota_2"&gt;Dota 2&lt;/a&gt;, which I think could be a very interesting application of deep reinforcement learning, if only the metagame would stabilise long enough to allow for acquisition of sufficient training data!&lt;/p&gt;
&lt;p&gt;Anyway, this talk mostly convinced me that DeepMind are doing cool stuff, which I imagine was the intended effect. Hassabis was coming from a pleasantly principled place. They do seem genuinely interested in AGI, rather than for example, beating benchmarks with &lt;em&gt;yet deeper&lt;/em&gt; networks. I don't mean to imply that beating benchmarks isn't important, but I think the types of discoveries one makes in the pursuit of larger/more abstract goals are quite important for the intellectual development of a field which can easily become dominated by engineering successes. So yes, the talk had the intended effect.&lt;/p&gt;
&lt;h3&gt;Session: Reinforcement Learning I&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://twitter.com/__hylandSL/status/699716539147137024"&gt;&lt;img src="/images/aaai_GIRL.png" class="floatl" style="width: 25vw;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/19Burchfiel11917.pdf"&gt;Distance Minimization for Reward Learning from Scored Trajectories&lt;/a&gt; - &lt;em&gt;Benjamin Burchfiel, Carlo Tomasi and Ronald Parr&lt;/em&gt;: this is about IRL with suboptimal experts (a popular and interesting topic). In this case, the 'demonstrator' need not be an expert but can operate as a judge, assigning scores to demonstrators. The real-world example would be of a sports coach who's no longer capable of creating expert trajectories (that is, demonstrating optimally) but who can still accurately &lt;em&gt;rate&lt;/em&gt; demonstrations from others, if they're available. They also study the case where the judge's scores are noisy and find the algorithm robust.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/12Pirotta12482.pdf"&gt;Inverse Reinforcement Learning through Policy Gradient Minimization&lt;/a&gt; - &lt;em&gt;Matteo Pirotta and Marcello Restelli&lt;/em&gt;: more IRL through parametrising the expert's reward function, but here it is no longer necessarily to repeatedly compute optimal policies, so this should be quite efficient. Also, this algorithm is called GIRL.&lt;/p&gt;
&lt;h3&gt;Poster Session 3&lt;/h3&gt;
&lt;p&gt;Some interesting posters (highly non-exhaustive list, but I'm exhausted):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/01Luo11843.pdf"&gt;Predicting ICU Mortality Risk by Grouping Temporal Trends from a Multivariate Panel of Physiologic Measurements&lt;/a&gt; - &lt;em&gt;Yuan Luo, Yu Xin, Rohit Joshi, Leo Celi and Peter Szolovits&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/12Masson11981.pdf"&gt;Reinforcement Learning with Parameterized Actions&lt;/a&gt; - &lt;em&gt;Warwick Masson, Pravesh Ranchod and George Konidaris&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/15Mueller12195.pdf"&gt;Siamese Recurrent Architectures for Learning Sentence Similarity&lt;/a&gt; - &lt;em&gt;Jonas Mueller and Aditya Thyagarajan&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Wednesday&lt;/h2&gt;
&lt;h3&gt;Sessions: Reinforcement Learning II &amp;amp; III&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/12Wirth12247.pdf"&gt;Model-Free Preference-Based Reinforcement Learning&lt;/a&gt; - &lt;em&gt;Christian Wirth, Johannes Fürnkranz and Gerhard Neumann&lt;/em&gt;: I didn't actually see this talk, but the paper has a number of interesting words in its title, so it must be good.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/12Bellemare12428.pdf"&gt;Increasing the Action Gap: New Operators for Reinforcement Learning&lt;/a&gt; - &lt;em&gt;Marc G. Bellemare, Georg Ostrovski, Arthur Guez, Philip S. Thomas and Remi Munos&lt;/em&gt;: this was a good talk. Basically, during value iteration one applies the Bellman operator to the state-action value function (Q-function). The fixed point of the operator is the optimal Q-function, which induces (greedily) the optional policy. They argue that this operator is &lt;em&gt;inconsistent&lt;/em&gt;, in that it suggests &lt;em&gt;nonstationary&lt;/em&gt; policies. They resolve this by definining a 'consistent Bellman operator' which preserves local stationarity and show that it increases the &lt;em&gt;action gap&lt;/em&gt; (the value difference between the best and second best actions). The action gap is relevant because it can allow for selecting the same (optimal) action even when estimates of the value function are noisy. And a link to the &lt;a href="http://www.arcadelearningenvironment.org/"&gt;Arcade Learning Environment&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.instagram.com/p/BBqx2fHDQM6/?taken-by=c0rcr4"&gt;&lt;img src="/images/aaai2016_phoenix.jpg" class="floatr" style="width: 25vw;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/12vanHasselt12389.pdf"&gt;Deep Reinforcement Learning with Double Q-Learning&lt;/a&gt; - &lt;em&gt;Hado van Hasselt, Arthur Guez and David Silver&lt;/em&gt;: more from DeepMind. I swear I am not a DeepMind fangirl. Setup here: Q-learning can result in overestimates for some action values. Using DQN (deep Q-learning algorithm) they find that this happens often and impacts performance. They solve the problem by showing how to generalise Double Q-learning to arbitrary function approximation (rather than just tabular Q functions). So this paper seems like a natural progression for Double Q-learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Exploration-exploitation trade-offs are everywhere. At this stage in my career, I consider going to conferences a largely exploratory activity: I can learn a little (or more) about a lot of things and get an idea for the kinds of research going on. For the people who spend conferences meeting with their collaborators, it's more about exploitation. &lt;em&gt;(For the appropriate interpretation of that word.)&lt;/em&gt; I am a little fatigued of exploration right now - I'm still processing things I saw at NIPS, so I was not well positioned to make the most out of AAAI. I kept wanting to run off and write code in a corner, but who does that? Well, I do that. I do that right now.&lt;/p&gt;</content><category term="phoenix"></category><category term="conference"></category><category term="aaai"></category><category term="ethics"></category></entry><entry><title>linking between zotero and evernote (in OSX)</title><link href="/tips/2015-12-18-zotero-and-evernote.html" rel="alternate"></link><published>2015-12-18T00:00:00+00:00</published><updated>2015-12-18T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2015-12-18:/tips/2015-12-18-zotero-and-evernote.html</id><summary type="html">&lt;p&gt;I'm upgrading my paper-management workflow from 'labyrinth of folders' to an
&lt;a href="https://evernote.com/"&gt;Evernote&lt;/a&gt; + &lt;a href="https://www.zotero.org/"&gt;Zotero&lt;/a&gt; mix. I
already use Evernote a little for this by writing paper summaries in
it, but I would rather do the 'heavy duty' management and organisation in
Zotero. So, I'd like to easily switch between the Evernote …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm upgrading my paper-management workflow from 'labyrinth of folders' to an
&lt;a href="https://evernote.com/"&gt;Evernote&lt;/a&gt; + &lt;a href="https://www.zotero.org/"&gt;Zotero&lt;/a&gt; mix. I
already use Evernote a little for this by writing paper summaries in
it, but I would rather do the 'heavy duty' management and organisation in
Zotero. So, I'd like to easily switch between the Evernote note about a paper and
the Zotero reference on it. I need:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Zotero → Evernote  &lt;/li&gt;
&lt;li&gt;Evernote → Zotero&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Note: This is all with OSX 10.11.1 (El Capitan), Evernote 6.3, and Zotero 4.0.28.10. YMMV.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Zotero to Evernote&lt;/h2&gt;
&lt;p&gt;My solution here is to add an attachment to the reference which is a link to 
the Evernote URI. Zotero gracefully handles this, you just right-click on the
reference and Add Attachment → Attach Link to URI...&lt;/p&gt;
&lt;p&gt;The hard part is then &lt;em&gt;getting&lt;/em&gt; the URI. In (my currently-up-to-date version of)
Evernote, the Copy Note Link gives you a HTTP(S) link:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;https://www.evernote.com/shard/asdfgh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;em&gt;This converts to the Evernote URI link if you paste it &lt;strong&gt;inside&lt;/strong&gt; Evernote,
which is pretty cool I guess but also not useful here.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I use the Evernote client and I don't want this, I want the URI like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;evernote:///view/adfgh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is (for now, probably) known as &lt;strong&gt;Classic&lt;/strong&gt; Note Link in Evernote, and you
get it by holding down &lt;code&gt;Alt&lt;/code&gt; on the menu after right-clicking on the note.
Bizarre and annoying, but whatever. It works.&lt;/p&gt;
&lt;p&gt;Now I can just double-click on the Evernote URI attachment on my reference in
Zotero and it'll open the note (in my Evernote client) with my notes on it.&lt;/p&gt;
&lt;h2&gt;Evernote to Zotero&lt;/h2&gt;
&lt;p&gt;Now, we want to get the Zotero URI (to the reference) and include it in an
Evernote note. The 'normal' URI you'd get from Zotero (using Item URI as your
Default Output Format under Preferences → Export → Quick Copy) is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;http://zotero.org/users/local/asdfgh/items/asdfgh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;What I really want is the non-HTTP URI, e.g.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;zotero://select/items/asdfgh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;em&gt;However&lt;/em&gt;, if you paste that into Evernote it doesn't recognise it as a URI
or anything that should be linkish. It just sits there, flat and idle and useless. &lt;em&gt;This is pretty annoying given it can
do this for Evernote URIs, as above, but whatever...&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Solution Sketch&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;This solution is from &lt;code&gt;brain.flush()&lt;/code&gt;; &lt;a href="http://blog.cdhq.de/archive/1366"&gt;
Connecting Zotero and Evernote&lt;/a&gt; and &lt;a href="http://blog.cdhq.de/archive/1396"&gt;RTF-Links from Zotero in Evernote&lt;/a&gt;. This post isn't just a link to that blog because I fear link rot.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The workaround is pretty gross but bear with me. First, you modify Zotero
so that its Quick Copy gives you a HTML-formatted URI, e.g.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;a&lt;/span&gt; &lt;span class="na"&gt;href=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;zotero://select/items/asdfgh&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;Paper Title&lt;span class="nt"&gt;&amp;lt;/a&amp;gt;&amp;lt;br&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;then convert the HTML in your clipboard to RTF, which can be pasted into Evernote and will act 'as desired'. The intermediate RTF representation looks something like this, btw, so clearly there's scope for further customisation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf130  
{\fonttbl\f0\froman\fcharset0 Times-Roman;}  
{\colortbl;\red255\green255\blue255;\red0\green0\blue233;\red0\green0\blue0;}  
\deftab720  
\pard\pardeftab720\sl280\partightenfactor0  
{\field{\*\fldinst{HYPERLINK &amp;quot;zotero://select/items/asdfgh&amp;quot;}}{\fldrslt  
\f0\fs24 \cf2 \expnd0\expndtw0\kerning0  
\ul \ulc2 \outl0\strokewidth0 \strokec2 Paper Title}}  
\f0\fs24 \cf3 \expnd0\expndtw0\kerning0  
\outl0\strokewidth0 \strokec3 \  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ew. But you never need to look at this.&lt;/p&gt;
&lt;h3&gt;Specific Steps&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Modifying Quick Copy:&lt;/li&gt;
&lt;li&gt;Put &lt;a href="https://gist.github.com/ColdDevil/9145021"&gt;this script&lt;/a&gt; in your &lt;code&gt;translators&lt;/code&gt; folder wherever your Zotero is.&lt;/li&gt;
&lt;li&gt;Tell Zotero to use it for Quick Copy: Preferences → Export → Default Output Format: select &lt;code&gt;ZotSelect Link (HTML)&lt;/code&gt; from the dropdown.&lt;/li&gt;
&lt;li&gt;Now &lt;code&gt;Cmd + Shift + C&lt;/code&gt; will put the HTML-formatted link in your clipboard.&lt;/li&gt;
&lt;li&gt;Converting HTML to RTF:&lt;/li&gt;
&lt;li&gt;The script to do the transformation is (choose your favourite &lt;code&gt;UTF-8&lt;/code&gt; region...)
  &lt;code&gt;export LANG=en_US.UTF-8; pbpaste | textutil -stdin -stdout -format html -convert rtf -inputencoding utf-8 | pbcopy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;You can automate this with &lt;a href="https://en.wikipedia.org/wiki/Automator_%28software%29"&gt;Automator&lt;/a&gt;, (OSX tool):&lt;ul&gt;
&lt;li&gt;Create a Service, add an action of &lt;code&gt;Run Shell Script&lt;/code&gt;, paste in above code.&lt;/li&gt;
&lt;li&gt;Make sure set the "Service receives selected..." dropdown to "no input".&lt;/li&gt;
&lt;li&gt;Call the service whatever you want - following the source blog I called mine "Convert HTML clipboard to RTF".&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Set a keyboard shortcut for the new service under Keyboard → Shortcuts → Services. Also following the blog, I used &lt;code&gt;Cmd + Shift + Ctrl + C&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So now the workflow is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[in Zotero] &lt;code&gt;Cmd + Shift + C&lt;/code&gt; on desired reference  &lt;/li&gt;
&lt;li&gt;[wherever] &lt;code&gt;Cmd + Shift + Ctrl + C&lt;/code&gt; to transform contents of clipboard     &lt;/li&gt;
&lt;li&gt;[in Evernote] paste normally to get a link which opens the reference in Zotero.&lt;/li&gt;
&lt;/ul&gt;</content><category term="zotero"></category><category term="evernote"></category><category term="osx"></category><category term="citations"></category></entry><entry><title>NIPS 2015 by the day</title><link href="/ml/2015-12-14-nips2015.html" rel="alternate"></link><published>2015-12-14T00:00:00+00:00</published><updated>2015-12-14T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2015-12-14:/ml/2015-12-14-nips2015.html</id><summary type="html">&lt;p&gt;I got back from Montreal yesterday. I was at the &lt;a href="https://nips.cc/"&gt;Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)&lt;/a&gt; - a rather large gathering of people interested in machine learning, neuroscience, artificial intelligence, and related topics. It's an academic conference, and it is intense. Many wonderful conversations were had, things learned …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I got back from Montreal yesterday. I was at the &lt;a href="https://nips.cc/"&gt;Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)&lt;/a&gt; - a rather large gathering of people interested in machine learning, neuroscience, artificial intelligence, and related topics. It's an academic conference, and it is intense. Many wonderful conversations were had, things learned, insights gained, ideas developed, coffees consumed. Old friends met and new friends made. I left physically exhausted, and this post is an attempt to summarise &lt;em&gt;some&lt;/em&gt; of what went down. This was also my first time attending NIPS, so next time I might be a &lt;em&gt;little&lt;/em&gt; more conservative with my energy.&lt;/p&gt;
&lt;p&gt;If it seems like my level of detail varies wildly, it's because sometimes I took
notes, sometimes I couldn't, and sometimes I didn't want to.&lt;/p&gt;
&lt;h2&gt;Sunday&lt;/h2&gt;
&lt;p&gt;&lt;img src="/images/nips2015_01.png" class="floatr"&gt;&lt;/p&gt;
&lt;p&gt;When I flew from Dublin to Hamburg for &lt;a href="https://events.ccc.de/category/31c3/"&gt;31C3&lt;/a&gt;
last year, the plane was full of vaguely unusual-looking people (myself
included, no doubt) clearly destined for Congress. Who else would fly to Hamburg 
on St. Stephen's day? The flight from NYC to Montreal for NIPS was a little less 
homogeneous, and machine learners are harder to spot (posters are strong 
evidence), but I nonetheless had the same vague feeling of unified purpose with 
my co-passengers. Conversation about optimisation broke out on the bus to the
city centre, and knowing glances were exchanged between strangers. And so NIPS 
began as it would continue, a bubble where the social convention of silence is
broken by mutual knowledge of shared purpose (this purpose being bringing about the robot apocalypse).&lt;/p&gt;
&lt;p&gt;Tip: don't try to register the day the conference starts. Angry Monday
morning tweets mentioned waiting times some multiples of how long I spent on
Sunday evening. ¯\_(ツ)_/¯&lt;/p&gt;
&lt;h2&gt;Monday&lt;/h2&gt;
&lt;p&gt;Because I forgot to register for the &lt;a href="http://wimlworkshop.org/"&gt;Women in Machine Learning workshop&lt;/a&gt;,
I went to tutorials.&lt;/p&gt;
&lt;h3&gt;Deep Learning: Yoshua Bengio &amp;amp; Yann LeCun&lt;/h3&gt;
&lt;p&gt;Topics mentioned were: curse-of-dimensionality, backpropagation, convolutional nets, 
recurrent nets, details about backprop (e.g. with ReLUs and max pooling,
GPUs), distributed training (e.g. asynchronous stochastic gradient descent), 
applications (eg. vision, more about vision, speech, natural language),
attention mechanisms, encoder/decoder networks (e.g. for machine translation),
multi-task learning, unsupervised learning, undirected graphical models,
more about auto-encoders (e.g. probabilistic interpretation, helmholtz
machines), semi-supervised learning (e.g. ladder networks), and some challenges
and open problems. The future questions/areas of interest highlighted were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;unsupervised learning, and how to evaluate it?  &lt;/li&gt;
&lt;li&gt;how to include long-term dependencies?  &lt;/li&gt;
&lt;li&gt;NLP, generally  &lt;/li&gt;
&lt;li&gt;optimisation  &lt;/li&gt;
&lt;li&gt;distributed training?  &lt;/li&gt;
&lt;li&gt;bridging the gap to biology  &lt;/li&gt;
&lt;li&gt;deep reinforcement learning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall, I wanted to see more gorey  details, but &lt;em&gt;deeper&lt;/em&gt; coverage of any one 
topic would have limited breadth, so it was more like a lit review/series of
pointers to publications in this field. There was criticism that it placed too 
much attention on the work of its presenters (also Hinton, who was meant to be 
there but couldn't make it unfortunately), and gave an incomplete treatment of 
the history of the field. I'm not in a position to comment intelligently on
that. Anyone giving an overview-style talk has a responsibility to adequately 
cover both history and breadth of research, so I can see why it might have been
made.&lt;/p&gt;
&lt;h3&gt;Probabilistic Programming: Frank Wood&lt;/h3&gt;
&lt;p&gt;I had already heard some of this content at MLSS 2015 Tübingen so didn't take
notes. Check out &lt;a href="https://bitbucket.org/probprog/mlss2015"&gt;this repo&lt;/a&gt; for the 
material from the practicals on Anglican. TL;DR:&lt;/p&gt;
&lt;blockquote&gt;&lt;p lang="en" dir="ltr"&gt;Goals of probabilistic programming: reduce coding burden, commodify inference, create weird new models, make it widely usable &lt;a href="https://twitter.com/hashtag/NIPS2015?src=hash"&gt;#NIPS2015&lt;/a&gt;&lt;/p&gt;&amp;mdash; Stephanie Hyland (@__hylandSL) &lt;a href="https://twitter.com/__hylandSL/status/673929524958990336"&gt;December 7, 2015&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;h3&gt;Introduction to Reinforcement Learning with Function Approximation: Rich Sutton&lt;/h3&gt;
&lt;p&gt;&lt;img src="/images/nips2015_02.png" class="floatl" style="width: 35vw;"&gt;&lt;/p&gt;
&lt;p&gt;I took &lt;em&gt;physical notes&lt;/em&gt; for this tutorial, because there was a severe lack
of power outlets in the convention centre. Tip: sometimes it's better
to have a notebook with a long battery life than a retina screen.&lt;/p&gt;
&lt;p&gt;I think reinforcement learning is really cool (and according to how popular
the deep RL workshop was, so do other people (or maybe they just like 'deep')).&lt;/p&gt;
&lt;p&gt;This tutorial was much more focused than deep learning: it was concerned with
policy-learning through first getting an action-value function. This function
gives you the expected reward (usually with discounting) upon taking a
particular action from a particular state, and can therefore be used to define a
policy (e.g., greedily, given your state choose the action with highest value). &lt;/p&gt;
&lt;p&gt;He spoke about on- and off-policy learning, where the agent is obtaining
information for its estimate of the action-value function while either following
the policy given by such a function (on-policy) or some other policy (off-policy),
such as a random policy. I hadn't properly appreciated the significance of this
difference before, so I found the exposition illuminating. He gave an example
where on-policy learning resulted in the highest average reward across 
episodes, but its learned policy was worse than that of an off-policy learner, since
the off-policy learner was able to explore 'riskier' actions. My intuition here
is that this result could be altered by tweaking rewards and the inclination 
towards exploration in the 'off' policy, and I'm sure there is loads of 
(ancient) work already done on the topic. More papers to read, eh.&lt;/p&gt;
&lt;p&gt;Another neat thing about off-policy learning is that you can gather information
about many potential policies simultaneously. This might seem 'trivially obvious'
(exploration leads to information about the system and its rewards which enables
policy-learning) but it is always reassuring to hear one's intuitions restated 
by an expert in the field.&lt;/p&gt;
&lt;p&gt;Overall this was the most lecture-like of the tutorials and hopefully it will
appear online soon, because it was well-paced, well-motivated and overall the
most useful, even if it wasn't all-encompassing for reinforcement learning (it 
wasn't trying to be). Sutton is a good educator.&lt;/p&gt;
&lt;h3&gt;Poster Session 1&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;I did this session 'wrong'. I tried navigating through an impassable crowd of
humans and coats and bags to peer at each poster and then decide if I wanted to
hear more. Tip: do not do this. For everyone's sake.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5717-taming-the-wild-a-unified-analysis-of-hogwild-style-algorithms"&gt;Taming the Wild: A Unified Analysis of Hogwild-Style Algorithms&lt;/a&gt;- &lt;em&gt;Christopher M. De Sa, Ce Zhang, Kunle Olukotun, Christopher Ré&lt;/em&gt; - I need more asynchronous
SGD in my life. They look at the noise you get from asynchronous updates,
derive some results and describe a lower-precision SGD algorithm. I am disproportionately likely to pay attention to posters/papers with cool titles.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.princeton.edu/~ms44/nips2015/"&gt;A Theory of Decision Making Under Dynamic Context&lt;/a&gt; - &lt;em&gt;Michael Shvartsman, Vaibhav Srivastava, Jonathan D. Cohen&lt;/em&gt; -  Neuroscience! Decision making! I have apparently forgotten the main message of this poster, possibly because we rapidly started talking about psycholinguistics. The danger of NIPS is exhaustion through too many interesting conversations. Added bonus for this poster: he made it + code + paper available (see link).&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language"&gt;Grammar as a Foreign Language&lt;/a&gt; - &lt;em&gt;Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton&lt;/em&gt; - by the time I made it to this poster the session was over so I didn't get to speak to any of the authors. Main idea: parsing with LSTM + attention! The 'foreign language' part comes in because it's sequence-to-sequence (sentence to linearised parse tree), which is typically found in machine translation settings.&lt;/p&gt;
&lt;h2&gt;Tuesday&lt;/h2&gt;
&lt;p&gt;I was unreasonably tired and questioned the wisdom of staying at a poster session until after midnight. &lt;a href="http://mlg.eng.cam.ac.uk/zoubin/"&gt;Zoubin Ghahramani&lt;/a&gt; spoke about &lt;strong&gt;Probabilistic Machine Learning&lt;/strong&gt; while I ate pastries with a fork in the overflow room. The overflow room would have been perfect if it had any power outlets in it. I've heard some variant of Zoubin's talk roughly twice already, thanks to &lt;a href="http://mlss.tuebingen.mpg.de/"&gt;MLSS 2015&lt;/a&gt; and &lt;a href="http://gpss.cc/"&gt;GPSS 2014&lt;/a&gt;, so I lost focus and probably missed something new and important. He mentioned probabilistic programming and the &lt;a href="http://www.automaticstatistician.com/index/"&gt;automatic statistician&lt;/a&gt;. One of the questions was about whether this (the automatic statistician) will replace machine learners : a terrible thought, and ironic for a discipline which (to some extent) aims to automate away many other jobs. The answer was (as you might expect, may have even given yourself); 'this will just make our jobs easier, allowing us to focus on more interesting problems'.&lt;/p&gt;
&lt;p&gt;The talk after Zoubin was rather technical and about singular value decomposition. I missed some critical thread of understanding at the start (see missing focus) and sort of give up following, although I note that the speaker was just &lt;em&gt;quite good&lt;/em&gt;, even if the topic is not directly relevant to me.&lt;/p&gt;
&lt;p&gt;Spotlights as a concept are interesting, and their intended purpose is a little unclear to me. If the poster is personally relevant and interesting, I will (possibly) already know about it and go to it. If it's not relevant, a three-minute summary is &lt;em&gt;unlikely&lt;/em&gt; to change my mind. The intended benefits I could imagine (for the presenters) are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;convincing other members of your field that your poster is interesting/worthwhile&lt;/li&gt;
&lt;li&gt;convincing people from outside your area that your poster is relevant  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, each of these necessitates a very different three-minute presentation (detailed versus high-level), and it's hard to say what the presenters went with. Further possible benefit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;be able to state that one's poster was selected for highlight  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In which case, the audience need not pay any attention. This is another way of saying that while I did listen, most of the spotlighted posters didn't make it into my cut for later. However, the final benefit (for the audience) was appreciated:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;exposure to entirely different sub-field and its different priorities and problems  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I spent much of the afternoon charging my laptop in a corner of the convention centre and doing some work, so there are some &lt;em&gt;deleted scenes&lt;/em&gt; from the conference here. I got an especially foul latte and suffered it for too long. What sort of monster uses artificial sweetener without asking first?&lt;/p&gt;
&lt;h3&gt;Poster Session 2:&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;New poster session policy: consult conference book, select posters, target.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1506.05751"&gt;Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks&lt;/a&gt; - &lt;em&gt;Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus&lt;/em&gt; - 
Emily Denton was part of the way into an explanation of adversarial networks when I arrived at this poster. I feel like I've heard a lot about them in recent days, but it's probably just the &lt;a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases#Frequency_illusion"&gt;Baader-Meinhof phenomenon&lt;/a&gt;. I like the idea, although I feel like there's probably a way to show that the procedure is equivalent to some other contrastive objective or falls out naturally from an appropriate model choice, but these idle thoughts are better substantiated later/elsewhere/in prior art.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences"&gt;Expressing an Image Stream with a Sequence of Natural Sentences&lt;/a&gt; - &lt;em&gt;Cesc C. Park, Gunhee Kim&lt;/em&gt; - fairly complicated deep network architecture, the idea is to create a reasonable-looking set of sentences to describe a sequence of images. Training data is blog posts containing pictures, assumed related (they break it into image/text-block segments). Some possibly-interesting pre-processing on the text data (I am biased to find text more interesting than images!), too.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.eecs.berkeley.edu/~igor.mordatch/policy/index.html"&gt;Interactive Control of Diverse Complex Characters with Neural Networks&lt;/a&gt; -&lt;em&gt;Igor Mordatch, Kendall Lowrey, Galen Andrew, Zoran Popović, Emanuel Todorov&lt;/em&gt; - using a recurrent neural network to learn the dynamics under a control policy; seemingly mapping from the state to the velocities (dynamics) caused by an action.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5792-efficient-learning-of-continuous-time-hidden-markov-models-for-disease-progression"&gt;Efficient Learning of Continuous-Time Hidden Markov Models for Disease Progression&lt;/a&gt; - &lt;em&gt;Yu-Ying Liu, Shuang Li, Fuxin Li, Le Song, James M. Rehg&lt;/em&gt; - a medically-focused paper! The advance seems to be making continuous-time HMMs more feasible. How much? I'm not sure, I didn't stay too long.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5751-asynchronous-parallel-stochastic-gradient-for-nonconvex-optimization"&gt;Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization&lt;/a&gt; - &lt;em&gt;Xiangru Lian, Yijun Huang, Yuncheng Li, Ji Liu&lt;/em&gt; - this is a theory paper about asynchronous SGD. I was initially confused because I didn't know the state of the theory here, and wasn't sure what their actual contribution was. The contribution is about the convergence rate. The take-home for me is roughly 'you can use asynchronous SGD'. See also the poster from Monday on Hogwild!.&lt;/p&gt;
&lt;h2&gt;Wednesday&lt;/h2&gt;
&lt;p&gt;I missed Tibhshirani's talk, &lt;em&gt;Post-selection Inference for Forward Stepwise Rregression, Lasso and other Adaptive Statistical Procedures&lt;/em&gt;. This was unfortunate given the topic of lunchtime discussion was adaptive statistical procedures (among other things). Being interdisciplinary is interesting: I can simultaneously observe biology's obsession with p-values and machine learning's apparent lack of interest (to generalise &lt;em&gt;wildly&lt;/em&gt;). I am not sure how many papers demonstrate &lt;em&gt;statistically significant&lt;/em&gt; improvement over state-of-the-art, and while I should back up this speculation with reality, at the present moment (1:19am on Thursday) I'll say 'not many' and leave a generic pointer to Gelman's paper &lt;a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf"&gt;The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research
hypothesis was posited ahead of time&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some of the talks/spotlights ended up being poster picks of mine, so I'll describe them below.&lt;/p&gt;
&lt;p&gt;I was strangely entranced by &lt;a href="http://arxiv.org/abs/1412.7091"&gt;Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets&lt;/a&gt;. I have a soft spot for linear algebra tricks. The idea is basically that when you have a very large, but sparse target - as you might get in a language modelling task (trying to predict which of O(100,000) words comes next) - you can do &lt;em&gt;smart things&lt;/em&gt; to obtain gradients without actually calculating the horrible non-sparse, high-dimensional output. Lovely. The problem is that this only works for &lt;em&gt;certain classes&lt;/em&gt; of loss functions, &lt;em&gt;not&lt;/em&gt; including the traditional log softmax one sees in these language applications. So possibly limited benefit, but worthy of further investigation.&lt;/p&gt;
&lt;h3&gt;Poster Session 3&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Refining the poster-session policy, I made it to too many posters and fried my brain.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5845-deep-visual-analogy-making"&gt;Deep Visual Analogy Making&lt;/a&gt; - &lt;em&gt;Scott E. Reed, Yi Zhang, Yuting Zhang, Honglak Lee&lt;/em&gt; - this was a full oral presentation so the poster was crowded. Oh, to be tall. Analogy idea: A 'is to' B as C 'is to' D, given (or simultaneously with) representations of A, ..., D, what does 'is to' mean? Oft-cited example from language modelling is the 'king is to queen as man is to woman' example (from &lt;a href="http://arxiv.org/abs/1310.4546"&gt;word2vec&lt;/a&gt;) where 'is to' is apparently a constant offset vector in the representation space (which is a vector space). This is a very general problem and one I could rant about for quite a long time (indeed, I have &lt;a href="http://arxiv.org/abs/1510.00259"&gt;a paper&lt;/a&gt; on a related topic) so I'll say that the new thing here seems to be the application to images, and nice results/experiments... and probably other details that will only emerge when I read the paper. Cool bonus: they used free (as in Free) game art from the &lt;a href="http://lpc.opengameart.org/"&gt;Liberated Pixel Cup&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5850-training-very-deep-networks"&gt;Training Very Deep Networks&lt;/a&gt; - &lt;em&gt;Rupesh K. Srivastava, Klaus Greff, Juergen Schmidhuber&lt;/em&gt; - (disclosure: Rupesh and Klaus are friends of mine) 
Deeper networks are more better, but training them is hard (vanishing gradients and whatnot) - what to do? Highway networks tackle this by putting gates on layers, choosing between 'transporting' and 'transforming' data. Transporting is just an identity operation and therefore doesn't complicate gradients at all. There are (probably very obvious, for those who know LSTMs) connections to LSTMs here also. Keeping in line wih Klaus's Kubrik-inspired paper titles (previous ones being &lt;a href="http://arxiv.org/abs/1402.3511"&gt;A Clockwork RNN&lt;/a&gt;) and &lt;a href="http://arxiv.org/abs/1503.04069"&gt;LSTM: A Search Space Odyssey&lt;/a&gt;) I'd suggest 'Highway Networks or: How I Learned to Stop Worrying and Transport the Data', but admit further work is needed in this direction.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks"&gt;End-to-end Memory Networks&lt;/a&gt; - &lt;em&gt;Sainbayar Sukhbaatar, arthur szlam, Jason Weston, Rob Fergus&lt;/em&gt; -  continuous extension of &lt;a href="http://arxiv.org/abs/1410.3916"&gt;memory networks&lt;/a&gt;, thus can be trained end-to-end (that is, without direct supervision at each layer, just from input-output pairs). The basic idea of a memory network is that you have some &lt;em&gt;memory component&lt;/em&gt; (surprisingly enough) which the model &lt;em&gt;learns&lt;/em&gt; to read and write to. Obvious applications is question-answering: feed it some text describing a scene, situation etc., then ask questions. I wondered how difficult these tasks could become before the methods started to break down and suggested (I think it was to szlam) that the GRE logic puzzles might be interesting for that, but alas, restricted-access data. One of many reasons we cannot have nice things.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5860-on-the-job-learning-with-bayesian-decision-theory"&gt;On-the-job Learning with Bayesian Decision Theory&lt;/a&gt; - &lt;em&gt;Keenon Werling, Arun Tejasvi Chaganty, Percy S. Liang, Christopher D. Manning&lt;/em&gt; - humans are quite good at tasks you might want an algorithm to perform, but employing humans is expensive (in many ways). Algorithms scale much better in this regard, but they have unacceptably bad performance until they've seen enough data. Solution: combine both. Get the algorithm to assess its certainty on the task, and ask for help when it needs it (using Amazon Mechanical Turk). Seems quite cool/useful, although I have some Complicated Feelings about turking (is it fine? is it creepy? is it exploitative somehow?).&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5873-a-framework-for-individualizing-predictions-of-disease-trajectories-by-exploiting-multi-resolution-structure"&gt;A Framework for Individualising Predictions of Disease Trajectories by Exploiting Multi-Resolution Structure&lt;/a&gt; - &lt;em&gt;Peter Schulam, Suchi Saria&lt;/em&gt; - carefully constructed hierarchical model of disease trajectory to identify patient subgroups. In particular, using a noise model (gaussian process with particular kernel choice) which allows for transient trends such as infection, medication etc. I think the disease severity is measured by lung capacity, so it's a 1-dimensional state space (although patients have covariates etc.), but I don't see any reason why a similar model couldn't handle other state-spaces. It makes for nice graphs, anyway. I'm glad to see probabilistic graphical models for healthcare represented at NIPS.&lt;/p&gt;
&lt;h2&gt;Thursday&lt;/h2&gt;
&lt;p&gt;Some last minute poster-printing shenanigans occupied the morning. For future reference: &lt;a href="http://www.copienova.com/"&gt;Copie Nova&lt;/a&gt; printed my A1 poster in 15 minutes.&lt;/p&gt;
&lt;h3&gt;Poster Session 4&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Update to policy: bump into a friend, end up chatting about twitter bots and other side projects. Miss half the poster session.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning"&gt;Semi-supervised Sequence Learning&lt;/a&gt; - &lt;em&gt;Andrew M. Dai, Quoc V. Le&lt;/em&gt; - I marked this and have no memory of actually reading the poster. I suspect it was mobbed and I gave up. Things in the direction of unsupervised learning are interesting, so the paper is probably interesting.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5950-skip-thought-vectors"&gt;Skip-Thought Vectors&lt;/a&gt; - &lt;em&gt;Ryan Kiros, Yukun Zhu, Ruslan R. Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, Sanja Fidler&lt;/em&gt; - this is the only paper I had already read before coming to the conference, so it was neat to get to talk to Kiros about it briefly! The idea is basically: as in word2vec, you learn a representation of 'meaning' by trying to predict context. This time, the prediction is of the preceding and subsequent &lt;em&gt;sentences&lt;/em&gt;, using RNN encoders/decoders. They also use an interesting trick to augment their underlying word representations by learning a mapping from pre-trained word2vec vectors into their mapping space. This allows for any word2vec-learned word to be used in their setup. I was surprised that this worked well, since the problem should be over-determined (they solve this approximately with a L2 loss, but still). The title is also very eye-catching (the term was coined by Hinton, according to Kiros), although I think we're still a ways away from actually representing &lt;em&gt;thoughts&lt;/em&gt;. Sentences are closer than words, but are they close enough?&lt;/p&gt;
&lt;h3&gt;Symposium: Algorithms Among Us: the Societal Impacts of Machine Learning&lt;/h3&gt;
&lt;p&gt;I am so excited to see the field talking about this. It is very easy as a scientist to divorce oneself from the social, ethical, economic etc. consequences of one's work. I was glad to see a large crowd turn out to this, although there's certainly an element of 'self preservation' here - that is, how do we make sure machine learning (and artificial intelligence) retains a positive status in the eyes of everyone else, and some element of sensationalism regarding 'scary killer robots of the future' (aka 'the children of the singularity are going to murder us all and something about Roko's basilisk'). Nonetheless, cool discussions were had.&lt;/p&gt;
&lt;h4&gt;Economic Issues&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://ebusiness.mit.edu/erik/"&gt;Erik Brynjolfsson&lt;/a&gt; spoke about the &lt;strong&gt;Economic Implications of Machine Intelligence&lt;/strong&gt;. He was proposing that we are in a 'second machine age'; where previously machines were used to replace physical power (as in the industrial revolution), we now see computers providing &lt;em&gt;mental power&lt;/em&gt;, which possibly threatens not to complement humans but replace us. This has implications for the economy (what doesn't?). He showed some graphs about income trends in the USA, which were (as usual) horrifying and enraging. It's uncertain how we can use machine learning to combat this without simultaneously bringing about other changes in society/the economy.&lt;/p&gt;
&lt;h4&gt;Legal Issues&lt;/h4&gt;
&lt;p&gt;&lt;a href="www.iankerr.ca"&gt;Ian Kerr&lt;/a&gt; spoke about &lt;strong&gt;Machine Learning and the Law&lt;/strong&gt;, which was fascinating. Question: can computers &lt;em&gt;make contracts&lt;/em&gt;? Apparently yes! What about &lt;em&gt;product liability&lt;/em&gt;? The manufacturer is usually responsible if there's a &lt;em&gt;defect&lt;/em&gt; in a product, but what if your autonomous vehicle drives into a wall to save a small child? It's doing what it was programmed to do - who's to blame? On that point, &lt;em&gt;should&lt;/em&gt; it do that? He mentions &lt;em&gt;volenti non fit injuria&lt;/em&gt;, that people who enter into risky activities should assume the risk (and entering a self-driving car is a risky activity, arguably). More questions: how much &lt;em&gt;faith&lt;/em&gt; should we put in the output of an algorithm? What if an automated medical diagnosis disagrees with a human? Who do we trust? There are questions of both &lt;em&gt;moral&lt;/em&gt; and &lt;em&gt;legal&lt;/em&gt; liability. If your instinct is to respond with 'trust the human, of course' - what if the algorithm's track record is provably much better than that of the human?&lt;/p&gt;
&lt;h4&gt;Panel on Near-Term Issues&lt;/h4&gt;
&lt;p&gt;(with &lt;em&gt;Tom Dietterich, Ian Kerr, Erik Brynjolfsson, Finale Doshi-Velez, Neil Lawrence, Cynthia Dwork&lt;/em&gt;.) &lt;/p&gt;
&lt;p&gt;I didn't write down who said what, so to anonymously summarise some of the points raised:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the philosophical problems (e.g. trolley/tunnel problem) aren't so clear-cut, because there is &lt;em&gt;uncertainty&lt;/em&gt; and also split-second decision-making which may render 'consulting the human driver' an untenable option.  &lt;/li&gt;
&lt;li&gt;re: people losing jobs to automation: this has been happening for a long time, but that doesn't necessarily make it acceptable. However, arbitrarily banning/regulating things is also not desirable. Both under and over regulation are possibly dangerous.  &lt;/li&gt;
&lt;li&gt;we should look for ways that AI can &lt;em&gt;enhance&lt;/em&gt; human capabilities, rather than trying to &lt;em&gt;replicate&lt;/em&gt; it - this might result in very different-looking research and outcomes.  &lt;/li&gt;
&lt;li&gt;sometimes there just &lt;em&gt;isn't a right answer&lt;/em&gt; because we don't know what the objective function is (particularly in ethics), and encoding a single system of values maybe a fool's errand. (I'm reasonably confident Neil Lawrence said this.)  &lt;/li&gt;
&lt;li&gt;counter-point to the above: robust loss functions exist to allow us to optimise a possibly-misspecified objective function.  &lt;/li&gt;
&lt;li&gt;we are actually already quite forgiving of (human) mistakes in medicine!  &lt;/li&gt;
&lt;li&gt;skill/income gap: what about &lt;em&gt;developing countries&lt;/em&gt;? Someone pointed out that China has moved to a higher-income country, but mostly by doing the low-skilled labour no longer performed in The West.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Panel: Human-level AI... If, How, and When?&lt;/h4&gt;
&lt;p&gt;(with &lt;em&gt;Yann LeCun, Andrew Ng, Gary Marcus, Shane Legg, Tom Dietterich&lt;/em&gt;) &lt;/p&gt;
&lt;p&gt;More semi-anonymous points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;obviously&lt;/em&gt; artificial general intelligence (AGI) is a crude concept, but it's still useful... "I'll know it when I see it."  &lt;/li&gt;
&lt;li&gt;&lt;em&gt;generality&lt;/em&gt; is the main difference between task-oriented algorithms and AGI, but maybe human-level AGI is not so important.  &lt;/li&gt;
&lt;li&gt;reasons to pursuse AGI include better understanding human intelligence, and other questions of psychology.  &lt;/li&gt;
&lt;li&gt;someone questions how useful AGI is to society, as individualised systems already work very well.  &lt;/li&gt;
&lt;li&gt;counter-point to above: hand-crafted systems are being outperformed in some tasks by 'less engineered' ones.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="/images/nips2015_03.png" class="floatr"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;quote from Andrew Ng: "working on AGI today is like working on colonising Alpha Centauri", although he isn't opposed to other people working on it.  &lt;/li&gt;
&lt;li&gt;LeCun emphasises the importance of unsupervised learning for approaching more intelligent machines.  &lt;/li&gt;
&lt;li&gt;Ng says that seeing into the future is hard if not impossible, and reiterates the importance of unsupervised learning for progress.  &lt;/li&gt;
&lt;li&gt;re: self-driving cars: Ng suggests starting with vehicles autonomous on &lt;em&gt;specific routes&lt;/em&gt;, and then expanding their range of activity, rather than starting with an everywhere-driving car which increases in autonomy.  &lt;/li&gt;
&lt;li&gt;"AGI will not be an event. It won't happen instantaneously. We will add capabilities. The hardware matters. Much of our meta-reasoning is about resource allocation. Different hardware infrastructures will lead to different trade-offs. We will see systems with different strengths and weaknesses to humans."  &lt;/li&gt;
&lt;li&gt;minor counter-point to above: maybe in the future, the point at which computers can read open-ended general-domain texts will be regarded as 'the turning point'&lt;/li&gt;
&lt;li&gt;Ng: (paraphrasing): "Forming a committee about evil AI robots is like worrying about overpopulation on Mars."&lt;/li&gt;
&lt;li&gt;LeCun: (approximately, my live-transcription has a non-zero error-rate): "We like to think of our mind as being a generally intelligent machine, but our brains are very very far from being general. We’re driven by basic instincts built into us by evolution for survival, our brains are very limited in their types of connections and functions/signals they can process/compute efficiently. We’re very slow at adding numbers together… it’s very difficult for us to imagine a different type of intelligence than human intelligence, because that’s the only example we have in front of us. Machines will look very different. They won’t have the drives which cause humans to do bad things to each other."&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Friday (&lt;a href="https://sites.google.com/site/nipsmlhc15/"&gt;Workshop on Machine Learning in Healthcare&lt;/a&gt;)&lt;/h2&gt;
&lt;p&gt;This is technically &lt;em&gt;how&lt;/em&gt; I was at NIPS, I was presenting a poster (and I got travel funding).&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/mlhc_poster_final.png" class="floatl"&gt;&lt;/p&gt;
&lt;p&gt;To my eternal shame and regret, I missed everything before the first poster session. I hope the talks will be online soon, because they sounded great:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrating Artificial Intelligence into Emergency Care - &lt;em&gt;Steven Horng&lt;/em&gt;  &lt;/li&gt;
&lt;li&gt;Data-driven Phenotyping of Autism Spectrum Disorders - &lt;em&gt;Finale Doshi Velez&lt;/em&gt;  &lt;/li&gt;
&lt;li&gt;Behavioral Analytics in Mental Health Care - &lt;em&gt;Gourab De&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also accidentaly double-presented my poster, so didn't have time to thoroughly examine work from others.&lt;/p&gt;
&lt;p&gt;Rich Caruana spoke about &lt;strong&gt;Accuracy on the test set is not enough --- the risk of deploying unintelligible models in healthcare&lt;/strong&gt;: interpretability is important in healthcare! He gave an example of a rule-based model which, upon inspection, revealed that asthma appeared to predict &lt;em&gt;better&lt;/em&gt; outcomes for pneumonia patients. Further reflection yielded the explanation that such patients are more closely monitored and may go to the hospital earlier/more often.&lt;/p&gt;
&lt;p&gt;This reminds me of a lesson from my biostatistics class during Part III: from an entirely unspecified population, the information that a given individual has a diagnosis of breast cancer &lt;em&gt;improves&lt;/em&gt; their life expectancy relative to the population at large. Why? Such a diagnosis means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;patient is likely female  &lt;/li&gt;
&lt;li&gt;patient is from a country with breast cancer screening programs  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;... both of which improve one's life expectancy relative to global average. Couple this with reasonable-ish outcomes for breast cancer diagnoses and you have the seemingly counter-intuitive result. The lesson is to always be vigilant (for confounders).&lt;/p&gt;
&lt;p&gt;&lt;a href="http://shahlab.stanford.edu/"&gt;Nigam Shah&lt;/a&gt; spoke about &lt;strong&gt;Building [Machine] Learning Healthcare Systems&lt;/strong&gt;. Apparently 91% of the increase in healthcare costs in the USA is attributable to price increases, and not specific services or ageing. Citation required, obviously, but I didn't take it down. He spent some time discussing how existing data sources (EHRs, clinical trials, chemical databases, health forums, physician query logs, PubMed) can be used to do three things (this is probably an overview of the work done in his lab):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;answer clinical questions, e.g. &lt;em&gt;does androgen therapy for prostate cancer influence risk of Alzheimer's, also as a function of age?&lt;/em&gt;  &lt;/li&gt;
&lt;li&gt;obtain insights from data, e.g. &lt;em&gt;here's a pile of data, tell me something I don't know&lt;/em&gt;  &lt;/li&gt;
&lt;li&gt;form predictive models, e.g.&lt;/li&gt;
&lt;li&gt;which patients will become expensive next year?&lt;/li&gt;
&lt;li&gt;which patients have wounds that won't heal?&lt;/li&gt;
&lt;li&gt;which patients may have latent diseases?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There were several &lt;strong&gt;contributed talks&lt;/strong&gt;. My favourite was from &lt;a href="http://jeannesauve.org/scholar/charles-onu/"&gt;Charles C. Onu&lt;/a&gt; about &lt;strong&gt;detecting asphyxia from a baby's cry&lt;/strong&gt;. Problem setting: asphyxia in newborns is potentially fatal or debilitating, but typical clinical diagnosis requires resources which are not always available (e.g. in rural locations in Nigeria). It turns out that babies with asphyxia &lt;em&gt;cry in a detectably different way&lt;/em&gt;. So he developed the tools (signal processing, classification) and &lt;a href="http://www.ubenwa.com/"&gt;an app&lt;/a&gt; to do this on smartphones (high penetration even in low-resource settings). This is one of the coolest applications of machine learning I've heard of, and it didn't require deep learning. He won the prize for best contribution, and deservedly so. This should be a reminder that impact comes from &lt;em&gt;solving important problems&lt;/em&gt;, not (necessarily) using high-tech solutions.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://dept.stat.lsa.umich.edu/~tewaria/"&gt;Ambuj Tewari&lt;/a&gt; spoke about &lt;strong&gt;Personalised mHealth&lt;/strong&gt;. I find this stuff really fascinating (and wish I had any spare time to work on it - all my spare time is occupied by lasers right now). He motivated the issue by pointing out the &lt;em&gt;dire&lt;/em&gt; state of mental health care in India: apparently there are 343 clinical psychologists (in the &lt;em&gt;country&lt;/em&gt;?), out of a required 13,259, and 290 psycho-social workers out of 19,064. Clearly, anything technology can do to bridge this gap is huge. He pointed out that since smartphone penetration is very high, mHealth has a lot of potential. Then my laptop battery died.&lt;/p&gt;
&lt;p&gt;The rough idea is to use reinforcement learning and expertise from human-computer interaction to devise apps which encourage &lt;em&gt;behaviour modification&lt;/em&gt; in their users. This idea is a little terrifying when you think of malicious actors (encouraging addiction to pay-per-use services/games for example), but their intentions here were noble (so the tech will never be misused, right?). The stated application was fitness for people at risk of heart disease, if I recall. Finding the right balance of push/pull notifications (including frequency and number) is important to encourage persistent engagement. I'm particularly excited for mHealth applications to mental health, especially for self-monitoring and anomaly-detection. These things already exist to an extent, but I'm not sure how much they rely on machine learning.&lt;/p&gt;
&lt;h2&gt;Saturday (&lt;a href="http://www.thespermwhale.com/jaseweston/ram/"&gt;Reasoning, Attention and Memory Workshop&lt;/a&gt;)&lt;/h2&gt;
&lt;p&gt;This workshop was very crowded, and I was only able to get a seat for the first session, so my notes are terrible/missing. The slides are on the workshop page, anyway, so I'm going to substitute an actual overview of this workshop with the following set of bad jokes:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://twitter.com/__hylandSL/status/675669409344671744"&gt;&lt;img src="/images/nips2015_pun1.png" class="center"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/__hylandSL/status/675698712761511936"&gt;&lt;img src="/images/nips2015_pun2.png" class="center"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/__hylandSL/status/675699512430764032"&gt;&lt;img src="/images/nips2015_pun3.png" class="center"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/__hylandSL/status/675761837279965185"&gt;&lt;img src="/images/nips2015_pun4.png" class="center"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/__hylandSL/status/675817625876852736"&gt;&lt;img src="/images/nips2015_pun5.png" class="center"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;No regrets.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Workshop bonus: free icecream.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;There are a lot of smart people working on a lot of interesting problems. Industry interest in machine learning is very high, and it is possible to do research outside of academia. (This is a curiosity for a former theoretical physicist.) The deep learning wave is &lt;em&gt;possibly&lt;/em&gt; cresting, although &lt;em&gt;deep reinforcement learning&lt;/em&gt; seems to be pretty hot. Models including memory are exciting and potentially very powerful, for performance on standard tasks and for (the non-orthogonal problem of) modelling &lt;em&gt;reasoning&lt;/em&gt;. Unsupervised learning is the future. Healthcare is clearly a wonderful match for &lt;em&gt;interpretable models&lt;/em&gt;, both as an application and a source of inspiration for theoretical development (&lt;em&gt;cf.&lt;/em&gt; &lt;a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations#Physical_interpretation"&gt;complex analysis and physics&lt;/a&gt;). The community is vibrant (if hilariously gender-imbalanced), and I'm looking forward to next year.&lt;/p&gt;</content><category term="nips"></category><category term="conference"></category><category term="montreal"></category><category term="canada"></category><category term="ethics"></category><category term="AI"></category></entry><entry><title>vortidplenigilo</title><link href="/lang/2015-11-21-vortidplenigilo.html" rel="alternate"></link><published>2015-11-21T00:00:00+00:00</published><updated>2015-11-21T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2015-11-21:/lang/2015-11-21-vortidplenigilo.html</id><summary type="html">&lt;p&gt;I have been learning Esperanto lately (see &lt;a href="/lang/2015-10-31-esperanto-done-quick.html"&gt;here&lt;/a&gt;).
One of the really cool features of the language is &lt;a href="http://en.lernu.net/lernado/gramatiko/konciza/afiksoj.php"&gt;affixes&lt;/a&gt;.
Basically, you can create new words using some simple morphological rules, e.g.:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;bona (good) → bonulo (good person)&lt;br&gt;
juna (young) → junulo (young person)&lt;/p&gt;
&lt;p&gt;vorto (word) → vortaro (group of words = dictionary/vocabulary …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;I have been learning Esperanto lately (see &lt;a href="/lang/2015-10-31-esperanto-done-quick.html"&gt;here&lt;/a&gt;).
One of the really cool features of the language is &lt;a href="http://en.lernu.net/lernado/gramatiko/konciza/afiksoj.php"&gt;affixes&lt;/a&gt;.
Basically, you can create new words using some simple morphological rules, e.g.:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;bona (good) → bonulo (good person)&lt;br&gt;
juna (young) → junulo (young person)&lt;/p&gt;
&lt;p&gt;vorto (word) → vortaro (group of words = dictionary/vocabulary)&lt;br&gt;
arbo (tree) → arbaro (group of trees = forest)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are a lot of affixes &lt;em&gt;(at time of writing I have 48 suffixes and 18 
prefixes)&lt;/em&gt;, so I thought it might be useful to write a small program to create 
new words by  randomly attaching these affixes, then quizzing myself on them.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://github.com/corcra/esperanto"&gt;Here it is.&lt;/a&gt; (see &lt;a href="https://github.com/corcra/esperanto/blob/master/soup.py"&gt;&lt;code&gt;soup.py&lt;/code&gt;&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;Usage is like this, for example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; soup(root=u&amp;#39;hundo&amp;#39;, n_p=1, n_s=4, cheat=True)
hundo   + pseŭdo : false
        + uj : container for objects described by root
        + esk : similar to/in the manner of whatever is described by root
        + eg : augments or strengthens idea shown by root affix(opposite of -et)
        + ec : quality/characteristic defined by root
pseŭdohundujeskegeco
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;n_p&lt;/code&gt; is the number of prefixes, &lt;code&gt;n_s&lt;/code&gt; suffixes.
The &lt;code&gt;cheat&lt;/code&gt; flag toggles printing the explanation.&lt;/p&gt;
&lt;p&gt;So let's interpret &lt;code&gt;pseŭdohundujeskegeco&lt;/code&gt;... this is an abstract noun, the
quality/characteristic of being a large thing similar to a container for false
dogs. Or a false quality of being a large thing similar to a container for dogs.
The order of interpretation is clear for suffixes &lt;em&gt;or&lt;/em&gt; prefixes, I'm not sure
how to resolve it when both are present.&lt;/p&gt;
&lt;p&gt;This is obviously a ridiculous word which no normal person would use, but I
find generating and interpreting these very entertaining. Another example...&lt;/p&gt;
&lt;p&gt;&lt;a href="https://twitter.com/corcra/status/666986280199262209"&gt;
&lt;img src="/images/ballet_eo.png" class="center"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;baledejarinegestro&lt;/code&gt;: boss of an enormous, somehow female collection of ballet theatres&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I could go on all day. To save myself the effort of doing this, I automated it.
So now there's a...&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://twitter.com/vortidplenigilo"&gt;Twitter bot: vortidplenigilo&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;vortidplenigilo&lt;/code&gt;:&lt;br&gt;
&lt;em&gt;tool to make [something] full of word derivatives&lt;/em&gt;, from &lt;code&gt;vorto + ido + plena + igi + ilo&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Every hour (or so), it tweets a random root (grabbed from a &lt;a href="http://www.esperanto-panorama.net/vortaro/eoen.htm"&gt;dictionary&lt;/a&gt;)
with a random number of suffixes and prefixes. Code is in the same repo as
before, see &lt;a href="https://github.com/corcra/esperanto/blob/master/vortidplenigilo.py"&gt;&lt;code&gt;vortidplenigilo.py&lt;/code&gt;&lt;/a&gt;. It chooses how many affixes to use based on
two draws from Poisson distributions, preferring fewer prefixes. Since it's 
limited by Twitter's 140 character limit, those with &lt;code&gt;n_s&lt;/code&gt; or &lt;code&gt;n_p&lt;/code&gt; above 1 tend
not to make it, unfortunately. Future work will shorten the descriptions so I 
can squeeze more in. The selection of &lt;em&gt;which&lt;/em&gt; affix is not entirely random, 
however...&lt;/p&gt;
&lt;h3&gt;Making affixes make sense&lt;/h3&gt;
&lt;p&gt;Not every affix can go on every type of word. Some take nouns and output nouns,
other take nouns and output adjectives, etc. The &lt;a href="http://donh.best.vwh.net/Esperanto/affixes.html"&gt;page I grabbed the affixes from&lt;/a&gt;
thankfully lists which &lt;code&gt;transformations&lt;/code&gt; are valid, so I encoded that. See &lt;code&gt;affixes.py&lt;/code&gt;
for what is essentially a rendering of aforementioned page into python. The 
sort of information I recorded is explicit in this class definition:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;affix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;u&amp;#39;undefined&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                 &lt;span class="n"&gt;transformations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; 
                 &lt;span class="n"&gt;explanation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;undefined&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                 &lt;span class="n"&gt;conflicts&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{},&lt;/span&gt;
                 &lt;span class="n"&gt;category&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;undefined&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transformations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transformations&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;explanation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;explanation&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conflicts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conflicts&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;category&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;category&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;transformations&lt;/code&gt; is a dict of valid word-type maps, based on word-endings
(since Esperanto is so very regular in this regard). In practice these
dictionaries either have one element (e.g. &lt;code&gt;'a': 'o'&lt;/code&gt;) or all (&lt;code&gt;'i':'i' for 'i'
in valid_word_endings&lt;/code&gt;), but in theory one could have an affix which turns
adjectives into verbs and nouns into adjectives, I suppose. Or something like
that. My code is future-proofed against complicated Esperanto dystopias. The
point is that as the compound word is created, I keep track of its current 'word
type' and make sure I only accept affixes which are compatible with that (and
then it gets a new type from its new affix, and so on). This all takes place in 
the &lt;code&gt;make_soup&lt;/code&gt; function in &lt;code&gt;soup.py&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;explanation&lt;/code&gt; is just the string explaining the affix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;conflicts&lt;/code&gt; is a list of other affixes (by &lt;code&gt;name&lt;/code&gt;) which I forbid to co-exist
in a compound word. The idea is to prevent illogical things like&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;arbarero&lt;/code&gt;: one of a collection of trees... a tree&lt;br&gt;
&lt;code&gt;dormigiĝi&lt;/code&gt;: to become made to be asleep... to sleep&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'm not entirely convinced I want this, though. For example,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;hundetego&lt;/code&gt;: huge small dog&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;sort of makes sense. Jury is out on this decision.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The final attribute, &lt;code&gt;category&lt;/code&gt;, records what &lt;em&gt;type&lt;/em&gt; of affix it is, and is
currently not used. Future version could restrict to &lt;em&gt;true affixes&lt;/em&gt; or
&lt;em&gt;adjective suffixes&lt;/em&gt; or something. Future proof, yo. Maybe.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of its first tweets was beautifully meta:
&lt;a href="https://twitter.com/vortidplenigilo/status/666914030615928832"&gt;&lt;img src="/images/vortidplenigilo.png" class="center"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;morfologiido&lt;/code&gt;: offspring of morphology&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3&gt;Feedback&lt;/h3&gt;
&lt;p&gt;I would gladly welcome comments/ideas on the &lt;a href="https://github.com/corcra/esperanto"&gt;GitHub repository&lt;/a&gt;,
be it language suggestions or corrections (since I am still a &lt;em&gt;komencisto&lt;/em&gt;), code
fixes, ideas for automatically producing 'interpretations' of the generated
words, or anything else. The contents of &lt;a href="https://github.com/corcra/esperanto/blob/master/affixes.py"&gt;&lt;code&gt;affixes.py&lt;/code&gt;&lt;/a&gt; might also be useful
for other people doing things with Esperanto. &lt;/p&gt;</content><category term="esperanto"></category><category term="twitter"></category><category term="bot"></category><category term="grammar"></category></entry><entry><title>esperanto in forty-five seconds</title><link href="/lang/2015-10-31-esperanto-done-quick.html" rel="alternate"></link><published>2015-10-31T00:00:00+00:00</published><updated>2015-10-31T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2015-10-31:/lang/2015-10-31-esperanto-done-quick.html</id><summary type="html">&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Esperanto"&gt;Esperanto&lt;/a&gt; is a constructed international auxiliary language designed to be simple and easy to learn. It achieves this by having a small vocabulary and very regular grammar. It's quite influenced by Indo-European (particularly Romance) languages, so knowing some of those helps a lot.&lt;/p&gt;
&lt;p&gt;This post is not intended to be …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Esperanto"&gt;Esperanto&lt;/a&gt; is a constructed international auxiliary language designed to be simple and easy to learn. It achieves this by having a small vocabulary and very regular grammar. It's quite influenced by Indo-European (particularly Romance) languages, so knowing some of those helps a lot.&lt;/p&gt;
&lt;p&gt;This post is not intended to be exhautive or comprehensive, I mostly want to express how simple the grammar is and highlight other cool things. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Forty-five seconds is about how long it takes me to read this page, but excludes time spent on trying to remember things. If you want to &lt;b&gt;actually&lt;/b&gt; learn, check out the links at the end.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Gender?&lt;/h3&gt;
&lt;p&gt;Nope. The definite article is 'la' for everything.
There is no indefinite article.&lt;/p&gt;
&lt;h3&gt;Nouns?&lt;/h3&gt;
&lt;p&gt;They end in '&lt;strong&gt;-o&lt;/strong&gt;':&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;hundo&lt;/strong&gt;: dog (like the German &lt;em&gt;Hund&lt;/em&gt;)&lt;br&gt;
&lt;strong&gt;feliĉo&lt;/strong&gt;: happiness (like the Spanish &lt;em&gt;felicidad&lt;/em&gt;)&lt;br&gt;
&lt;strong&gt;fromaĝo&lt;/strong&gt;: cheese (like the French &lt;em&gt;fromage&lt;/em&gt;)&lt;br&gt;
&lt;strong&gt;arbo&lt;/strong&gt;: tree (like the Latin &lt;em&gt;arbor&lt;/em&gt;, Spanish &lt;em&gt;àrbol&lt;/em&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To pluralise, add '&lt;strong&gt;-j&lt;/strong&gt;':&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;hundoj&lt;/strong&gt;: dogs&lt;br&gt;
&lt;strong&gt;arboj&lt;/strong&gt;: trees&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Pronouns?&lt;/h3&gt;
&lt;p&gt;This is just a list but people like these things.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;mi&lt;/strong&gt;: me&lt;br&gt;
&lt;strong&gt;vi&lt;/strong&gt;: you      &lt;br&gt;
&lt;strong&gt;ŝi&lt;/strong&gt;: she&lt;br&gt;
&lt;strong&gt;li&lt;/strong&gt;: he      &lt;br&gt;
&lt;strong&gt;ĝi&lt;/strong&gt;: it&lt;br&gt;
&lt;strong&gt;ni&lt;/strong&gt;: we      &lt;br&gt;
&lt;strong&gt;ili&lt;/strong&gt;: they        &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There's no plural 'you', for some reason.&lt;/p&gt;
&lt;h3&gt;Verbs?&lt;/h3&gt;
&lt;p&gt;Conjugation by person isn't a thing. One ending for each tense. Using &lt;strong&gt;esti&lt;/strong&gt; (to be):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;-i&lt;/strong&gt;: &lt;em&gt;Infinitive&lt;/em&gt;: est&lt;b&gt;i&lt;/b&gt;: to be&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-as&lt;/strong&gt;: &lt;em&gt;Present&lt;/em&gt;: est&lt;b&gt;as&lt;/b&gt;: is&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-is&lt;/strong&gt;: &lt;em&gt;Past&lt;/em&gt;: est&lt;b&gt;is&lt;/b&gt;: was&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-os&lt;/strong&gt;: &lt;em&gt;Future&lt;/em&gt;: est&lt;b&gt;os&lt;/b&gt;: will be&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-us&lt;/strong&gt;: &lt;em&gt;Conditional&lt;/em&gt;: est&lt;b&gt;us&lt;/b&gt;: would be&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Mi estas...: &lt;em&gt;I am...&lt;/em&gt;&lt;br&gt;
  La arboj estis...: &lt;em&gt;the trees were...&lt;/em&gt;  &lt;br&gt;
  Feliĉo estus...: &lt;em&gt;happiness would be...&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Objects?&lt;/h3&gt;
&lt;p&gt;Denote the object of a verb (&lt;em&gt;accusative case&lt;/em&gt;) with '&lt;strong&gt;-n&lt;/strong&gt;':&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;La hundo manĝas la fromaĝo&lt;b&gt;n&lt;/b&gt;: &lt;em&gt;The dog eats the cheese&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This stacks with plurals:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Viro vidas la arbo&lt;b&gt;jn&lt;/b&gt;: &lt;em&gt;A man sees the trees&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Adjectives?&lt;/h3&gt;
&lt;p&gt;They end in '&lt;strong&gt;-a&lt;/strong&gt;':&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;bela&lt;/strong&gt;: beautiful      &lt;br&gt;
&lt;strong&gt;rapida&lt;/strong&gt;: fast      &lt;br&gt;
&lt;strong&gt;malrapida&lt;/strong&gt;: slow&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Examples: (note, they must match the noun in case and number),&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Mi estas bela: &lt;em&gt;I am beautiful&lt;/em&gt;&lt;br&gt;
La rapida&lt;b&gt;j&lt;/b&gt; viro&lt;b&gt;j&lt;/b&gt;: &lt;em&gt;The fast men&lt;/em&gt; &lt;br&gt;
La rapida bruna vulpo transsaltas la pigra&lt;b&gt;n&lt;/b&gt; hundo&lt;b&gt;n&lt;/b&gt;: &lt;em&gt;The quick brown fox jumps over the lazy dog&lt;/em&gt;.&lt;a href="#1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Affixes?&lt;/h3&gt;
&lt;p&gt;Observe rapida (fast) → &lt;b&gt;m&lt;/b&gt;alrapida (slow).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;mal&lt;/strong&gt; is a prefix for &lt;em&gt;negation&lt;/em&gt;. Esperanto has &lt;a href="https://en.wiktionary.org/wiki/Category:Esperanto_suffixes"&gt;very&lt;/a&gt; &lt;a href="http://en.lernu.net/lernado/gramatiko/konciza/afiksoj.php"&gt;many&lt;/a&gt; affixes (prefixes or suffixes) which modify word roots to form other words. This is where it gets &lt;em&gt;really cool&lt;/em&gt; and therefore beyond the scope of this post. Briefly, some examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hundo → hund&lt;b&gt;id&lt;/b&gt;o: &lt;em&gt;dog&lt;/em&gt; → &lt;em&gt;puppy&lt;/em&gt; (offspring of dog)&lt;/li&gt;
&lt;li&gt;ĵurnalo → ĵurnal&lt;b&gt;ist&lt;/b&gt;o: &lt;em&gt;newspaper&lt;/em&gt; → &lt;em&gt;journalist&lt;/em&gt;  (professional of newspaper)&lt;/li&gt;
&lt;li&gt;salo → sal&lt;b&gt;er&lt;/b&gt;o: &lt;em&gt;salt&lt;/em&gt; → &lt;em&gt;grain of salt&lt;/em&gt; (one of the many same salt objects)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This was just a taste of Esperanto. For more, see &lt;a href="https://www.duolingo.com/course/eo/en/Learn-Esperanto-Online"&gt;Duolingo&lt;/a&gt;, &lt;a href="http://en.lernu.net/lernado/gramatiko/konciza/index.php"&gt;grammar at lernu!&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=mWbyXVSiCxw&amp;amp;list=PLC8C7DF5ECBAA4E5E"&gt;Mazi en Gondolando&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a name="1"&gt;[1]&lt;/a&gt; This English pangram is not an Esperanto pangram! From &lt;a href="http://clagnut.com/blog/2380"&gt;here&lt;/a&gt;, an Esperanto pangram would be:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Laŭ Ludoviko Zamenhof bongustas freŝa ĉeĥa manĝaĵo kun spicoj.&lt;br&gt;
&lt;em&gt;According to Ludwig Zamenhof, fresh Czech food with spices tastes good.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><category term="esperanto"></category><category term="grammar"></category></entry><entry><title>I will [not] follow you into the darknet (radical networks)</title><link href="/sec/2015-10-27-radical-networks.html" rel="alternate"></link><published>2015-10-27T13:37:00+00:00</published><updated>2015-10-27T13:37:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2015-10-27:/sec/2015-10-27-radical-networks.html</id><summary type="html">&lt;p&gt;Last weekend I presented with two friends (&lt;a href="https://twitter.com/huertanix/"&gt;huertanix&lt;/a&gt; and &lt;a href="https://twitter.com/carolinesinders"&gt;Caroline Sinders&lt;/a&gt;) at the first ever &lt;a href="http://radicalnetworks.org/"&gt;Radical Networks&lt;/a&gt; conference. Our talk was originally called 'Blogging on the Darknet' but after a questionably productive co-working session we changed it to 'I Will Follow You Into the Darknet', after a Death Cab for …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last weekend I presented with two friends (&lt;a href="https://twitter.com/huertanix/"&gt;huertanix&lt;/a&gt; and &lt;a href="https://twitter.com/carolinesinders"&gt;Caroline Sinders&lt;/a&gt;) at the first ever &lt;a href="http://radicalnetworks.org/"&gt;Radical Networks&lt;/a&gt; conference. Our talk was originally called 'Blogging on the Darknet' but after a questionably productive co-working session we changed it to 'I Will Follow You Into the Darknet', after a Death Cab for Cutie song. Caroline even &lt;a href="https://twitter.com/carolinesinders/status/651509665705127936"&gt;wrote lyrics&lt;/a&gt; for it. I sort of hate the term 'darknet' but until the Tor Project settle on a nomenclature for location-hidden services (and their related -ome&lt;a href="#1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;) it'll have to do. And that is how terms become accepted. Sorry, world.&lt;/p&gt;
&lt;p&gt;Anyway, the slides are &lt;a href="http://fu.io/followdarknet"&gt;here&lt;/a&gt; (warning: gifs, dank memes) and there is a video recording &lt;a href="http://livestream.com/internetsociety/radicalnetworks/videos/102829121"&gt;here&lt;/a&gt;, and have an embedded version too:&lt;/p&gt;
&lt;iframe src="https://livestream.com/accounts/686369/events/4317418/videos/102829121/player?width=640&amp;height=360&amp;autoPlay=false&amp;mute=false" width="640" height="360" frameborder="0" scrolling="no"&gt;&lt;/iframe&gt;

&lt;p&gt;Full disclosure: we didn't practice and most of our working sessions devolved into expeditions into giphy, in case that wasn't obvious. I think the last section could have done with being an interactive/hands-on workshop, but we only had an hour slot. And then &lt;a href="https://twitter.com/brianloveswords"&gt;@brianloveswords&lt;/a&gt; turned (the technical part of) our talk into &lt;a href="https://github.com/brianloveswords/onion-ghost-blog"&gt;two commands&lt;/a&gt;. Also, in case there was &lt;em&gt;any uncertainty&lt;/em&gt;, I'm not an opsec professional. Do those even exist?&lt;/p&gt;
&lt;p&gt;The rest of the conference was really good, although I missed a lot of Saturday due to occupying a table of cannolis and devising grandiose schemes. Events like this are great for inspiring ideas, so my todo list is ever-growing. In an attempt to avoid the lure of shiny new projects, I'm disallowing myself from starting new things before I've cleared some of my backlog. Let's see how well that goes.&lt;/p&gt;
&lt;p&gt;Relevant to this website, one result of the conference was (after much discussion of TLDs) learning that .io belongs to the British Indian Ocean Territory, which has an unpleasant colonial history: the native Chagossians were forced out by the British in the 60s/70s so that the USA could set up a military base, which has &lt;a href="http://www.theguardian.com/world/2015/jan/30/cia-interrogation-diego-garcia-lawrence-wilkerson"&gt;apparently been used in the CIA's rendition (and torture) program!&lt;/a&gt; It's unclear how much actually &lt;em&gt;having&lt;/em&gt; a .io domain contributes to any of this, of course, but I'm ashamed I didn't know any of this before. It's easy to forget that country TLDs come with political baggage.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a name="1"&gt;[1]&lt;/a&gt; An awful idea which just occurred to me is 'onion'-ome, after genome and proteome and transcriptome and epitranscriptome and connectome and interactome and literome, etc. In the grim future of Hello Kitty, everything is an ome.&lt;/p&gt;</content><category term="opsec"></category><category term="anonymity"></category><category term="tor"></category></entry><entry><title>thoughts from mlss 2015 tübingen</title><link href="/ml/2015-07-30-mlss2015.html" rel="alternate"></link><published>2015-07-30T13:37:00+01:00</published><updated>2015-07-30T13:37:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2015-07-30:/ml/2015-07-30-mlss2015.html</id><summary type="html">&lt;p&gt;I recently attended the &lt;a href="http://mlss.tuebingen.mpg.de/2015/index.html"&gt;Machine Learning Summer School at the MPI in Tübingen&lt;/a&gt;. This wasn't my first time at an event like this - I attended the &lt;a href="http://ml.dcs.shef.ac.uk/gpss/gpss14/"&gt;Gaussian Process 'Summer' School&lt;/a&gt; in September 2014 - but the MLSS is a lot bigger/diverse. I'm fairly sure I didn't even speak to all …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I recently attended the &lt;a href="http://mlss.tuebingen.mpg.de/2015/index.html"&gt;Machine Learning Summer School at the MPI in Tübingen&lt;/a&gt;. This wasn't my first time at an event like this - I attended the &lt;a href="http://ml.dcs.shef.ac.uk/gpss/gpss14/"&gt;Gaussian Process 'Summer' School&lt;/a&gt; in September 2014 - but the MLSS is a lot bigger/diverse. I'm fairly sure I didn't even speak to all the other participants (unfortunately).&lt;/p&gt;
&lt;p&gt;The basic format is lectures all day (9am til roughly 5pm) and various academic or social activities in the evenings. I foolishly though it would be possible to get lots of work done in the free time, which was wrong on two counts: free time was limited, and the hostel had essentially no WiFi. By that I mean it was impressively bad: pings of the order 5s, packet loss above 50%. Free café WiFi is also harder to find in Tübingen than in NYC (somehow!), so by the end of the two weeks, MLSS participants could be found sitting near eduroam hotspots across the town.&lt;/p&gt;
&lt;p&gt;Luckily there are better things to do at a summer school than struggle with laggy ssh tunnels. The lectures provided good exposure various topics within machine learning, although a 3-hour course is necessarily limited in depth. Most of the lectures were recorded and will probably be &lt;a href="http://webdav.tuebingen.mpg.de/mlss2013/2015/speakers.html"&gt;here&lt;/a&gt; eventually. Those from 2013 are still available &lt;a href="http://webdav.tuebingen.mpg.de/mlss2013/2013/index.html"&gt;here&lt;/a&gt;. There are also more from other MLSS venues &lt;a href="http://videolectures.net/site/search/?q=mlss"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My favourites were Tamara Broderick's &lt;em&gt;Bayesian Nonparametrics&lt;/em&gt; and Zoubin Ghahramani's &lt;em&gt;Bayesian Inference&lt;/em&gt; (note my bias). Michael Hirsch's &lt;em&gt;Computational Imaging&lt;/em&gt; and Michael Black's &lt;em&gt;Learning Human Body Shape&lt;/em&gt; were also enjoyable, largely due to demonstrations. The former briefly covered MIT's &lt;a href="https://people.csail.mit.edu/mrub/VisualMic/"&gt;visual microphone&lt;/a&gt; which prompted a similar level of disquiet as it did on infosec twitter, although more fascination. The unearthly sounds of the reconstructions do little to ease the creep level.&lt;/p&gt;
&lt;p&gt;My favourite session overall was the practical from Frank Wood and Brooks Paige on &lt;em&gt;Probabilistic Programming&lt;/em&gt; (&lt;a href="https://bitbucket.org/probprog/mlss2015"&gt;bitbucket repo&lt;/a&gt;), possibly because I am a nascent Clojure fan, or maybe I just love sampling. I'm also quite enthusiastic about abstracting away implementation details and focusing on models, which &lt;a href="http://www.robots.ox.ac.uk/~fwood/anglican/"&gt;Anglican&lt;/a&gt; facilitates. How much use I'll make of it in my own research has yet to be determined.&lt;/p&gt;
&lt;p&gt;Something which cannot be replicated via video lectures or git repos (yet) is interaction with other participants. As I mentioned, there were a lot of us (about 100), and the poster sessions were probably the best opportunity to talk science. I'm not sure how participants were selected but I was impressed by the diversity of research represented. It turns out not &lt;em&gt;everyone&lt;/em&gt; is throwing convnets at everything (but maybe they should be?). There was also a lot more theory than I was expecting, which is what happens when you assume your biased sample (of largely-applied colleagues) is representative of the whole. Lesson learned. I didn't take any notes at the poster sessions (nor did I read all of the posters), so I'll just mention a few that stand out in my memory (and have something concrete to link to).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.mpi-sws.org/~mzafar/"&gt;Muhammad Bilal Zafar&lt;/a&gt; had a poster about &lt;a href="http://www.mpi-sws.org/~mzafar/papers/fatml_15.pdf"&gt;Fairness Constraints: A Mechanism for Fair Classification&lt;/a&gt;. Quoting from the paper,  &lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;"Fairness prevents a classifier from outputting predictions correlated with certain sensitive attributes in the data."&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I was really excited to see a poster about fairness, especially having just read &lt;a href="http://jeremykun.com/2015/07/13/what-does-it-mean-for-an-algorithm-to-be-fair/"&gt;"What Does it Mean for an Algorithm to be Fair?"&lt;/a&gt;. The danger exists for people to believe that the recommendations from a machine learning algorithm are 'fair' (for some nebulous definition of fair, likely including 'not racist' and 'not sexist'), which could be used to avoid addressing systemic social injustices. It's important for machine learning researchers/users to stress that the output of learning algorithm is a function of its training data (madness, I know), and as long as our historical data contains biases, models trained on it will have them too. That is, unless we do something about it. I'm sure there are more subtle factors at play that I'm not aware of, but I'm glad that these issues are being considered by the research community.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://people.idsia.ch/~greff/index.html"&gt;Klaus Greff&lt;/a&gt; presented a poster about an experiment-management tool he created called &lt;a href="https://github.com/IDSIA/sacred"&gt;Sacred&lt;/a&gt;. (The name is a reference to Monty Python's &lt;a href="https://www.youtube.com/watch?v=fUspLVStPbk"&gt;Every Sperm is Sacred&lt;/a&gt;). This obviously isn't research, but it seems extremely useful. It records things like config options, a snapshot of the source code(!), runtime trace(s), and saves them in a (mongoDB) database. I already have a semi-elaborate setup for running reproducible experiments (the details of which are too gory and shameful to provide), but this seems more pleasant and sane.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.robots.ox.ac.uk/~twgr/"&gt;Tom Rainforth&lt;/a&gt; had a poster about &lt;a href="http://arxiv.org/pdf/1507.05444v2.pdf"&gt;Canonical Correlation Forests&lt;/a&gt; which I didn't actually get to look at (I was in the same poster session), but the gist I got from a chat in the pub is that they're better than random forests (my brain has a very aggressive compression algorithm, clearly). I'll need to read the paper. I have a picture of him explaining the poster to someone on the bus after the poster session, demonstrating that science never rests.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;My friend &lt;a href="http://www.maillard.it/"&gt;Jean Maillard&lt;/a&gt; had a poster on &lt;a href="https://aclweb.org/anthology/K/K15/K15-1035.pdf"&gt;Learning Adjective Meanings with a Tensor-Based Skip-Gram Model&lt;/a&gt;. This was by far the most similar to mine (my poster was also on distributional semantics), although this paper focuses moreso on language-modelling, by representing adjectives as matrices. I'm amused that Jean and I started off doing something entirely different (at the time, Part III in Mathematics at Cambridge, mostly flipping tables over quantum algorithms) and then converged (if only temporarily, for me) on something that is (at least by MLSS standards) &lt;em&gt;somewhat&lt;/em&gt; obscure. Maybe there was something in the water at St. John's.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There were also lots of opportunities to talk non-science. On the last afternoon, a straw poll was conducted on the viability of human-level AI during our lifetime. The majority present (n ~ 10) felt it wasn't going to happen, which seems to go against popular opinion on the matter (at least judging by recent articles about the threat of such AIs). Maybe grad students are too pessimistic (optimistic?) a group, or we succumbed to our small sample size. The poll wasn't even conducted in secret.&lt;/p&gt;
&lt;p&gt;Another outcome of the MLSS is that I reaffirmed my desire to write a &lt;em&gt;Gaussian Processes for Biologists&lt;/em&gt; tutorial. &lt;em&gt;Biologist&lt;/em&gt; here really means 'anyone lacking a strong mathematical background' (hopefully in the future it will be offensive for me to use biologist as a proxy for that). I'd originally planned to do this after the GPSS last year, partially out of GP evangelism (all the people doing simple linear regression could be doing GP regression!) and partially to deepen my own understanding (one learns much through teaching), but progress stalled due to lack of interest. Interest is briefly re-ignited, so maybe I'll actually do it this time[citation needed].&lt;/p&gt;</content><category term="class"></category><category term="life"></category><category term="poster"></category><category term="AI"></category><category term="bayesian"></category></entry><entry><title>quotes in awk</title><link href="/tips/2015-05-27-quotes-in-awk.html" rel="alternate"></link><published>2015-05-27T00:00:00+01:00</published><updated>2015-05-27T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2015-05-27:/tips/2015-05-27-quotes-in-awk.html</id><summary type="html">&lt;p&gt;Lazily trying to paste a long list of strings into python, which means I need things wrapped in quotes (and commas, but I'm excluding them from this example cause they're easy). Grabbing the strings from a file using &lt;code&gt;awk&lt;/code&gt;, but since quotes (apostrophes) are special in &lt;code&gt;awk&lt;/code&gt; this messes things …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lazily trying to paste a long list of strings into python, which means I need things wrapped in quotes (and commas, but I'm excluding them from this example cause they're easy). Grabbing the strings from a file using &lt;code&gt;awk&lt;/code&gt;, but since quotes (apostrophes) are special in &lt;code&gt;awk&lt;/code&gt; this messes things up a little. The way to do it is either&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;awk &amp;#39;{ print &amp;quot;&amp;#39;&amp;quot;&amp;#39;&amp;quot;&amp;#39;&amp;quot;$F&amp;quot;&amp;#39;&amp;quot;&amp;#39;&amp;quot;&amp;#39;&amp;quot; }&amp;#39; file.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(I kid you not), or&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;awk &amp;#39;{ print &amp;quot;\047&amp;quot;$F }&amp;#39; file.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where &lt;code&gt;$F&lt;/code&gt; is whatever field of &lt;code&gt;file.txt&lt;/code&gt; is interesting.&lt;/p&gt;
&lt;p&gt;I think the second method might be a bit nicer.&lt;/p&gt;</content><category term="awk"></category><category term="bash"></category><category term="unix"></category><category term="shell"></category></entry><entry><title>density plots without outlines in ggplot2</title><link href="/tips/2015-05-20-density-ggplot2.html" rel="alternate"></link><published>2015-05-20T13:37:00+01:00</published><updated>2015-05-20T13:37:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2015-05-20:/tips/2015-05-20-density-ggplot2.html</id><summary type="html">&lt;p&gt;It is not:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;anything in the &lt;code&gt;aes&lt;/code&gt; of the &lt;code&gt;ggplot&lt;/code&gt; call  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;color=FALSE&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;color=NULL&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;color=NA&lt;/code&gt; in the &lt;code&gt;geom_density&lt;/code&gt; call&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;e.g.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ggplot&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;,&lt;/span&gt; aes&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="o"&gt;=&lt;/span&gt;value&lt;span class="p"&gt;,&lt;/span&gt; fill&lt;span class="o"&gt;=&lt;/span&gt;grouping&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;geom_density&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;b&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;color&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;b&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Example (this is some real data I'm currently working with, but I've changed …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It is not:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;anything in the &lt;code&gt;aes&lt;/code&gt; of the &lt;code&gt;ggplot&lt;/code&gt; call  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;color=FALSE&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;color=NULL&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;color=NA&lt;/code&gt; in the &lt;code&gt;geom_density&lt;/code&gt; call&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;e.g.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ggplot&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;,&lt;/span&gt; aes&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="o"&gt;=&lt;/span&gt;value&lt;span class="p"&gt;,&lt;/span&gt; fill&lt;span class="o"&gt;=&lt;/span&gt;grouping&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;geom_density&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;b&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;color&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;b&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Example (this is some real data I'm currently working with, but I've changed the labels so it's hopefully not meaningful):&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/density_outline.png" class="center" style="width: 40vw;"/&gt;&lt;/p&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ggplot&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;,&lt;/span&gt; aes&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="o"&gt;=&lt;/span&gt;variable&lt;span class="p"&gt;,&lt;/span&gt; fill&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kr"&gt;switch&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
geom_density&lt;span class="p"&gt;(&lt;/span&gt;alpha&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; color&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
facet_grid&lt;span class="p"&gt;(&lt;/span&gt;case&lt;span class="o"&gt;~&lt;/span&gt;condition&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
xlim&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;-3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; theme_bw&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; 
scale_fill_manual&lt;span class="p"&gt;(&lt;/span&gt;values&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;darkorchid2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;darkturquoise&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
ggtitle&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;density plots with no outline&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="ggplot2"></category><category term="R"></category><category term="vis"></category><category term="graphics"></category></entry><entry><title>just the tips</title><link href="/meta/2015-05-20-just-the-tips.html" rel="alternate"></link><published>2015-05-20T00:00:00+01:00</published><updated>2015-05-20T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2015-05-20:/meta/2015-05-20-just-the-tips.html</id><summary type="html">&lt;p&gt;For fear of turning this blog into a &lt;a href="https://en.wikipedia.org/wiki/Chinese_restaurant_process"&gt;Chinese Restaurant Process&lt;/a&gt; I've spawned a new category, called &lt;font class="tips"&gt;tips&lt;/font&gt;. This category is the closest to my original vision for a PhD-blog, where I would write down useful things as I learned them. It turns out that's not very sustainable (PhD involves …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For fear of turning this blog into a &lt;a href="https://en.wikipedia.org/wiki/Chinese_restaurant_process"&gt;Chinese Restaurant Process&lt;/a&gt; I've spawned a new category, called &lt;font class="tips"&gt;tips&lt;/font&gt;. This category is the closest to my original vision for a PhD-blog, where I would write down useful things as I learned them. It turns out that's not very sustainable (PhD involves a lot of learning). Every time I went to write a post I found it difficult to resist the urge to make it very comprehensive and pedagogical, which ended up in me carefully studying &lt;code&gt;man&lt;/code&gt; pages.&lt;/p&gt;
&lt;p&gt;The &lt;font class="tips"&gt;tips&lt;/font&gt; category will contain posts that are unashamedly brief and perhaps shamefully trivial. If I have a simple problem that took me &lt;em&gt;too long&lt;/em&gt; (like  over 5 minutes) to solve, I'll put my solution in &lt;font class="tips"&gt;tips&lt;/font&gt;. As ever, making hard categorical assignments to soft things like blog posts containing multiple topics is a fool's errand, but I'll deal with it as it comes. Perhaps I should be using &lt;a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"&gt;LDA&lt;/a&gt;, or like, tags or something.&lt;/p&gt;</content><category term="categories"></category><category term="LDA"></category><category term="bayesian nonparametrics"></category><category term="webdev"></category></entry><entry><title>pgp 101</title><link href="/sec/2015-04-28-pgp-101.html" rel="alternate"></link><published>2015-04-28T13:37:00+01:00</published><updated>2015-04-28T13:37:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2015-04-28:/sec/2015-04-28-pgp-101.html</id><summary type="html">&lt;p&gt;I gave an 'Introduction to PGP'-type talk/tutorial at &lt;a href="https://twitter.com/cryptoharlem"&gt;CryptoHarlem&lt;/a&gt; last night. PGP can be a little confusing and it gets a lot of criticism for being unusable, so I tried to focus on the higher-level aspects, like how it works and why you want that. I fear it ended …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I gave an 'Introduction to PGP'-type talk/tutorial at &lt;a href="https://twitter.com/cryptoharlem"&gt;CryptoHarlem&lt;/a&gt; last night. PGP can be a little confusing and it gets a lot of criticism for being unusable, so I tried to focus on the higher-level aspects, like how it works and why you want that. I fear it ended up being a little too theoretical, but my rationale was that a person who understands how PGP achieves what it does will find it less confusing overall. That said, I have always fallen on the 'theory then application' side of the fence when this comes up, so I admit my bias.&lt;/p&gt;
&lt;p&gt;I resisted the temptation to make it insufferably long and all-encompassing, so an observer may note that many details have been glossed over. My plan is to incrementally improve it until I can't stand it, so feedback is greatly appreciated. I'm also thinking to make a &lt;code&gt;pgp_102&lt;/code&gt; which covers things like subkeys, key-signing, and anything else that seems advanced-but-actually-useful.&lt;/p&gt;
&lt;h2&gt;&lt;a href="/pdfs/pgp_101.pdf"&gt;Link to the slides&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I think it also bears mentioning that the entire presentation was an excuse to personally indulge in &lt;a href="https://thenounproject.com/"&gt;The Noun Project&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And unrelatedly...&lt;br&gt;
&lt;a href='https://twitter.com/corcra/status/592772656878977024'&gt;&lt;img src="/images/pgpond.png" style='width: 350px;'&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="pgp"></category><category term="tutorial"></category><category term="encryption"></category><category term="cryptography"></category></entry><entry><title>aligning vectors (animation)</title><link href="/ml/2015-04-09-aligning-vectors-animation.html" rel="alternate"></link><published>2015-04-09T13:37:00+01:00</published><updated>2015-04-09T13:37:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2015-04-09:/ml/2015-04-09-aligning-vectors-animation.html</id><summary type="html">&lt;p&gt;I'm working on an embedding procedure for placing &lt;em&gt;things&lt;/em&gt; into vector spaces. Things which might not normally live in a vector space (although in a sense we all live in a Lorentzian manifold). The basic idea is to take a model, present a bunch of examples of these objects relating …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm working on an embedding procedure for placing &lt;em&gt;things&lt;/em&gt; into vector spaces. Things which might not normally live in a vector space (although in a sense we all live in a Lorentzian manifold). The basic idea is to take a model, present a bunch of examples of these objects relating to each other (outside of any notion of a metric), and then it figures out where to put them. &lt;em&gt;(In the event of me getting my model to do anything actually useful, I will provide excessive technical detail, but that's not the objective here)&lt;/em&gt;. This evening I remembered that 2 dimensions are very easy to visualise, so I made an animation of how the objects move in the space as training progresses (number of training examples is depicted in the plot title).&lt;/p&gt;
&lt;p&gt;The example here is exceedingly trivial - five objects, two pairs of which are designed (by way of engineering the training data) to end up together and apart (colour-coded), and one loner who goes wherever. So they're not so much learning to agree as learning to jealously cling to their partner.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://corcra.github.io/assets/w5.gif" class="center"/&gt;&lt;/p&gt;
&lt;p&gt;Look at them twitch! Stochastic gradient descent in action. I'm tempted to make more of these with different learning rates and see how badly I can get it to break, but I foolishly started doing this &lt;em&gt;too late&lt;/em&gt;, so I'll save it for next time. (Also next time: evolving energy surfaces).&lt;/p&gt;</content><category term="animation"></category><category term="visualisation"></category><category term="embedding"></category><category term="nlp"></category></entry><entry><title>lá fhéile pádraig</title><link href="/lang/2015-03-17-la-fheile-padraig.html" rel="alternate"></link><published>2015-03-17T00:00:00+00:00</published><updated>2015-03-17T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2015-03-17:/lang/2015-03-17-la-fheile-padraig.html</id><summary type="html">&lt;p&gt;Today is St. Patrick's day, which means there's a lot of this online:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lá Féile Pádraig shona duit&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;or this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lá Fhéile Phádraig sona duit&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;or basically any variation of:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lá F(h)éile P(h)ádraig s(h)ona duit&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;which &lt;em&gt;all&lt;/em&gt; roughly mean 'Happy St. Patrick's Day' in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today is St. Patrick's day, which means there's a lot of this online:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lá Féile Pádraig shona duit&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;or this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lá Fhéile Phádraig sona duit&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;or basically any variation of:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lá F(h)éile P(h)ádraig s(h)ona duit&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;which &lt;em&gt;all&lt;/em&gt; roughly mean 'Happy St. Patrick's Day' in Irish, but some of which are grammatically incorrect. (&lt;strong&gt;Note&lt;/strong&gt;: I'm not going to get into pronunciation here, but for the curious, &lt;a href="https://www.youtube.com/watch?v=Pop2ivFUOw8"&gt;here's a video&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;When I was learning Irish in school this was generally the trouble. The meaning is clear most of the time, but where or when to put the séimhiú (lenition - those 'h's above), the urú (eclipsis), or the additional 'i's (slendering) was never particularly clear. I got through Irish on the &lt;em&gt;vague feeling&lt;/em&gt; about what was correct, mostly based off the sound of the thing&lt;a href="#1"&gt;[1]&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;On some recent trip home to Dublin I bought a book on Irish grammar in some optimistic hope of fixing the situation. Inside this book must be contained a set of concise and logical rules, the mastery of which will enable me to unambiguously resolve problems like the above. &lt;em&gt;(hahahaha)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's break the phrase down. Translated literally (&lt;em&gt;and with caveats&lt;/em&gt;)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;lá fhéile Pádraig -&amp;gt; the feast day of Patrick&lt;br&gt;
X shona duit -&amp;gt; happy X to you&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The second one is a bit easier so we can get that out of the way.&lt;/p&gt;
&lt;p&gt;'Sona' means 'happy', and as an adjective it is modified to agree with its noun. As a member of the &lt;em&gt;third declension&lt;/em&gt;, 'sona' undergoes no change to its &lt;em&gt;ending&lt;/em&gt;, but it does pick up a séimhiú (h) when it's paired with a &lt;em&gt;feminine&lt;/em&gt; noun.&lt;/p&gt;
&lt;p&gt;'Duit' is a prepositional pronoun, which in this case means&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;duit = do + tú = to + you = to you&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;the 'to' of 'do' is in the proposition sense (don't try to use it to form infinitives of verbs, or anything like that)&lt;a href="#2"&gt;[2]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So we really have&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;XY sona duit -&amp;gt; happy XY (masc) to you&lt;br&gt;
XX shona duit -&amp;gt; happy XX (fem) to you&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now for the first part. This is where things go wrong for me.&lt;/p&gt;
&lt;p&gt;'Lá' means 'day'. It is masculine.&lt;br&gt;
'Féile' means 'feast'. It is feminine.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.tearma.ie/Search.aspx?term=l%C3%A1+f%C3%A9ile"&gt;Tearma.ie&lt;/a&gt; seems to think that one makes 'feast day' with 'lá féile', which seems to agree with my grammar book. In the compound noun, 'féile' &lt;em&gt;should&lt;/em&gt; go into the tuiseal ginideach (genitive case). As a &lt;em&gt;fourth declension&lt;/em&gt; noun, this means it undergoes &lt;em&gt;no change&lt;/em&gt;. The resultant word ('feast day') is &lt;em&gt;masculine&lt;/em&gt; (the gender is taken from the first noun), so we'll follow it up with 'sona'.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;However&lt;/em&gt;, &lt;a href="http://www.tearma.ie/Search.aspx?term=L%C3%A1+Fh%C3%A9ile+P%C3%A1draig"&gt;tearma.ie&lt;/a&gt; also says St. Patrick's day is 'Lá F&lt;strong&gt;h&lt;/strong&gt;éile Pádraig', so something is going on here. Is the presence of the 'Pádraig' complicating the issue?&lt;/p&gt;
&lt;p&gt;The answer that I've found for this is: &lt;strong&gt;yes&lt;/strong&gt;. My grammar book is silent on the topic of this slightly-advanced compound noun case. According to &lt;a href="http://nualeargais.ie/gnag/subst2.htm#genitivverwend"&gt;Nua Leargais&lt;/a&gt;, when two genitives come together (as we would naïvely try for 'féile' and 'Pádraig' after 'lá'), we encounter the &lt;em&gt;functional genitive&lt;/em&gt;. In this case:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The first word is in the 'functional genitive', and its form is that of a &lt;em&gt;lenited&lt;/em&gt;, nominative form. So this gives us 'f&lt;strong&gt;h&lt;/strong&gt;éile', even though the &lt;em&gt;genitive&lt;/em&gt; of 'féile' is 'féile', as I mentioned before.&lt;/li&gt;
&lt;li&gt;The second word is in the genitive. The question is then...&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;What the hell is the genitive of Pádraig?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The answer seems to be 'it depends'. Situations like this are why I end up feeling like 'Phádraig' and 'Pádraig' are equally valid and get confused. It seems like:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In general&lt;/em&gt;, the genitive of 'Pádraig' is 'P&lt;strong&gt;h&lt;/strong&gt;ádraig' (so 'Patrick's house' is 'teach Phádraig'). This is supported by &lt;a href="http://www.tearma.ie/Search.aspx?term=Pádraig"&gt;tearma.ie&lt;/a&gt;. However, in special cases it &lt;em&gt;doesn't change&lt;/em&gt;, and those cases include 'after féile' and 'after naomh'&lt;a href="#3"&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So to summarise, if you want to say 'Happy St. Patrick's Day' in Irish, and be (probably) grammatically correct about it, it's:&lt;/p&gt;
&lt;h2&gt;Lá Fhéile Pádraig sona duit!&lt;/h2&gt;
&lt;p&gt;And now you know.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a name="1"&gt;[1]&lt;/a&gt; This turned out to be sufficient to get an A1 in the exam (somehow), but I'll never feel capable of &lt;em&gt;teaching&lt;/em&gt; anyone the language if I can't explain why things are how they are.&lt;/p&gt;
&lt;p&gt;&lt;a name="2"&gt;[2]&lt;/a&gt; We learned the prepositional pronouns using songs and hand gestures in school, so they're indelibly seared in my memory. Here's the full list of the 'do' ones:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;dom = to me&lt;br&gt;
duit = to you&lt;br&gt;
dó = to him/it&lt;br&gt;
di = to her/it&lt;br&gt;
dúinn = to us&lt;br&gt;
daoibh = to you (plural)&lt;br&gt;
dóibh = to them&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And &lt;a href="http://www.irishpage.com/quiz/preppron.htm"&gt;here's a page&lt;/a&gt; with some more. You might notice that 'do' means 'to/for' there, which is true, but that is beyond the scope of this post.&lt;/p&gt;
&lt;p&gt;&lt;a name="3"&gt;[3]&lt;/a&gt; 'Naomh' means 'saint', so if you, like me, went to a school named after St. Patrick and grew up hearing 'Naomh Pádraig' all the time, you might be forgiven in thinking that's its genitive form.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Here are some pages I found useful/interesting while trying to resolve this problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.daltai.com/discus/messages/13510/50292.html?1268966813"&gt;Don't lenite names after 'féile', also some functional genitive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.daltai.com/discus/messages/13510/25063.html?1179788462"&gt;Combining nouns but possibly not aware of the functional genitive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://heatherrosejones.com/names/goedelic/irishgenitives.html"&gt;More detail about the genitive of Irish names than I have time for&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Irish_initial_mutations#Environments_of_lenition"&gt;When to lenite nouns?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.googlefight.com/index.php?word1=naomh+p%E1draig&amp;amp;word2=naomh+ph%E1draig"&gt;Naomh Pádraig wins a google fight against Naomh Phádraig, confirming my intuition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nualeargais.ie/gnag/lenition.htm"&gt;Nua Leargais on lenition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.daltai.com/forums/viewthread/707/"&gt;More about not leniting Pádraig after féile&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="irish"></category><category term="ireland"></category></entry><entry><title>temporal anomaly</title><link href="/meta/2015-03-16-temporal-anomaly.html" rel="alternate"></link><published>2015-03-16T00:00:00+00:00</published><updated>2015-03-16T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2015-03-16:/meta/2015-03-16-temporal-anomaly.html</id><summary type="html">&lt;p&gt;I'm gradually migrating old posts over from my &lt;a href="https://jackofalljacks.wordpress.com/"&gt;WordPress blog&lt;/a&gt;, which will be backdated, ruining the timestamps-through-commits system that could have been, but never was. Hopefully the validity of the alleged timestamps on these posts will never be important for anything.&lt;/p&gt;
&lt;p&gt;Comparing the new post&lt;a href="#1"&gt;[1]&lt;/a&gt; with the old also …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm gradually migrating old posts over from my &lt;a href="https://jackofalljacks.wordpress.com/"&gt;WordPress blog&lt;/a&gt;, which will be backdated, ruining the timestamps-through-commits system that could have been, but never was. Hopefully the validity of the alleged timestamps on these posts will never be important for anything.&lt;/p&gt;
&lt;p&gt;Comparing the new post&lt;a href="#1"&gt;[1]&lt;/a&gt; with the old also reminds me that Wordpress has competent web designers, words which cannot be applied to me. However, my blog has the advantage of working just as well(badly) without JavaScript, which is part of my decision to design it with &lt;a href="https://www.torproject.org/projects/torbrowser.html.en"&gt;Tor Browser&lt;/a&gt; in mind.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a name="1"&gt;[1]&lt;/a&gt; At the time of writing this I had just migrated &lt;a href="{static}/2014-09-30-updating-shared-variables-in-theano.md" class="ml"&gt;"updating shared variables in theano"&lt;/a&gt;.&lt;/p&gt;</content><category term="wordpress"></category><category term="time travel"></category><category term="design"></category><category term="tor"></category><category term="webdev"></category></entry><entry><title>databases all the way down</title><link href="/biomed/2015-02-12-databases.html" rel="alternate"></link><published>2015-02-12T14:20:00+00:00</published><updated>2015-02-12T14:20:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2015-02-12:/biomed/2015-02-12-databases.html</id><summary type="html">&lt;p&gt;I'm (arguably) a data scientist, so I need data to do science. A problem with data is that it's all over the place, a natural and unsurprising consequence of its many origins. Producing databases seems to be a hobby for bioinformaticians, which also includes &lt;a href="http://metadatabase.org/wiki/Main_Page"&gt;databases of databases&lt;/a&gt; (&lt;a href="http://nar.oxfordjournals.org/content/40/D1/D1250.long"&gt;Bolser et al …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm (arguably) a data scientist, so I need data to do science. A problem with data is that it's all over the place, a natural and unsurprising consequence of its many origins. Producing databases seems to be a hobby for bioinformaticians, which also includes &lt;a href="http://metadatabase.org/wiki/Main_Page"&gt;databases of databases&lt;/a&gt; (&lt;a href="http://nar.oxfordjournals.org/content/40/D1/D1250.long"&gt;Bolser et al., 2012&lt;/a&gt;), so I think it's fair to say some nontrivial effort is expended in trying to deal with this data problem. Large collaborations are pretty good for creating big datasets (e.g. &lt;a href="http://genome.ucsc.edu/ENCODE/downloads.html"&gt;ENCODE&lt;/a&gt;, &lt;a href="https://tcga-data.nci.nih.gov/tcga/"&gt;TCGA&lt;/a&gt;), which can lessen the appearance of scattered data (or at least, heighten the attractiveness of centralised data), but these efforts are less about data curation and more about data generation. One notable effort towards more efficient (genomic) data discovery/sharing is the &lt;a href="http://genomicsandhealth.org/"&gt;Global Alliance for Genomics and Health&lt;/a&gt;, which is working on defining standards, data formats, APIs, things like that. It's not the only project thinking about APIs and formats (of course). Skimming my notes from the &lt;a href="http://meetings.cshl.edu/meetings/2014/data14.shtml"&gt;Biological Data Science&lt;/a&gt; meeting at CSHL last year, I see: &lt;a href="http://osdf.igs.umaryland.edu/"&gt;Open Science Data Framework&lt;/a&gt;, the NIH's &lt;a href="https://biocaddie.org/"&gt;bioCADDIE&lt;/a&gt;, Ensembl's &lt;a href="http://rest.ensembl.org/"&gt;REST API&lt;/a&gt;, and so on&lt;a href="#1"&gt;[1]&lt;/a&gt;. For the sake of efficiency I hope the community can come to some consensus on how best to store/index data and metadata, although a quote from Richard Durbin comes to mind&lt;a href="#2"&gt;[2]&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"In science, always there are lots of people looking at the same thing in different ways. There are people trying out all sorts of crazy things. It's extremely successful to not have top-down control. It can look a little bit redundant when you have a person write yet another read mapper, but sometimes things will be influential. New ideas will come. Sometimes things can be relevant to individual projects. I think for sure things are done inefficiently. I accept that. It's a bit like evolution. Random mutation and testing is very powerful." &lt;a href="http://www.nature.com/nbt/journal/v31/n10/full/nbt.2721.html"&gt;The anatomy of successful computational biology software (Nature Biotech, 2013)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Which is to say I'm excessively justifying my decision to create yet another list of resources (but just a list, I have no intention of actually &lt;em&gt;serving&lt;/em&gt; any data). In this case it's something I wanted to do for myself anyway, and &lt;em&gt;more importantly&lt;/em&gt; I'm starting a new category of non-blog pages on my site (although technically the &lt;a href="https://corcra.github.io/contact/"&gt;contact&lt;/a&gt; page was first). The idea is to separate pages which I feel are more time-insensitive (like tutorials), or which I intend to keep updated (like my contact details), and treat things on the blog as an unmodifiable record that will likely become outdated. So &lt;a href="https://corcra.github.io/datasets.html"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt; is my dataset database. It covers 'things which are relevant to me', which you might find useful, if you're me. It might also be useful for people using convoluted methods to infer my research interests.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a name="1"&gt;[1]&lt;/a&gt; It's reasonably likely these are not equivalent projects. I'm not really familiar with any of them, so YMMV.&lt;/p&gt;
&lt;p&gt;&lt;a name="2"&gt;[2]&lt;/a&gt; Duplication of effort is something I think about a lot in the context of bioinformatics/computational biology/computers. This quote makes me feel better about it.&lt;/p&gt;</content><category term="biology"></category><category term="medicine"></category><category term="data"></category><category term="links"></category><category term="meta"></category></entry><entry><title>into the gui pond</title><link href="/sec/2015-02-03-pond-with-gui.html" rel="alternate"></link><published>2015-02-03T13:37:00+00:00</published><updated>2015-02-03T13:37:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2015-02-03:/sec/2015-02-03-pond-with-gui.html</id><summary type="html">&lt;p&gt;In my &lt;a href="https://corcra.github.io/sec/2015/01/30/pond-on-yosemite.html"&gt;previous post&lt;/a&gt; about getting &lt;a href="https://pond.imperialviolet.org/"&gt;Pond&lt;/a&gt; running on Yosemite, I ran into an issue with the GUI. The CLI interface seems to be fully-functional and pleasant enough for me, but GUI-errors are no guid. So yesterday I tried to figure out what the problem was, and somehow fixed it …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my &lt;a href="https://corcra.github.io/sec/2015/01/30/pond-on-yosemite.html"&gt;previous post&lt;/a&gt; about getting &lt;a href="https://pond.imperialviolet.org/"&gt;Pond&lt;/a&gt; running on Yosemite, I ran into an issue with the GUI. The CLI interface seems to be fully-functional and pleasant enough for me, but GUI-errors are no guid. So yesterday I tried to figure out what the problem was, and somehow fixed it in the process. Naturally I was not keeping detailed logs.&lt;/p&gt;
&lt;p&gt;It went something like this.&lt;/p&gt;
&lt;h3&gt;gtk3&lt;/h3&gt;
&lt;p&gt;First, I made sure I could run something else using GTK, so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; gtk3-demo
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This technically ran, but produced a lot of warnings and errors, e.g.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(gtk3-demo:99098): Gtk-WARNING **: Error loading theme icon &amp;#39;image-missing&amp;#39; for stock: Icon &amp;#39;image-missing&amp;#39; not present in theme
(gtk3-demo:99098): GLib-GObject-CRITICAL **: g_object_ref: assertion &amp;#39;G_IS_OBJECT (object)&amp;#39; failed
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which is possibly a related problem to the one I had with Pond, but given Pond now works for me, and I still get these errors, I suspect they have different origins/are not directly causally related.&lt;/p&gt;
&lt;h3&gt;becoming one with the error&lt;/h3&gt;
&lt;p&gt;So I tried to understand the actual error I was getting (from &lt;a href="https://corcra.github.io/assets/pond-panic.txt"&gt;this log&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;panic: (*gdkpixbuf._Ctype_struct__GError) (0x4383380,0x5c6be20)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This involved following the golden braid &lt;a href="https://github.com/agl/pond/blob/master/client/gtk.go"&gt;deeper&lt;/a&gt; into &lt;a href="https://github.com/agl/go-gtk/blob/master/gdkpixbuf/gdkpixbuf.go"&gt;the&lt;/a&gt; inner &lt;a href="https://git.gnome.org/browse/gdk-pixbuf/tree/gdk-pixbuf/gdk-pixbuf-loader.c"&gt;workings&lt;/a&gt; of &lt;code&gt;go&lt;/code&gt; &lt;a href="https://git.gnome.org/browse/gdk-pixbuf/tree/gdk-pixbuf/gdk-pixbuf-io.c"&gt;and&lt;/a&gt; &lt;code&gt;gdk-pixbuf&lt;/code&gt;, in the hopes that something would start to make sense.&lt;/p&gt;
&lt;p&gt;Clarity was not forthcoming. I got to look at non-scientific code for a while, and I decided &lt;code&gt;gdk-pixbuf&lt;/code&gt; was to blame, but otherwise learned little. This isn't terribly surprising given my lack of knowledge of Go/gtk/OSX/etc. I tried. At least I had a witch (&lt;code&gt;gdk-pixbuf&lt;/code&gt;) to burn.&lt;/p&gt;
&lt;h3&gt;fun with reinstalling everything repeatedly&lt;/h3&gt;
&lt;p&gt;I thought about trying to test &lt;code&gt;gdkpixbuf.PixbufLoaderWithType("png")&lt;/code&gt; on its own, but I was lazy and foolish, so I just tried reinstalling &lt;code&gt;gdk-pixbuf&lt;/code&gt; instead.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew install gdk-pixbuf
/usr/local/Cellar/gdk-pixbuf/2.30.8/bin/gdk-pixbuf-query-loaders --update-cache
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I think this is when I introduced a new problem, thought 'damn, OK I'll just fix this problem and get back to carefully recording how I fix this GUI error', and in the process fixed the GUI. Naturally. The problem I introduced looked like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;&amp;gt; client&lt;/span&gt;

&lt;span class="x"&gt;dyld: Library not loaded: /usr/local/lib/libgdk_pixbuf-2.0.0.dylib&lt;/span&gt;
&lt;span class="x"&gt;  Referenced from: [&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;GOPATH&lt;/span&gt;&lt;span class="x"&gt;]/bin/client&lt;/span&gt;
&lt;span class="x"&gt;    Reason: Incompatible library version: client requires version 3101.0.0 or later, but libgdk_pixbuf-2.0.0.dylib provides version 3001.0.0&lt;/span&gt;
&lt;span class="x"&gt;    Trace/BPT trap: 5&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Which looks suspiciously like a version got messed up, maybe because I installed &lt;code&gt;gdk-pixbuf&lt;/code&gt; individually, maybe because of magic. So I then uninstalled it, reinstalled &lt;code&gt;gtk+3&lt;/code&gt; (which pulled &lt;code&gt;gdk-pixbuf&lt;/code&gt; down first; error persisted), reinstalled &lt;code&gt;gtkspell3&lt;/code&gt;, all to no avail. Then I did a &lt;code&gt;brew cleanup&lt;/code&gt; and it deleted some old versions and cached bottles. Determined to somehow update my &lt;code&gt;gdk-pixbuf&lt;/code&gt; (naively believing that the newest version of a thing must be the optimal one), I explored:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; brew info gdk-pixbuf

gdk-pixbuf: stable 2.30.8 (bottled)
http://gtk.org
/usr/local/Cellar/gdk-pixbuf/2.30.8 (209 files, 4.3M) *
  Poured from bottle
From: https://github.com/Homebrew/homebrew/blob/master/Library/Formula/gdk-pixbuf.rb
==&amp;gt; Dependencies
Build: xz ✘, pkg-config ✔
Required: glib ✔, jpeg ✔, libtiff ✔, libpng ✔, gobject-introspection ✔
==&amp;gt; Options
--universal
    Build a universal binary
==&amp;gt; Caveats
Programs that require this module need to set the environment variable
  export GDK_PIXBUF_MODULEDIR=&amp;quot;/usr/local/lib/gdk-pixbuf-2.0/2.10.0/loaders&amp;quot;
If you need to manually update the query loader cache, set GDK_PIXBUF_MODULEDIR then run
  /usr/local/Cellar/gdk-pixbuf/2.30.8/bin/gdk-pixbuf-query-loaders --update-cache
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;maximum &lt;code&gt;gdk-pixbuf&lt;/code&gt; installation&lt;/h3&gt;
&lt;p&gt;I don't feel proud of this, but I liked the sound of a 'universal binary' so I went for that.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; brew reinstall --universal gdk-pixbuf

==&amp;gt; Installing dependencies for gdk-pixbuf: sqlite, gdbm, makedepend, openssl, python, xz, gettext, glib, jpeg, libtiff, libpng, gobject-intros
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;. . . But then it was taking too long, so I cancelled it during &lt;code&gt;gettext&lt;/code&gt;. &lt;em&gt;I'm so, so sorry.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The next event I have recorded in my logs was me &lt;em&gt;uninstalling&lt;/em&gt; &lt;code&gt;gdk-pixbuf&lt;/code&gt; and &lt;code&gt;gtk+3&lt;/code&gt;, then reinstalling &lt;code&gt;gtk+3&lt;/code&gt;. All for good measure. I then tried to cleanly reinstall pond, so&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; &lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="nv"&gt;$GOPATH&lt;/span&gt;
&amp;gt; rm -r *
&amp;gt; go get github.com/agl/pond/client
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;. . . and . . &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; client
Feb  &lt;span class="m"&gt;1&lt;/span&gt; 02:03:50: Starting fetch from home server
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It Just Worked.&lt;/p&gt;
&lt;p&gt;I was hoping the &lt;code&gt;dylib&lt;/code&gt; problem would go away and I could return to my GUI issues, but nope. All fixed. I tried uninstalling things (&lt;code&gt;gtk+3&lt;/code&gt;, &lt;code&gt;go&lt;/code&gt;, &lt;code&gt;gtkspell3&lt;/code&gt;, &lt;code&gt;mercurial&lt;/code&gt;, &lt;code&gt;gdk-pixbuf&lt;/code&gt;, &lt;code&gt;pond&lt;/code&gt;) in an attempt to recreate the original (or subsequent) issue(s), but it refused to be broken. I have a list of all the dependencies I installed during this fiasco, so I could &lt;em&gt;in theory&lt;/em&gt; spend more time trying to break it again, or I could not.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://beesandbombs.tumblr.com/post/106618422479/funnel"&gt;at the end of the magical vortex, a thousand bald yaks&lt;/a&gt;&lt;br&gt;
&lt;a href="http://beesandbombs.tumblr.com/post/106618422479/funnel"&gt;&lt;img src="http://38.media.tumblr.com/2671206c90dbe899d8bcd2116cbd7cb4/tumblr_nhem70bBYT1r2geqjo1_500.gif" style="width: 40vw;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Incidentally, after all of this, it transpires that that One Weird Trick:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; gdk-pixbuf-query-loaders &amp;gt; &lt;span class="nv"&gt;$GOPATH&lt;/span&gt;/lib/gdk-pixbuf-2.0/2.10.0/loaders.cache
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;is no longer required for &lt;code&gt;client&lt;/code&gt; to run. So that's fewer files and more GUIs, what could be better?&lt;/p&gt;</content><category term="osx"></category><category term="golang"></category><category term="pond"></category><category term="anonymity"></category><category term="gtk"></category></entry><entry><title>setting up pond on yosemite</title><link href="/sec/2015-01-30-pond-on-yosemite.html" rel="alternate"></link><published>2015-01-30T00:00:00+00:00</published><updated>2015-01-30T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2015-01-30:/sec/2015-01-30-pond-on-yosemite.html</id><summary type="html">&lt;p&gt;This is about &lt;a href="https://pond.imperialviolet.org/"&gt;Pond&lt;/a&gt;. If you have no idea what that is but don't want to click on the link, I shall quote directly;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Pond is not email. Pond is forward secure, asynchronous messaging for the discerning. Pond messages are asynchronous, but are not a record; they expire automatically a …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is about &lt;a href="https://pond.imperialviolet.org/"&gt;Pond&lt;/a&gt;. If you have no idea what that is but don't want to click on the link, I shall quote directly;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Pond is not email. Pond is forward secure, asynchronous messaging for the discerning. Pond messages are asynchronous, but are not a record; they expire automatically a week after they are received. Pond seeks to prevent leaking traffic information against everyone except a global passive attacker.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If that seems interesting, go back and click on the link. Otherwise get out while you still can.&lt;/p&gt;
&lt;p&gt;My goal here is to get Pond running. I unfortunately don't have any Pond secrets to hand right now, so testing its messaging functionality will have to come later. The instructions on the main Pond page for OSX are fairly good, so this is just a slight elaboration/modification on those. This is going to look longer than it should because I haven't solved formatting code snippets on this blog yet.&lt;/p&gt;
&lt;h2&gt;System&lt;/h2&gt;
&lt;p&gt;I'm using a ~2012 Macbook Pro 13inch Retina blah blah, running OSX Yosemite (version 10.10.1). It's a fairly recent install, so I haven't had time to ruin everything yet. That said, I have &lt;a href="https://www.torproject.org/projects/torbrowser.html.en"&gt;Tor Browser&lt;/a&gt; installed and running. Obviously.&lt;/p&gt;
&lt;h2&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;GUI is b0rked, CLI seems to work. From &lt;a href="https://pond.imperialviolet.org/"&gt;the site&lt;/a&gt;, for a CLI-version:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; brew install go
&amp;gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;GOPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;/gopkg
&amp;gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$PATH&lt;/span&gt;:&lt;span class="nv"&gt;$GOPATH&lt;/span&gt;/bin
&amp;gt; go get -tags nogui github.com/agl/pond/client
&amp;gt; &lt;span class="nb"&gt;alias&lt;/span&gt; &lt;span class="nv"&gt;pond&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$GOPATH&lt;/span&gt;&lt;span class="s2"&gt;/bin/client&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;But now for &lt;em&gt;how I spent my Friday evening:&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Dependencies&lt;/h2&gt;
&lt;h3&gt;Go&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; brew install go
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Painless success, version: go1.4.1 darwin/amd64. Then make a folder for &lt;a href="http://golang.org/doc/code.html#GOPATH"&gt;Go packages&lt;/a&gt;,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; mkdir &lt;span class="nv"&gt;$HOME&lt;/span&gt;/gopkg
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Export some environment variables to keep Go happy, (I also added these to my .bash_profile)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;GOPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;/gopkg
&amp;gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$PATH&lt;/span&gt;:&lt;span class="nv"&gt;$GOPATH&lt;/span&gt;/bin
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One more for good measure,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PKG_CONFIG_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/opt/X11/lib/pkgconfig:/usr/local/lib/pkgconfig:&lt;span class="nv"&gt;$PKG_CONFIG_PATH&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;gtk+3&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(These other dependencies are probably a bit pointless until I figure out how to get the GUI to reliably work, but for the sake of completeness they are here.)&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; brew install gtk+3
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It complains that it needs XQuartz 2.5, so that is resolved (as it suggests) with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; brew install Caskroom/cask/xquartz
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;gtkspell3&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; brew install gtkspell3
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This installed a whole pile of dependencies (including, concerningly, Python: the number of Pythons I have installed grows by the day), and my laptop crashed shortly after they finished, but I think that was random.&lt;/p&gt;
&lt;h3&gt;mercurial&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; brew install mercurial
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Pond&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; go get github.com/agl/pond/client
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It produces lots of warnings, but seemingly no errors. And who reads warnings? Since it's already in my path (see above), running it is simply (I think this could do with being more descriptive):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; client
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Disaster strikes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt;Dynamic session lookup supported but failed: launchd did not provide a socket path, verify that org.freedesktop.dbus-session.plist is loaded!
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt;&lt;span class="o"&gt;(&lt;/span&gt;&amp;lt;unknown&amp;gt;:917&lt;span class="o"&gt;)&lt;/span&gt;: GdkPixbuf-WARNING **: Cannot open pixbuf loader module file &lt;span class="s1"&gt;&amp;#39;[$GOPATH]/bin/../lib/gdk-pixbuf-2.0/2.10.0/loaders.cache&amp;#39;&lt;/span&gt;: No such file or directory

&amp;gt;This likely means that your installation is broken.
&amp;gt;Try running the &lt;span class="nb"&gt;command&lt;/span&gt; gdk-pixbuf-query-loaders &amp;gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;$GOPATH&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;/bin/../lib/gdk-pixbuf-2.0/2.10.0/loaders.cache to make things work again &lt;span class="k"&gt;for&lt;/span&gt; the &lt;span class="nb"&gt;time&lt;/span&gt; being.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(I'm using rectangular brackets to denote that I manually replaced its output with the logical meaning, so you don't have to look at my home directory path :)&lt;/p&gt;
&lt;h3&gt;dbus&lt;/h3&gt;
&lt;p&gt;First problem is where in the hell is &lt;code&gt;org.freedesktop.dbus-session.plist&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;(it's not in in /Library/LaunchAgents or /Library/LaunchDaemons, or ~/Library/LaunchAgents)&lt;/p&gt;
&lt;p&gt;The answer, if you &lt;code&gt;brew reinstall dbus&lt;/code&gt; to find out (or use some other kind of sorcery) is:&lt;/p&gt;
&lt;p&gt;To have launchd start d-bus at login:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ln -sfv /usr/local/opt/d-bus/*.plist ~/Library/LaunchAgents
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That's where it is.&lt;/p&gt;
&lt;p&gt;Then to load d-bus now:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;launchctl load ~/Library/LaunchAgents/org.freedesktop.dbus-session.plist
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;pixbuf&lt;/h3&gt;
&lt;p&gt;So the folder it's expecting (&lt;code&gt;lib/gdk-pixbuf...&lt;/code&gt;) is certainly not found at &lt;code&gt;[$GOPATH]/bin/...&lt;/code&gt;, since that's just &lt;code&gt;$GOPATH&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;By running &lt;code&gt;gdk-pixbuf-query-loaders&lt;/code&gt; I can see it's probably expecting this path or something:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/usr/local/Cellar/gdk-pixbuf/2.31.2/lib/gdk-pixbuf-2.0/2.10.0/loaders/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Which would only be appropriate if &lt;code&gt;$GOPATH=/usr/local/Cellar/gdk-pixbuf/2.31.2/&lt;/code&gt;, which it is not by construction. I can get around this as it suggests by creating a bunch of empty folders and then running&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; gdk-pixbuf-query-loaders &amp;gt; &lt;span class="nv"&gt;$GOPATH&lt;/span&gt;/lib/gdk-pixbuf-2.0/2.10.0/loaders.cache
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Which gets rid of the warnings. Now when I run&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; client
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I get no (immediate) errors! Success! Maybe!&lt;/p&gt;
&lt;h2&gt;GUI-sadness&lt;/h2&gt;
&lt;p&gt;Empowered by the graphics, I set a password and create an account on the default server.&lt;/p&gt;
&lt;p&gt;. . . and it &lt;em&gt;panics&lt;/em&gt;. That is to say, the GUI vanishes (although X11 is still running) and I get a &lt;a href="https://corcra.github.io/assets/pond-panic.txt"&gt;lot of errors&lt;/a&gt;. Not only that, but now if I try to run it again,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; client
    Jan &lt;span class="m"&gt;30&lt;/span&gt; 23:46:58: Fatal error: state file is too small to be valid
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Shit.&lt;/p&gt;
&lt;p&gt;Maybe I just need the gentle embrace of the command line?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; client -cli&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;

&amp;gt;&amp;gt;&amp;gt; Pond...
&amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; state file is too small to be valid
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Abort, abort!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; rm &lt;span class="nv"&gt;$HOME&lt;/span&gt;/.pond
&amp;gt; client -cli&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And now, everything is beautiful and nothing hurts.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://corcra.github.io/assets/pond_screenshot.png" style="width: 40vw; border:5px solid #000000;"/&gt;&lt;/p&gt;</content><category term="osx"></category><category term="golang"></category><category term="pond"></category><category term="anonymity"></category></entry><entry><title>first post</title><link href="/meta/2015-01-26-first-post.html" rel="alternate"></link><published>2015-01-26T00:00:00+00:00</published><updated>2015-01-26T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2015-01-26:/meta/2015-01-26-first-post.html</id><summary type="html">&lt;p&gt;Does anyone remember their first website? I know I was a nine-year-old with Microsoft Word and "Save as HTML", which seems like a decent reason to forget it, even if it was Pokemon-themed. Given my complete lack of gainful employment at the time, I took to website-making as if it …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Does anyone remember their first website? I know I was a nine-year-old with Microsoft Word and "Save as HTML", which seems like a decent reason to forget it, even if it was Pokemon-themed. Given my complete lack of gainful employment at the time, I took to website-making as if it were important, as if its utility would not soon be eclipsed by the simplicity afforded by services like Livejournal and Bebo&lt;a href="#1"&gt;[1]&lt;/a&gt;. I spent many hours refining and redefining my vision, a vision comprised mostly of iframes.&lt;/p&gt;
&lt;p&gt;Here comes one now:
&lt;a href="http://media.ccc.de/browse/congress/2014/31c3_-_6373_-_en_-_saal_6_-_201412291600_-_the_only_thing_we_know_about_cyberspace_is_that_its_640x480_-_olia_lialina.html"&gt;The Only Thing We Know About Cyberspace Is That Its 640x480 (Olia Lialina @31C3)&lt;/a&gt;&lt;/p&gt;
&lt;iframe width="670" height="380" src="http://media.ccc.de/browse/congress/2014/31c3_-_6373_-_en_-_saal_6_-_201412291600_-_the_only_thing_we_know_about_cyberspace_is_that_its_640x480_-_olia_lialina/oembed.html" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;This talk is largely about the GeoCities era of personal websites, which was already on the way out when I was getting started, but things were different in Ireland anyway. We had 'Ireland On-Line', which is now a five-line &lt;a href="https://en.wikipedia.org/wiki/Ireland_On-Line"&gt;Wikipedia article&lt;/a&gt;, a legacy webmail service and a mausoleum of personal pages and auto-redirects.&lt;/p&gt;
&lt;p&gt;Selected snippets:&lt;/p&gt;
&lt;p&gt;&lt;i&gt;optimised for Netscape 4.70&lt;/i&gt; &lt;a href="http://www.iol.ie/~taeger/"&gt;[source]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src='http://www.iol.ie/~gscoil/images/netgaeilge.gif'&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;What's cool: Gameboy, playstation, me, Doom.&lt;/i&gt; &lt;a href="http://www.iol.ie/~gscoil/darlingtons/zac/index.htm"&gt;[source]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;While as yet, SnoopDos exists only on the Amiga, I'm happy to report that similar utilities have sprung up for DOS, Windows 3.1, Windows 95, and Windows NT. &lt;/i&gt; &lt;a href="http://www.iol.ie/~ecarroll/snoopdos.html"&gt;[source]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Divorce, in the Church's view, threatens to deconstruct the primary unit upon which the model for the Church is built. The battle for the retention of the form of the traditional family becomes the battle for the retention of a particular model of church organisation. This is why the debate is so fiercely fought.&lt;/i&gt;&lt;a href="#2"&gt;[2]&lt;/a&gt; &lt;a href="http://www.iol.ie/~mazzoldi/toolsforchange/zine/sam94/divorce.html"&gt;[source]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;"'Yo motherfucker,' Marx greeted. Proudhon was at a loss."&lt;/i&gt; &lt;a href="http://www.iol.ie/~mazzoldi/toolsforchange/marx/marx7.html"&gt;[source]&lt;/a&gt; (somehow related to previous one)&lt;/p&gt;
&lt;p&gt;&lt;i&gt;The genesis of a new British police force can be seen in Mowlam's proposal to introduce the subtitle 'Northern Ireland Police Service' (NIPS) to the RUC.&lt;/i&gt;&lt;a href="#3"&gt;[3]&lt;/a&gt; &lt;a href="iol.ie/~saoirse/iris/1997/225.htm"&gt;[source]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cutting that unexpectedly-engrossing tangent short, I'll clarify that this site is not a nostalgia trip. I will be keeping the animated gifs to a minimum (unless they come from &lt;a href="http://beesandbombs.tumblr.com/"&gt;David Whyte&lt;/a&gt;) and attempting to adhere to sane design principles. The talk I linked planted a seed of sorts in me, though. I can't argue that it caused this site, because it's been in the works for a while, but it catalysed me. &lt;/p&gt;
&lt;p&gt;The seed is roughly this: In a time when one's internet presence was a deliberate act rather than a social necessity, making a website and appearing on cyberspace&lt;a href="#4"&gt;[4]&lt;/a&gt; was preceded by a question of surprisingly existential nature: &lt;/p&gt;
&lt;p&gt;&lt;i&gt;"What will I make my website about? What do I have to say?"&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;I believe that one's web presence today has similar associated questions (notably "what image do I wish to portray?"), but thanks to the existence of a set of well-established social norms, a person can largely ignore them. It is entirely possible to exist online without asking any existentalist questions. This is arguably great, because the internet is fantastic and should not be restricted to a set of people who find it necessary to frame their actions as manifestations of the struggle for self-definition, and/or people who know HTML.&lt;/p&gt;
&lt;p&gt;I think I fall somewhere in the 'and' category.&lt;/p&gt;
&lt;p&gt;So I'm making a website for a few reasons, which are secretly the same reason&lt;a href="#5"&gt;[5]&lt;/a&gt;. I like thinking about what I have to say. I think questions of that nature are interesting and can be quite personally fulfilling, even if the answer turns out to be 'mostly nothing'. I also like thinking about how to say things, which is part of the reason this entry has taken me far too long to finish&lt;a href="#6"&gt;[6]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There is also the careful deliberateness which goes into creating a website like this. My CSS was completely nonexistent when I started this a few weeks ago.  I'd recommend against reading the source code if you are sensitive about CSS, because terrible, likely-forbidden things have happened and I am remorseless. Things will only get worse as I enact my weird design plans on other parts of the site&lt;a href="#7"&gt;[7]&lt;/a&gt;. I cannot wait.&lt;/p&gt;
&lt;p&gt;So yeah, I could run a blog on an existing service, but where would be the fun in that?&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a name="1"&gt;[1]&lt;/a&gt; Bebo became extremely popular in Ireland when I was about 15, and suddenly using the internet became socially acceptable.&lt;/p&gt;
&lt;p&gt;&lt;a name="2"&gt;[2]&lt;/a&gt; I think this is particularly interesting, because Ireland finds itself (a mere 20 years later) on the precipice of another historic referendum (this time to legalise gay marriage), but the church makes the same old arguments. Society marches ever onwards, and the church is immobile.&lt;/p&gt;
&lt;p&gt;&lt;a name="3"&gt;[3]&lt;/a&gt; They went with PSNI in the end, presumably in deference to &lt;a href="http://nips.cc/"&gt;NIPS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name="4"&gt;[4]&lt;/a&gt; Before it became infested with cybercriminals and c y b  e    r w a r  f   a     r        e.&lt;/p&gt;
&lt;p&gt;&lt;a name="5"&gt;[5]&lt;/a&gt; Proof left as exercise to the reader.&lt;/p&gt;
&lt;p&gt;&lt;a name="6"&gt;[6]&lt;/a&gt; Also fighting a losing battle against pre-ironic post-irony creeping into my tone.&lt;/p&gt;
&lt;p&gt;&lt;a name="7"&gt;[7]&lt;/a&gt; I intend to keep the blog as minimalist as is palatable.&lt;/p&gt;</content><category term="ccc"></category><category term="webdev"></category><category term="nostalgia"></category></entry><entry><title>updating shared variables in theano</title><link href="/ml/2014-09-30-updating-shared-variables-in-theano.html" rel="alternate"></link><published>2014-09-30T00:00:00+01:00</published><updated>2014-09-30T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2014-09-30:/ml/2014-09-30-updating-shared-variables-in-theano.html</id><summary type="html">&lt;p&gt;Background: I am running python with &lt;a href="http://deeplearning.net/software/theano/index.html"&gt;Theano&lt;/a&gt; on a GPU, and I care about speed.&lt;/p&gt;
&lt;p&gt;Scenario: I have a largeish matrix (&lt;code&gt;C&lt;/code&gt;) which is stored as a shared variable, and I need to update a subset of the rows (&lt;code&gt;modified_rows&lt;/code&gt;) by some other matrix (&lt;code&gt;C_delta&lt;/code&gt;). What should I do?&lt;/p&gt;
&lt;p&gt;Initialising …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Background: I am running python with &lt;a href="http://deeplearning.net/software/theano/index.html"&gt;Theano&lt;/a&gt; on a GPU, and I care about speed.&lt;/p&gt;
&lt;p&gt;Scenario: I have a largeish matrix (&lt;code&gt;C&lt;/code&gt;) which is stored as a shared variable, and I need to update a subset of the rows (&lt;code&gt;modified_rows&lt;/code&gt;) by some other matrix (&lt;code&gt;C_delta&lt;/code&gt;). What should I do?&lt;/p&gt;
&lt;p&gt;Initialising, e.g.:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;  
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;theano&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shared&lt;/span&gt;  
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;theano.tensor&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fmatrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ivector&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;set_subtensor&lt;/span&gt;  
    &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;70000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;  
    &lt;span class="n"&gt;modified_rows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;low&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;high&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;70000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
    &lt;span class="n"&gt;C_delta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;modified_rows&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
    &lt;span class="n"&gt;C_d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fmatrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;C_delta&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
    &lt;span class="n"&gt;mod_rows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ivector&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;modified_rows&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Slow method: manually reset the values:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    C_temp = C.get_value()
    C_temp[modified_rows, :] = C_temp[modified_rows, :] + C_delta
    C.set_value(C_temp)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Speed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    32 function calls in 0.055 seconds
    ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    ...
    1 0.000 0.000 0.026 0.026 sharedvalue.py:100(set_value)
    1 0.000 0.000 0.027 0.027 sharedvalue.py:80(get_value)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is bad because it requires unpacking and repacking the value in the shared variable (via &lt;code&gt;get_value&lt;/code&gt; and &lt;code&gt;set_value&lt;/code&gt;). We only need to modify a small number of the rows (200 out of 70k) so having to update every single one seems extremely wasteful.&lt;/p&gt;
&lt;p&gt;Part of the 'nice thing' about shared variables is that they can be updated by functions which use them, so you might try:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    update_C = function([C_d, mod_rows], [],
                        updates=[(C[mod_rows, :], C[mod_rows, :] + C_d)], 
                        allow_input_downcast=True)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(remember, &lt;code&gt;C_d&lt;/code&gt; and &lt;code&gt;mod_rows&lt;/code&gt; are &lt;em&gt;symbolic variables&lt;/em&gt; (specifically &lt;code&gt;fmatrix&lt;/code&gt; and ivector) defined above).
The &lt;code&gt;allow_input_downcast=True&lt;/code&gt; will deal with numpy's love of dealing in double-precision floats, which Theano rejects for GPU work. This loss of precision &lt;em&gt;may&lt;/em&gt; be important to you.&lt;/p&gt;
&lt;p&gt;So then a simple call to &lt;code&gt;update_C(C_delta, modified_rows)&lt;/code&gt; will do what you want, except that what I just wrote won't work. You can't update shared variables like that. I think it's because the first element of the tuple is not &lt;em&gt;really&lt;/em&gt; the shared variable, so Theano freaks out. (Full disclosure: little idea of theano's inner workings.)&lt;/p&gt;
&lt;p&gt;Focusing solely on the &lt;code&gt;updates=[...]&lt;/code&gt; part (everything else should be OK), you need to do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    updates = [(C, set_subtensor(C[modified_rows], C_delta))]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So the full command (if you are lazily copying and pasting this into iPython to test speed):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    update_C = function([C_d, mod_rows], [],  
                        updates=[(C, set_subtensor(C[mod_rows, :] + C_d))],  
                        allow_input_downcast=True)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Things which won't work (for reasons unknown to me):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    C[modified_rows] -&amp;gt; C[modified_rows, :]
    C_delta -&amp;gt; C_delta[:, :]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As for the speed for this method: well,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    28 function calls in 0.001 seconds
    ncalls tottime percall cumtime percall filename:lineno(function)
    ...
    1 0.001 0.001 0.001 0.001 subtensor.py:1644(perform)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I think that solves the problem.&lt;/p&gt;
&lt;p&gt;Related:&lt;br&gt;
&lt;a href="https://stackoverflow.com/questions/24229361/theano-indexing-inside-a-compiled-function-gpu"&gt;https://stackoverflow.com/questions/24229361/theano-indexing-inside-a-compiled-function-gpu&lt;/a&gt;      &lt;br&gt;
&lt;a href="https://stackoverflow.com/questions/15917849/how-can-i-assign-update-subset-of-tensor-shared-variable-in-theano"&gt;https://stackoverflow.com/questions/15917849/how-can-i-assign-update-subset-of-tensor-shared-variable-in-theano&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Possibly relevant technical information:&lt;br&gt;
&lt;em&gt;Theano version is 0.6.0.&lt;br&gt;
Numpy version is 1.8.2, using Intel's Math Kernel Library (MKL) as part of Anaconda.&lt;br&gt;
GPU is a GeForce GTX 680.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; this post first appeared on my &lt;a href="https://jackofalljacks.wordpress.com/"&gt;wordpress blog.&lt;/a&gt;&lt;/p&gt;</content><category term="python"></category><category term="theano"></category><category term="gpu"></category><category term="optimisation"></category></entry><entry><title>deletion and replacement of strings in bash</title><link href="/tips/2013-12-24-strings-in-bash.html" rel="alternate"></link><published>2013-12-24T00:00:00+00:00</published><updated>2013-12-24T00:00:00+00:00</updated><author><name>corcra</name></author><id>tag:None,2013-12-24:/tips/2013-12-24-strings-in-bash.html</id><summary type="html">&lt;p&gt;I try to record useful one-liners for future reference. I forgot to write down what this one does:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    mv &lt;span class="nv"&gt;$f&lt;/span&gt; &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="c1"&gt;##START&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;.&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="c1"&gt;##START&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This awkwardly-timed (time-zone troubles) blog post is atonement for my carelessness. Like all one-liners it looks complicated but is pretty simple. It does this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &amp;gt; f …&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;I try to record useful one-liners for future reference. I forgot to write down what this one does:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    mv &lt;span class="nv"&gt;$f&lt;/span&gt; &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="c1"&gt;##START&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;.&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="c1"&gt;##START&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This awkwardly-timed (time-zone troubles) blog post is atonement for my carelessness. Like all one-liners it looks complicated but is pretty simple. It does this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &amp;gt; f=STARTonetwothree 
    &amp;gt; echo &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="c1"&gt;##START&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;.&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="c1"&gt;##START&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;}
    onetwothree.START
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;${...}&lt;/code&gt; construction is brace expansion, which allows us to generate strings. In stages we have&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="c1"&gt;##START&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This takes the string &lt;code&gt;f&lt;/code&gt; and deletes the substring &lt;code&gt;START&lt;/code&gt; from the beginning. In fact, it deletes the longest match to the substring, which makes sense when you allow your substring to be something interesting including wildcards (which may have numerous matches). A single &lt;code&gt;#&lt;/code&gt; would delete the shortest match: try&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &amp;gt; echo &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="c1"&gt;#S*T&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;
    ARTonetwothree

    &amp;gt; echo &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="c1"&gt;##S*T&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;
    onetwothree
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To delete the substring from the end of the string, we would use &lt;code&gt;%%&lt;/code&gt; or &lt;code&gt;%&lt;/code&gt;, where once again two means longest and one means shortest.&lt;/p&gt;
&lt;p&gt;So my horrible code fragment creates &lt;code&gt;f'&lt;/code&gt; by removing &lt;code&gt;START&lt;/code&gt; from the start of &lt;code&gt;f&lt;/code&gt;, then creates &lt;code&gt;f''&lt;/code&gt; by removing &lt;code&gt;f'&lt;/code&gt; from the end of&lt;code&gt;f&lt;/code&gt;, combines these with a period in the middle and renames the file named &lt;code&gt;f&lt;/code&gt; with this new string-aberration. An easier way to achieve the same result would have been&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;   mv &lt;span class="nv"&gt;$f&lt;/span&gt; &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="c1"&gt;##START&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;.START
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;but this does not generalise to the case of an arbitrary substring.&lt;/p&gt;
&lt;p&gt;The obvious application of this is removing/modifying file extensions. To rename all &lt;code&gt;.tgz&lt;/code&gt; files as &lt;code&gt;.tar.gz&lt;/code&gt;, for example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    for f in *.tgz do mv &lt;span class="nv"&gt;$f&lt;/span&gt; &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="n"&gt;tgz&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;tar.gz done
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If the offending substring is not at the beginning or end, you could use replacement:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &amp;gt; f=all_workERROR_and_ERRORno_playERROR
    &amp;gt; echo &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;
    all_work_and_no_play
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The syntax is &lt;code&gt;${string/substring/newsubstring}&lt;/code&gt;, where a double first slash (as in previous example) replaces all instances of substring. We can also do partial matching&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &amp;gt; echo &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;
    all_work
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and of course replacement with something other than an empty string:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &amp;gt; fp=&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;_and_all_play&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt; 
    &amp;gt; echo &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;all&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;no&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;
    no_work_and_all_play
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So if you have a bunch of messy gzipped files like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    data.gz.modified.gz.why.gz.did.gz.i.do.this.gz
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;the solution is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    for f in *.gz do mv &lt;span class="nv"&gt;$f&lt;/span&gt; &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;//.&lt;/span&gt;&lt;span class="n"&gt;gz&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;.gz done
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and to wonder how you got into that mess in the first place.&lt;/p&gt;</content><category term="bash"></category><category term="strings"></category><category term="regex"></category><category term="shell"></category></entry><entry><title>putting a line in a filename (with sed)</title><link href="/tips/2013-08-06-putting-a-line-sed.html" rel="alternate"></link><published>2013-08-06T00:00:00+01:00</published><updated>2013-08-06T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2013-08-06:/tips/2013-08-06-putting-a-line-sed.html</id><summary type="html">&lt;p&gt;&lt;em&gt;"How can I cut a line from a file and paste the rest into a file whose title is the line I just cut?"&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If you find yourself asking yourself this question with any degree of regularity, you may have issues. Luckily for you, the help you so desperately need …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;"How can I cut a line from a file and paste the rest into a file whose title is the line I just cut?"&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If you find yourself asking yourself this question with any degree of regularity, you may have issues. Luckily for you, the help you so desperately need is at hand. It is not the help you want, but it is the help you deserve. For added complication let's assume you want to do this for every line in the file.&lt;/p&gt;
&lt;p&gt;Solution (bash):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;    &amp;gt; for i in &lt;/span&gt;&lt;span class="p"&gt;$(&lt;/span&gt;&lt;span class="err"&gt;seq&lt;/span&gt; &lt;span class="err"&gt;`wc&lt;/span&gt; &lt;span class="err"&gt;-l&lt;/span&gt; &lt;span class="err"&gt;&amp;lt;&lt;/span&gt; &lt;span class="err"&gt;FILENAME`&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="x"&gt;    &amp;gt; do&lt;/span&gt;
&lt;span class="x"&gt;    &amp;gt;   sed &amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;d&amp;#39; FILENAME &amp;gt; something_else_maybe-`sed -n &amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="x"&gt;&amp;#39;p&amp;#39; FILENAME`&lt;/span&gt;
&lt;span class="x"&gt;    &amp;gt; done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;What is going on here is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;seq N&lt;/code&gt; prints a sequence of integers, 1 to N.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wc -l &amp;lt; FILENAME&lt;/code&gt; produces the number of lines in &lt;code&gt;FILENAME&lt;/code&gt;. The normal way I do this is &lt;code&gt;wc -l FILENAME&lt;/code&gt;, but that also prints the name of the file, which would confuse &lt;code&gt;seq&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Enclosing a bash command in ` ` (note these are not ' or ", these are backticks (also known as grave accents (bracket nesting))) replaces the command string with the output of the command. &lt;code&gt;$()&lt;/code&gt; also does this. Why are there two ways to do this, and why did I use both of them in my solution? We may never know.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sed 'Md' FILENAME&lt;/code&gt;, as per the rules of &lt;code&gt;sed&lt;/code&gt;, has &lt;code&gt;FILENAME&lt;/code&gt; as output, having deleted line &lt;code&gt;M&lt;/code&gt; (in our case, &lt;code&gt;'$i'&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sed -n 'Qp' FILENAME&lt;/code&gt; runs through &lt;code&gt;FILENAME&lt;/code&gt;, printing nothing (the &lt;code&gt;-n&lt;/code&gt;) flag unless otherwise instructed (with &lt;code&gt;p&lt;/code&gt;), as occurs for line &lt;code&gt;Q&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;"something_else_maybe"&lt;/code&gt; just demonstrates that you could include other elements in the output filename. Further complication could be introduced here (say replacing it with &lt;code&gt;$i&lt;/code&gt; or whatever you want), but that is too far. Too damn far.&lt;/li&gt;
&lt;/ul&gt;</content><category term="sed"></category><category term="shell"></category><category term="bash"></category></entry><entry><title>learning hangul(한글)</title><link href="/lang/2013-08-02-learning-hangul.html" rel="alternate"></link><published>2013-08-02T00:00:00+01:00</published><updated>2013-08-02T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2013-08-02:/lang/2013-08-02-learning-hangul.html</id><summary type="html">&lt;p&gt;&lt;a title="By Illegitimate Barrister (Own work) [CC0, Public domain, Public domain, Public domain, Public domain or Public domain], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3AHang-Cool_1.png"&gt;&lt;img src="//upload.wikimedia.org/wikipedia/commons/thumb/4/41/Hang-Cool_1.png/512px-Hang-Cool_1.png" alt="Hang-Cool 1" width="512" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Korean alphabet (Hangul) is - so far - my favourite writing system. It is logical and efficient. It pleases my sense of style. Since starting this post over a month ago I took up learning Mandarin so my feelings towards Hanzi are liable to threaten Hangul's dominance in the future, but …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a title="By Illegitimate Barrister (Own work) [CC0, Public domain, Public domain, Public domain, Public domain or Public domain], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3AHang-Cool_1.png"&gt;&lt;img src="//upload.wikimedia.org/wikipedia/commons/thumb/4/41/Hang-Cool_1.png/512px-Hang-Cool_1.png" alt="Hang-Cool 1" width="512" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Korean alphabet (Hangul) is - so far - my favourite writing system. It is logical and efficient. It pleases my sense of style. Since starting this post over a month ago I took up learning Mandarin so my feelings towards Hanzi are liable to threaten Hangul's dominance in the future, but for now I side with space-robot alphabet. Because that's what Hangul is.&lt;/p&gt;
&lt;p&gt;&lt;img src="{static}/images/nuclearlaunch.png" alt="See? Space Robots" class="center"&gt;&lt;/p&gt;
&lt;p&gt;At first glance one may assume that Hangul consists of logograms - characters representing words rather than phonemes, but this is not the case. The alphabet is very much phonetic. Each "block" is a single syllable, so for example Hangul(한글) is Han(한)+gul(글).&lt;/p&gt;
&lt;p&gt;Since syllables are made of phonemes, it is not surprising that the blocks consist of sub-components representing these phonemes. (It was surprising the first time I learned of this, because such an elegant solution to written language had not occurred to me - though upon further reflection, the trick is just "writing words more compactly" so it's not as novel as it is aesthetically pleasing.) Some insane person wrote a &lt;a href="http://en.wikipedia.org/wiki/Hangul_consonant_and_vowel_tables"&gt;Wikipedia page&lt;/a&gt; documenting every possible syllabic block in Korean, so all you need to do is memorise all ten thousand of these (give or take a few thousand) and reading Korean will become trivial. End of post. If this idea is appealing to you, I might suggest going to Cambridge to do Part III of the Mathematical Tripos.&lt;/p&gt;
&lt;p&gt;The more elegant solution is to learn the alphabet. Each letter is called a "jamo", but they only occur inside blocks, sort of like quarks. Unlike quarks, we can still look at them individually. I'll include the &lt;a href="http://web.uvic.ca/ling/resources/ipa/charts/IPAlab/IPAlab.htm"&gt;IPA&lt;/a&gt; in [], and a 'translation' of IPA into my accent (mileage may vary). For pronunciation purposes, text is no replacement for audio, so I would suggest finding some videos, like &lt;a href="http://www.youtube.com/watch?v=CdiR-6e1h0o"&gt;this one&lt;/a&gt;, for example.&lt;/p&gt;
&lt;h3&gt;Simple vowels:&lt;/h3&gt;
&lt;p&gt;Simple vowels are made of horizontal or vertical lines and short strokes.&lt;/p&gt;
&lt;p&gt;ㅣ [&lt;strong&gt;i&lt;/strong&gt;] ("ee" in "tree")&lt;br&gt;
ㅏ [&lt;strong&gt;a&lt;/strong&gt;] ("a" in "mad")&lt;br&gt;
ㅓ [&lt;strong&gt;ʌ&lt;/strong&gt;] ("u" in "mud")  &lt;/p&gt;
&lt;p&gt;ㅡ [&lt;strong&gt;ɯ&lt;/strong&gt;] (somewhere between the "oo" in "cool" and the "eu" in "eugh" - I have a really hard time differentiating this from ㅜ)&lt;br&gt;
ㅗ [&lt;strong&gt;o&lt;/strong&gt;] ("o" in "bowl")&lt;br&gt;
ㅜ [&lt;strong&gt;u&lt;/strong&gt;] ("oo" in "too")  &lt;/p&gt;
&lt;h3&gt;Complex vowels:&lt;/h3&gt;
&lt;p&gt;Combinations of simple vowels (including diphthongs). I'm not going to include all combinations because many of them are self-evident given the simple vowels.&lt;/p&gt;
&lt;p&gt;These ones are less obvious:&lt;/p&gt;
&lt;p&gt;ㅐ [&lt;strong&gt;ɛ&lt;/strong&gt;] ("e" in "bed")&lt;br&gt;
ㅔ [&lt;strong&gt;e&lt;/strong&gt;] ("e" in "grey")  &lt;/p&gt;
&lt;p&gt;Generally, ㅗ or ㅜ combined with another vowel gives a "w-" sound, so for example ㅘ is "wah", ㅙ is "weh", and ㅟ is "wee".&lt;/p&gt;
&lt;p&gt;There's no letter for "y" in Korean, so if you want to "y" up a vowel, double up on short strokes (I believe this process is called &lt;em&gt;'iotation'&lt;/em&gt;. You can do something similar in Slavic languages with ь - Cyrillic comes a close second in the space-robot race.) You get:&lt;/p&gt;
&lt;p&gt;ㅑ [&lt;strong&gt;ja&lt;/strong&gt;] ("yah")&lt;br&gt;
ㅕ [&lt;strong&gt;jʌ&lt;/strong&gt;] ("yuh")&lt;br&gt;
ㅛ [&lt;strong&gt;jo&lt;/strong&gt;] ("yoh")&lt;br&gt;
ㅠ [&lt;strong&gt;ju&lt;/strong&gt;] ("yoo")  &lt;/p&gt;
&lt;p&gt;We can extend this to the complex vowels, to get ㅒ for "yeh" and ㅖ for a slightly different "yeh".&lt;/p&gt;
&lt;h3&gt;Consonants:&lt;/h3&gt;
&lt;p&gt;Syllables are usually a consonant-vowel sandwich, so consonants can be "initial", "medial", or "final" (I'll write [&lt;strong&gt;i/m/f&lt;/strong&gt;]), and the placement makes a (small) difference to the pronunciation of the letter.&lt;/p&gt;
&lt;p&gt;ㄱ [&lt;strong&gt;k/g/k̚&lt;/strong&gt;] ("k" as in "Kant", "g" as in "gravity", "k̚" as in "quark")&lt;br&gt;
ㄴ [&lt;strong&gt;n/n/n&lt;/strong&gt;] ("n" as in "neutron")&lt;br&gt;
ㄷ [&lt;strong&gt;t/d/t̚&lt;/strong&gt;] ("t" as in "tachyon", "d" as in "down", "t̚" as in "cat")&lt;br&gt;
ㅅ [&lt;strong&gt;s/s/t̚&lt;/strong&gt;] ("s" as in "strange")&lt;br&gt;
ㅁ [&lt;strong&gt;m/m/m&lt;/strong&gt;] ("m" as in "mass")&lt;br&gt;
ㅂ [&lt;strong&gt;p/b/p̚&lt;/strong&gt;] ("p" as in "point", "b" as in "baryon", "p̚" as in "top")&lt;br&gt;
ㅇ [&lt;strong&gt;-/ŋ/ŋ&lt;/strong&gt;] (This is just a silent placeholder in the initial position. In all others it's "ng", as in "ping")&lt;br&gt;
ㄹ [&lt;strong&gt;ɾ/ɾ/l&lt;/strong&gt;] ("ɾ" as in "alveolar tap", a sound which is neither "r" nor "l")  &lt;/p&gt;
&lt;p&gt;Some consonants are obtained from others by &lt;em&gt;aspiration&lt;/em&gt;. Aspiration is basically just adding air to the sound - so imagine trying to sneak a "h-" sound in after the consonant. In Hangul, the addition of a horizontal line seems to denote this aspiration, or a general 'softening' or alteration of the sound (in the case of the letter I like to think of as "j"). This produces:&lt;/p&gt;
&lt;p&gt;ㄱ &amp;gt; ㅋ [&lt;strong&gt;kʰ/kʰ/k̚&lt;/strong&gt;] ("kʰ" is an aspirated "k", oddly enough)&lt;br&gt;
ㄷ &amp;gt; ㅌ [&lt;strong&gt;tʰ/tʰ/t̚&lt;/strong&gt;]&lt;br&gt;
ㅅ &amp;gt; ㅈ [&lt;strong&gt;tɕ/dʑ/t̚&lt;/strong&gt;] ("tɕ" as in "charm", "dʑ" as in "jam")&lt;br&gt;
ㅈ &amp;gt; ㅊ [&lt;strong&gt;tɕʰ/tɕʰ/t̚&lt;/strong&gt;] ("tɕʰ" as in "oh god send help")&lt;br&gt;
ㅂ &amp;gt; ㅍ [&lt;strong&gt;pʰ/pʰ/p̚&lt;/strong&gt;] ("pʰ" as in &lt;em&gt;strangling noises&lt;/em&gt;)&lt;br&gt;
ㅇ &amp;gt; ㅎ [&lt;strong&gt;h/ɦ/-&lt;/strong&gt;] ("h" as in "hello", "ɦ" as in "cool whip")&lt;/p&gt;
&lt;h3&gt;Doubled letters:&lt;/h3&gt;
&lt;p&gt;There are also "double letters":&lt;br&gt;
ㄲ, ㄸ, ㅃ, ㅆ, ㅉ&lt;br&gt;
... which are "tense", so they're pronounced a bit like you're after spending the last hour reading articles about phonetics and just realised it's too late to watch Breaking Bad. "Damn it!" ~ "땀읻!"&lt;/p&gt;
&lt;p&gt;I should stress that this entire post has very little to do with the Korean language. I don't know any Korean, but transliteration can be fun, and this article was largely about IPA. Trying to cram English into a foreign language really makes you appreciate phonetic differences.&lt;/p&gt;
&lt;p&gt;치샔시챐파에대시추러...&lt;/p&gt;
&lt;p&gt;피탤퍼이팰챜앧아퍀어퍀랟페펤... (curse you lack of "ɘ"!)&lt;/p&gt;</content><category term="hangul"></category><category term="sc2"></category><category term="starcraft"></category><category term="alphabet"></category><category term="IPA"></category><category term="korean"></category></entry><entry><title>man cut and other simple yet useful unix bits</title><link href="/tips/2013-07-22-man-cut.html" rel="alternate"></link><published>2013-07-22T00:00:00+01:00</published><updated>2013-07-22T00:00:00+01:00</updated><author><name>corcra</name></author><id>tag:None,2013-07-22:/tips/2013-07-22-man-cut.html</id><summary type="html">&lt;p&gt;Instead of just reading the man file, you could read this post about &lt;code&gt;cut&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;Printing columns ('fields') n to m (inclusive) from a file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    cut -d [delimiter] -f n-m filename
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Thus, removing the first n-1 fields from a file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    cut -d [delimiter] -f n- filename
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;[delimiter]&lt;/code&gt; is automatically a tab …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Instead of just reading the man file, you could read this post about &lt;code&gt;cut&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;Printing columns ('fields') n to m (inclusive) from a file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    cut -d [delimiter] -f n-m filename
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Thus, removing the first n-1 fields from a file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    cut -d [delimiter] -f n- filename
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;[delimiter]&lt;/code&gt; is automatically a tab space. You could also have ' ' (space), '&lt;code&gt;`'&lt;/code&gt;, '&lt;code&gt;:&lt;/code&gt;', '&lt;code&gt;-&lt;/code&gt;', '&lt;code&gt;_&lt;/code&gt;'. Apparently '&lt;code&gt;HELLO&lt;/code&gt;' is not an acceptable delimiter, which is some kind of bug I guess.&lt;/p&gt;
&lt;p&gt;If you just want a specific column, you could use &lt;code&gt;awk&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;    awk &amp;#39;&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt; print &lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="x"&gt; }&amp;#39; filename&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Or do some fancier things like - say* we have a file containing a list of chromosome numbers and SNPids and some other information separated into columns, and we want to extract just the chromosomes and SNPids, rewriting '&lt;code&gt;2&lt;/code&gt;' as '&lt;code&gt;chr02&lt;/code&gt;' etc. and including a tab space, we could write&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    awk &amp;#39;{ if ($1&amp;lt;10) print &amp;quot;chr0&amp;quot; $1 &amp;quot;\t&amp;quot; $2; else print &amp;quot;chr&amp;quot; $1 &amp;quot;\t&amp;quot; $2 }&amp;#39; filename
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The double-quotation marks are necessary here. In &lt;code&gt;awk&lt;/code&gt; it's not that column numbering intentionally starts from 1 (note that chromosomes, which are in the first column are accessed via &lt;code&gt;$1&lt;/code&gt;), but &lt;code&gt;$0&lt;/code&gt; contains the full line. So you could do&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    awk &amp;#39;{ if ($1+$2 == 3) print $0; else print $1+$2,&amp;quot;is not 3&amp;quot; }&amp;#39; filename
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;if for some reason you wanted to pick out lines whose first two columns sum to three. If you try doing that and &lt;code&gt;$2&lt;/code&gt; or &lt;code&gt;$1&lt;/code&gt; don't contain something which could reasonably be added (e.g. in the SNPid example) &lt;code&gt;awk&lt;/code&gt; will just give weird output and not realise the horrible things it's doing, so be careful with that.&lt;/p&gt;
&lt;p&gt;Note the comma (eg in &lt;code&gt;print $1+$2,"is not 3"&lt;/code&gt;) just denotes a space. As per earlier, use &lt;code&gt;"\t"&lt;/code&gt; to insert a tab.&lt;/p&gt;
&lt;p&gt;You could do something similar to extract all the even or odd columns in a file by silencing those you don't want:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;    awk &amp;#39;&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="x"&gt; for(i=1;i&amp;lt;=NF;i+=2) &lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="x"&gt;=&amp;quot;&amp;quot; }1&amp;#39; filename &amp;gt; evencols&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;No, the &lt;code&gt;1&lt;/code&gt; is not a typo. It just tells &lt;code&gt;awk&lt;/code&gt; to print every line. Now, this will produce some unwanted spaces between fields, so we can get rid of the with &lt;code&gt;sed&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    sed &amp;quot;s/^ //;s/  / /g&amp;quot; evencols
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The basic thing going on here is s/string_to_replace/with_this_string, separated by ; indicating a new command for &lt;code&gt;sed&lt;/code&gt;. In the first one we're stripping a leading whitespace from each line - &lt;code&gt;^&lt;/code&gt; indicates 'start of line', so we're replacing "white space at start" with "nothing". The second command is simply replacing double whitespace with single whitespace. I'm sure there are more rigorous ways to do this, but this worked for me.&lt;/p&gt;
&lt;p&gt;What about finding things? Suppose I have a giant folder - how giant you say?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    ls . | wc -l
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This just pipes the output of &lt;code&gt;ls .&lt;/code&gt; into &lt;code&gt;wc&lt;/code&gt; which, with the &lt;code&gt;-l&lt;/code&gt; flag counts how many lines we have. The folder I'm looking at has 948 things in it, because I am organised like that. I want to find a file with 'wolf' in the title, so I can do&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    ls -l . | grep &amp;#39;wolf&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I inclued the &lt;code&gt;-l&lt;/code&gt; flag on ls because I'm interested in things like the biggest file with &lt;code&gt;wolf&lt;/code&gt; in its title. Supposing I had a worryingly large number of wolf-related files, I could get straight to the biggest one by piping more commands together:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    ls -l . | grep &amp;#39;wolf&amp;#39; | sort -n | tail -1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;sort&lt;/code&gt; outputs low to high, which is why we take the &lt;code&gt;tail -1&lt;/code&gt; one.&lt;/p&gt;
&lt;p&gt;Now, let's suppose I don't know which subdirectory my wolf file is in. I could do&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    find [directory] -name &amp;#39;*wolf*&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;to find all files with '&lt;code&gt;wolf&lt;/code&gt;' anywhere in their title in the directory &lt;code&gt;[directory]&lt;/code&gt; and all subdirectories of it. To search from the current directory, use . as [directory], etc. To only find &lt;code&gt;wolf&lt;/code&gt; files over a certain size (say 1 MB) from the current directory, we have&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    find [directory] -name &amp;#39;*wolf*&amp;#39; -size +1M
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(use &lt;code&gt;-1k&lt;/code&gt; to get &lt;code&gt;wolf&lt;/code&gt; files under 1 kB) or to find all wolf files, sort them by size, and pick out the biggest one, we do&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    find . -name &amp;#39;gray_wolf*&amp;#39; -ls | sort -k5 -n | tail -1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;-ls&lt;/code&gt; flag tells &lt;code&gt;find&lt;/code&gt; to give output in a sort of &lt;code&gt;ls&lt;/code&gt; format. For me, the 5th column of this output is the file-size, so we sort based on this column (&lt;code&gt;sort -k5&lt;/code&gt;), and the rest is the same as before.&lt;/p&gt;
&lt;p&gt;*based on a real event&lt;/p&gt;</content><category term="awk"></category><category term="cut"></category><category term="find"></category><category term="grep"></category><category term="unix"></category><category term="shell"></category></entry></feed>